{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from typing import Dict\n",
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'inputs',\n",
    "        type=str,\n",
    "        nargs='?',\n",
    "        default = './tests/data/kinect/color/',\n",
    "        help='Input image/video path or folder path.')\n",
    "    parser.add_argument(\n",
    "        '--pose3d',\n",
    "        type=str,\n",
    "        default='human3d',\n",
    "        help='Pretrained 3D pose estimation algorithm. It\\'s the path to the '\n",
    "        'config file or the model name defined in metafile.')\n",
    "    parser.add_argument(\n",
    "        '--pose3d-weights',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Path to the custom checkpoint file of the selected pose model. '\n",
    "        'If it is not specified and \"pose3d\" is a model name of metafile, '\n",
    "        'the weights will be loaded from metafile.')\n",
    "    parser.add_argument(\n",
    "        '--device',\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help='Device used for inference. '\n",
    "        'If not specified, the available device will be automatically used.')\n",
    "    parser.add_argument(\n",
    "        '--vis-out-dir',\n",
    "        type=str,\n",
    "        default='./vis_results/human3d',\n",
    "        help='Directory for saving visualized results.')\n",
    "    parser.add_argument(\n",
    "        '--pred-out-dir',\n",
    "        type=str,\n",
    "        default='./vis_results/human3d',\n",
    "        help='Directory for saving inference results.')\n",
    "    parser.add_argument(\n",
    "        '--show-alias',\n",
    "        action='store_true',\n",
    "        help='Display all the available model aliases.')\n",
    "    \n",
    "    call_args = vars(parser.parse_args())\n",
    "\n",
    "    init_kws = [\n",
    "        'device', 'pose3d', 'pose3d_weights'\n",
    "    ]\n",
    "    init_args = {}\n",
    "    for init_kw in init_kws:\n",
    "        init_args[init_kw] = call_args.pop(init_kw)\n",
    "\n",
    "    diaplay_alias = call_args.pop('show_alias')\n",
    "\n",
    "    return init_args, call_args, diaplay_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMBS_KINECT = np.array([[20, 3], # head\n",
    "                         [20, 8], [8, 9], [9, 10], # right arm\n",
    "                         [20, 4], [4, 5], [5, 6], # left arm\n",
    "                         [20, 16], [16, 17], [17, 18], # right leg\n",
    "                         [20, 12], [12, 13], [13, 14]]) # left leg --> 왼쪽 다리 안나옴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_model_aliases(model_aliases: Dict[str, str]) -> None:\n",
    "    \"\"\"Display the available model aliases and their corresponding model\n",
    "    names.\"\"\"\n",
    "    aliases = list(model_aliases.keys())\n",
    "    max_alias_length = max(map(len, aliases))\n",
    "    print(f'{\"ALIAS\".ljust(max_alias_length+2)}MODEL_NAME')\n",
    "    for alias in sorted(aliases):\n",
    "        print(f'{alias.ljust(max_alias_length+2)}{model_aliases[alias]}')\n",
    "\n",
    "# 이미지 파일명에서 프레임 번호 추출 (예: '1_00000200.png' -> 200)\n",
    "def extract_frame_id(image_path):\n",
    "    frame_id_match = re.search(r'(\\d+)_(\\d+).png', image_path)\n",
    "    if frame_id_match:\n",
    "        first_num = frame_id_match.group(1)\n",
    "        second_num = str(int(frame_id_match.group(2)))  # 앞의 0들을 제거하여 숫자 형태로 변환\n",
    "        return f\"{first_num}_{second_num}\"  # 첫 번째 숫자와 뒷부분의 숫자를 연결하여 반환\n",
    "    else:\n",
    "        print(\"Could not extract frame number from the image path\")\n",
    "        return None\n",
    "    \n",
    "# 추가된 함수: Kinect SDK 예측 결과를 로드하는 함수\n",
    "def load_kinect_sdk_predictions(path_to_sdk_json, frame_id):\n",
    "    with open(path_to_sdk_json, 'r') as file:\n",
    "        pred_sdk = json.load(file)\n",
    "\n",
    "    # 문자열 형식의 frame_id를 분리하고 숫자 부분만 정수로 변환\n",
    "    frame_id_num = int(frame_id.split('_')[1])\n",
    "\n",
    "    try:\n",
    "        if frame_id_num < len(pred_sdk) and len(pred_sdk[frame_id_num]) > 0:\n",
    "            print(f\"Loaded prediction for frame index {frame_id_num}\")\n",
    "            return pred_sdk[frame_id_num]\n",
    "        else:\n",
    "            print(f\"No prediction data found for frame index {frame_id_num}\")\n",
    "            return None\n",
    "    except IndexError as e:\n",
    "        print(f\"Error accessing data for frame {frame_id_num}: {e}\")\n",
    "        return None\n",
    "    \n",
    "def load_gt(path_to_gt_json, frame_id):\n",
    "    # JSON 파일로부터 GT 결과를 로드\n",
    "    with open(path_to_gt_json, 'r') as file:\n",
    "        anno_gt = json.load(file)\n",
    "\n",
    "    # 문자열 형식의 frame_id를 분리하고 숫자 부분만 정수로 변환\n",
    "    frame_id_num = int(frame_id.split('_')[1])\n",
    "\n",
    "    # 리스트 내에서 인덱스를 기반으로 예측 결과 반환\n",
    "    if 0 <= frame_id_num < len(anno_gt):\n",
    "        print(f\"Loaded annotation for frame index {frame_id_num}\")\n",
    "        return anno_gt[frame_id_num]\n",
    "    else:\n",
    "        print(f\"No annotation data found for frame index {frame_id_num}\")\n",
    "        return None\n",
    "    \n",
    "def filter_kinect_joints(kinect_joints, joint_mapping):\n",
    "    # Kinect SDK 조인트 좌표 배열에서 MMPose와 매핑된 조인트만 추출\n",
    "    filtered_joints = [kinect_joints[joint_mapping[joint]['kinect']] for joint in joint_mapping]\n",
    "    return np.array(filtered_joints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
