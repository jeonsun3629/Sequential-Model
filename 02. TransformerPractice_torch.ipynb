{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './csvFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 목록을 랜덤하게 섞습니다.\n",
    "random.seed(42)  # 재현 가능한 결과를 위해 시드 설정\n",
    "random.shuffle(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.iloc[300:-100]\n",
    "    df = df.apply(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Time</th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>3.454812</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>352.2222</td>\n",
       "      <td>350.739100</td>\n",
       "      <td>0.733168</td>\n",
       "      <td>0.943708</td>\n",
       "      <td>0.631755</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>359.55840</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>302</td>\n",
       "      <td>3.465498</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>352.2729</td>\n",
       "      <td>350.772300</td>\n",
       "      <td>0.733238</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.631620</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>359.53570</td>\n",
       "      <td>0.884086</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>303</td>\n",
       "      <td>3.476308</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>352.2918</td>\n",
       "      <td>350.806000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944234</td>\n",
       "      <td>0.631682</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>359.52710</td>\n",
       "      <td>0.923961</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>3.487449</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>352.2754</td>\n",
       "      <td>350.814000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944235</td>\n",
       "      <td>0.631826</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>359.53850</td>\n",
       "      <td>0.959352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>305</td>\n",
       "      <td>3.498579</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>352.2285</td>\n",
       "      <td>350.846900</td>\n",
       "      <td>0.733279</td>\n",
       "      <td>0.944438</td>\n",
       "      <td>0.631913</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>359.57180</td>\n",
       "      <td>0.958712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250226</th>\n",
       "      <td>4599</td>\n",
       "      <td>51.249270</td>\n",
       "      <td>-0.673822</td>\n",
       "      <td>0.459417</td>\n",
       "      <td>0.821094</td>\n",
       "      <td>18.55227</td>\n",
       "      <td>179.767100</td>\n",
       "      <td>357.6623</td>\n",
       "      <td>-0.613921</td>\n",
       "      <td>0.391424</td>\n",
       "      <td>...</td>\n",
       "      <td>11.127110</td>\n",
       "      <td>198.3036</td>\n",
       "      <td>3.762991</td>\n",
       "      <td>-1.322575</td>\n",
       "      <td>1.039530</td>\n",
       "      <td>0.926472</td>\n",
       "      <td>47.165740</td>\n",
       "      <td>37.40024</td>\n",
       "      <td>178.720300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250227</th>\n",
       "      <td>4600</td>\n",
       "      <td>51.260610</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>198.3722</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>-1.323386</td>\n",
       "      <td>1.040075</td>\n",
       "      <td>0.924556</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250228</th>\n",
       "      <td>4601</td>\n",
       "      <td>51.271360</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>198.4202</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>-1.323311</td>\n",
       "      <td>1.040272</td>\n",
       "      <td>0.924719</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250229</th>\n",
       "      <td>4602</td>\n",
       "      <td>51.283000</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>198.4699</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>-1.323236</td>\n",
       "      <td>1.040406</td>\n",
       "      <td>0.924903</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250230</th>\n",
       "      <td>4603</td>\n",
       "      <td>51.293670</td>\n",
       "      <td>-0.674747</td>\n",
       "      <td>0.459201</td>\n",
       "      <td>0.823021</td>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>357.4926</td>\n",
       "      <td>-0.614688</td>\n",
       "      <td>0.391303</td>\n",
       "      <td>...</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>198.5145</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>-1.322936</td>\n",
       "      <td>1.038698</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250231 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Frame       Time  m_avg_PelvisPosX  m_avg_PelvisPosY  \\\n",
       "0          301   3.454812          0.021056          0.537794   \n",
       "1          302   3.465498          0.021056          0.537794   \n",
       "2          303   3.476308          0.021056          0.537794   \n",
       "3          304   3.487449          0.021056          0.537794   \n",
       "4          305   3.498579          0.021056          0.537794   \n",
       "...        ...        ...               ...               ...   \n",
       "1250226   4599  51.249270         -0.673822          0.459417   \n",
       "1250227   4600  51.260610         -0.674628          0.459754   \n",
       "1250228   4601  51.271360         -0.674628          0.459754   \n",
       "1250229   4602  51.283000         -0.674628          0.459754   \n",
       "1250230   4603  51.293670         -0.674747          0.459201   \n",
       "\n",
       "         m_avg_PelvisPosZ  m_avg_PelvisRotX  m_avg_PelvisRotY  \\\n",
       "0                0.689871         358.82500          0.679502   \n",
       "1                0.689871         358.82500          0.679502   \n",
       "2                0.689871         358.82500          0.679502   \n",
       "3                0.689871         358.82500          0.679492   \n",
       "4                0.689871         358.82500          0.679492   \n",
       "...                   ...               ...               ...   \n",
       "1250226          0.821094          18.55227        179.767100   \n",
       "1250227          0.821685          18.35563        179.608500   \n",
       "1250228          0.821685          18.35563        179.608500   \n",
       "1250229          0.821685          18.35563        179.608500   \n",
       "1250230          0.823021          18.44855        179.665600   \n",
       "\n",
       "         m_avg_PelvisRotZ  m_avg_L_HipPosX  m_avg_L_HipPosY  ...  \\\n",
       "0                359.8412        -0.037938         0.455332  ...   \n",
       "1                359.8412        -0.037938         0.455332  ...   \n",
       "2                359.8412        -0.037938         0.455332  ...   \n",
       "3                359.8412        -0.037938         0.455332  ...   \n",
       "4                359.8412        -0.037938         0.455332  ...   \n",
       "...                   ...              ...              ...  ...   \n",
       "1250226          357.6623        -0.613921         0.391424  ...   \n",
       "1250227          357.6249        -0.614788         0.391658  ...   \n",
       "1250228          357.6249        -0.614788         0.391658  ...   \n",
       "1250229          357.6249        -0.614788         0.391658  ...   \n",
       "1250230          357.4926        -0.614688         0.391303  ...   \n",
       "\n",
       "         m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "0                 8.262754           352.2222         350.739100   \n",
       "1                 8.274371           352.2729         350.772300   \n",
       "2                 8.273871           352.2918         350.806000   \n",
       "3                 8.267890           352.2754         350.814000   \n",
       "4                 8.257694           352.2285         350.846900   \n",
       "...                    ...                ...                ...   \n",
       "1250226          11.127110           198.3036           3.762991   \n",
       "1250227          11.131550           198.3722           3.791916   \n",
       "1250228          11.133720           198.4202           3.836214   \n",
       "1250229          11.135700           198.4699           3.866319   \n",
       "1250230          11.140010           198.5145           3.896140   \n",
       "\n",
       "         m_avg_R_WristPosX  m_avg_R_WristPosY  m_avg_R_WristPosZ  \\\n",
       "0                 0.733168           0.943708           0.631755   \n",
       "1                 0.733238           0.943977           0.631620   \n",
       "2                 0.733297           0.944234           0.631682   \n",
       "3                 0.733297           0.944235           0.631826   \n",
       "4                 0.733279           0.944438           0.631913   \n",
       "...                    ...                ...                ...   \n",
       "1250226          -1.322575           1.039530           0.926472   \n",
       "1250227          -1.323386           1.040075           0.924556   \n",
       "1250228          -1.323311           1.040272           0.924719   \n",
       "1250229          -1.323236           1.040406           0.924903   \n",
       "1250230          -1.322936           1.038698           0.926400   \n",
       "\n",
       "         m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  Unnamed: 128  \n",
       "0                 0.200809          359.55840           0.846429           NaN  \n",
       "1                 0.114945          359.53570           0.884086           NaN  \n",
       "2                 0.151095          359.52710           0.923961           NaN  \n",
       "3                 0.239347          359.53850           0.959352           NaN  \n",
       "4                 0.385812          359.57180           0.958712           NaN  \n",
       "...                    ...                ...                ...           ...  \n",
       "1250226          47.165740           37.40024         178.720300           NaN  \n",
       "1250227          47.211770           37.32508         178.617100           NaN  \n",
       "1250228          47.288450           37.23001         178.460700           NaN  \n",
       "1250229          47.363370           37.13769         178.286900           NaN  \n",
       "1250230          47.438840           37.06465         178.126200           NaN  \n",
       "\n",
       "[1250231 rows x 129 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250231"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_columns = [col for col in combined_df.columns if 'Rot' in col]\n",
    "rotation_df = combined_df[rotation_columns]\n",
    "\n",
    "# -180~180 사이로 정규화\n",
    "normalize_angle = lambda x: (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "rotation_df = rotation_df.apply(lambda col: col.apply(normalize_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어나는 값이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = (rotation_df > 180).sum().sum() + (rotation_df < -180).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_df.to_csv('./rotation_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 데이터를 DataFrame으로 변환\n",
    "rotation_df = pd.DataFrame(rotation_df, columns=rotation_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>m_avg_L_HipRotY</th>\n",
       "      <th>m_avg_L_HipRotZ</th>\n",
       "      <th>m_avg_L_KneeRotX</th>\n",
       "      <th>m_avg_L_KneeRotY</th>\n",
       "      <th>m_avg_L_KneeRotZ</th>\n",
       "      <th>m_avg_L_AnkleRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.109241</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.92970</td>\n",
       "      <td>0.176747</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>...</td>\n",
       "      <td>1.795381</td>\n",
       "      <td>9.137008</td>\n",
       "      <td>-0.3370</td>\n",
       "      <td>-6.65920</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>-7.7778</td>\n",
       "      <td>-9.260900</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>-0.44160</td>\n",
       "      <td>0.846429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.109241</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.92970</td>\n",
       "      <td>0.176747</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>...</td>\n",
       "      <td>1.795381</td>\n",
       "      <td>9.137484</td>\n",
       "      <td>-0.3512</td>\n",
       "      <td>-6.63370</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>-7.7271</td>\n",
       "      <td>-9.227700</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>-0.46430</td>\n",
       "      <td>0.884086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.109241</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.92970</td>\n",
       "      <td>0.176747</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>0.043651</td>\n",
       "      <td>...</td>\n",
       "      <td>1.795381</td>\n",
       "      <td>9.128529</td>\n",
       "      <td>-0.3738</td>\n",
       "      <td>-6.60920</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>-7.7082</td>\n",
       "      <td>-9.194000</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>-0.47290</td>\n",
       "      <td>0.923961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.109216</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.92970</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.795413</td>\n",
       "      <td>9.122496</td>\n",
       "      <td>-0.3871</td>\n",
       "      <td>-6.61610</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>-7.7246</td>\n",
       "      <td>-9.186000</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>-0.46150</td>\n",
       "      <td>0.959352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>0.109216</td>\n",
       "      <td>-0.0179</td>\n",
       "      <td>-0.92970</td>\n",
       "      <td>0.176716</td>\n",
       "      <td>-0.0886</td>\n",
       "      <td>0.043725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.795413</td>\n",
       "      <td>9.118235</td>\n",
       "      <td>-0.3485</td>\n",
       "      <td>-6.60340</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>-7.7715</td>\n",
       "      <td>-9.153100</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>-0.42820</td>\n",
       "      <td>0.958712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250226</th>\n",
       "      <td>18.55227</td>\n",
       "      <td>179.767100</td>\n",
       "      <td>-2.3377</td>\n",
       "      <td>-0.4977</td>\n",
       "      <td>178.544800</td>\n",
       "      <td>-15.6818</td>\n",
       "      <td>53.06687</td>\n",
       "      <td>167.804900</td>\n",
       "      <td>-10.1720</td>\n",
       "      <td>61.003280</td>\n",
       "      <td>...</td>\n",
       "      <td>22.802470</td>\n",
       "      <td>12.731160</td>\n",
       "      <td>-177.8204</td>\n",
       "      <td>15.82798</td>\n",
       "      <td>11.127110</td>\n",
       "      <td>-161.6964</td>\n",
       "      <td>3.762991</td>\n",
       "      <td>47.165740</td>\n",
       "      <td>37.40024</td>\n",
       "      <td>178.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250227</th>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>178.673800</td>\n",
       "      <td>-15.7692</td>\n",
       "      <td>53.21533</td>\n",
       "      <td>167.780100</td>\n",
       "      <td>-10.3196</td>\n",
       "      <td>61.120850</td>\n",
       "      <td>...</td>\n",
       "      <td>21.790150</td>\n",
       "      <td>11.644520</td>\n",
       "      <td>-178.0025</td>\n",
       "      <td>15.93279</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>-161.6278</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250228</th>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>178.673800</td>\n",
       "      <td>-15.7692</td>\n",
       "      <td>53.21533</td>\n",
       "      <td>167.780100</td>\n",
       "      <td>-10.3196</td>\n",
       "      <td>61.120850</td>\n",
       "      <td>...</td>\n",
       "      <td>21.790150</td>\n",
       "      <td>11.644520</td>\n",
       "      <td>-178.0025</td>\n",
       "      <td>15.93279</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>-161.5798</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250229</th>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>178.673800</td>\n",
       "      <td>-15.7692</td>\n",
       "      <td>53.21533</td>\n",
       "      <td>167.780100</td>\n",
       "      <td>-10.3196</td>\n",
       "      <td>61.120850</td>\n",
       "      <td>...</td>\n",
       "      <td>21.790150</td>\n",
       "      <td>11.644520</td>\n",
       "      <td>-178.0025</td>\n",
       "      <td>15.93279</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>-161.5301</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250230</th>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>-2.5074</td>\n",
       "      <td>-0.7340</td>\n",
       "      <td>179.259500</td>\n",
       "      <td>-15.6526</td>\n",
       "      <td>53.54498</td>\n",
       "      <td>168.456400</td>\n",
       "      <td>-10.0485</td>\n",
       "      <td>59.391420</td>\n",
       "      <td>...</td>\n",
       "      <td>22.249130</td>\n",
       "      <td>12.379120</td>\n",
       "      <td>-177.6754</td>\n",
       "      <td>15.54438</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>-161.4855</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250231 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  \\\n",
       "0                -1.17500          0.679502           -0.1588   \n",
       "1                -1.17500          0.679502           -0.1588   \n",
       "2                -1.17500          0.679502           -0.1588   \n",
       "3                -1.17500          0.679492           -0.1588   \n",
       "4                -1.17500          0.679492           -0.1588   \n",
       "...                   ...               ...               ...   \n",
       "1250226          18.55227        179.767100           -2.3377   \n",
       "1250227          18.35563        179.608500           -2.3751   \n",
       "1250228          18.35563        179.608500           -2.3751   \n",
       "1250229          18.35563        179.608500           -2.3751   \n",
       "1250230          18.44855        179.665600           -2.5074   \n",
       "\n",
       "         m_avg_L_HipRotX  m_avg_L_HipRotY  m_avg_L_HipRotZ  m_avg_L_KneeRotX  \\\n",
       "0                -0.2778         0.109241          -0.0179          -0.92970   \n",
       "1                -0.2778         0.109241          -0.0179          -0.92970   \n",
       "2                -0.2778         0.109241          -0.0179          -0.92970   \n",
       "3                -0.2778         0.109216          -0.0179          -0.92970   \n",
       "4                -0.2778         0.109216          -0.0179          -0.92970   \n",
       "...                  ...              ...              ...               ...   \n",
       "1250226          -0.4977       178.544800         -15.6818          53.06687   \n",
       "1250227          -0.4831       178.673800         -15.7692          53.21533   \n",
       "1250228          -0.4831       178.673800         -15.7692          53.21533   \n",
       "1250229          -0.4831       178.673800         -15.7692          53.21533   \n",
       "1250230          -0.7340       179.259500         -15.6526          53.54498   \n",
       "\n",
       "         m_avg_L_KneeRotY  m_avg_L_KneeRotZ  m_avg_L_AnkleRotX  ...  \\\n",
       "0                0.176747           -0.0886           0.043651  ...   \n",
       "1                0.176747           -0.0886           0.043651  ...   \n",
       "2                0.176747           -0.0886           0.043651  ...   \n",
       "3                0.176716           -0.0886           0.043725  ...   \n",
       "4                0.176716           -0.0886           0.043725  ...   \n",
       "...                   ...               ...                ...  ...   \n",
       "1250226        167.804900          -10.1720          61.003280  ...   \n",
       "1250227        167.780100          -10.3196          61.120850  ...   \n",
       "1250228        167.780100          -10.3196          61.120850  ...   \n",
       "1250229        167.780100          -10.3196          61.120850  ...   \n",
       "1250230        168.456400          -10.0485          59.391420  ...   \n",
       "\n",
       "         m_avg_R_CollarRotZ  m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  \\\n",
       "0                  1.795381              9.137008               -0.3370   \n",
       "1                  1.795381              9.137484               -0.3512   \n",
       "2                  1.795381              9.128529               -0.3738   \n",
       "3                  1.795413              9.122496               -0.3871   \n",
       "4                  1.795413              9.118235               -0.3485   \n",
       "...                     ...                   ...                   ...   \n",
       "1250226           22.802470             12.731160             -177.8204   \n",
       "1250227           21.790150             11.644520             -178.0025   \n",
       "1250228           21.790150             11.644520             -178.0025   \n",
       "1250229           21.790150             11.644520             -178.0025   \n",
       "1250230           22.249130             12.379120             -177.6754   \n",
       "\n",
       "         m_avg_R_ShoulderRotZ  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "0                    -6.65920           8.262754            -7.7778   \n",
       "1                    -6.63370           8.274371            -7.7271   \n",
       "2                    -6.60920           8.273871            -7.7082   \n",
       "3                    -6.61610           8.267890            -7.7246   \n",
       "4                    -6.60340           8.257694            -7.7715   \n",
       "...                       ...                ...                ...   \n",
       "1250226              15.82798          11.127110          -161.6964   \n",
       "1250227              15.93279          11.131550          -161.6278   \n",
       "1250228              15.93279          11.133720          -161.5798   \n",
       "1250229              15.93279          11.135700          -161.5301   \n",
       "1250230              15.54438          11.140010          -161.4855   \n",
       "\n",
       "         m_avg_R_ElbowRotZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "0                -9.260900           0.200809           -0.44160   \n",
       "1                -9.227700           0.114945           -0.46430   \n",
       "2                -9.194000           0.151095           -0.47290   \n",
       "3                -9.186000           0.239347           -0.46150   \n",
       "4                -9.153100           0.385812           -0.42820   \n",
       "...                    ...                ...                ...   \n",
       "1250226           3.762991          47.165740           37.40024   \n",
       "1250227           3.791916          47.211770           37.32508   \n",
       "1250228           3.836214          47.288450           37.23001   \n",
       "1250229           3.866319          47.363370           37.13769   \n",
       "1250230           3.896140          47.438840           37.06465   \n",
       "\n",
       "         m_avg_R_WristRotZ  \n",
       "0                 0.846429  \n",
       "1                 0.884086  \n",
       "2                 0.923961  \n",
       "3                 0.959352  \n",
       "4                 0.958712  \n",
       "...                    ...  \n",
       "1250226         178.720300  \n",
       "1250227         178.617100  \n",
       "1250228         178.460700  \n",
       "1250229         178.286900  \n",
       "1250230         178.126200  \n",
       "\n",
       "[1250231 rows x 63 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = rotation_df.iloc[:897757]\n",
    "test = rotation_df.iloc[897757:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30  # Sequence length\n",
    "n_features = 63  # Number of features\n",
    "output_units = (21 * 3)  # Output shape\n",
    "head_size = 256  # Size of attention head\n",
    "num_heads = 7  # Number of attention heads\n",
    "ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "num_blocks = 4  # Number of transformer blocks\n",
    "mlp_units = [512, 256, 128]  # Size of the dense layers of the final classifier\n",
    "dropout_rate = 0.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.sequence_length] # : 입력 시퀀스 (30개의 데이터)\n",
    "        y = self.data[index+self.sequence_length-1] # : 예측 시퀀스 (1개의 데이터)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "# 데이터셋과 데이터 로더를 생성\n",
    "train_dataset = TimeseriesDataset(X_train.values, n_input)\n",
    "val_dataset = TimeseriesDataset(X_val.values, n_input)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([128, 30, 63])\n",
      "y batch shape: torch.Size([128, 63])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로더에서 첫 번째 배치를 가져와서 형태를 확인\n",
    "first_batch = next(iter(train_loader))\n",
    "X, y = first_batch\n",
    "\n",
    "print(\"X batch shape:\", X.shape)\n",
    "print(\"y batch shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_features, n_input):\n",
    "        super(PositionalEncoding, self).__init__() # 상속받은 nn.Module 클래스의 __init__() 메서드 호출\n",
    "        pe = torch.zeros(n_input, n_features) # 가장 큰 시퀀스 길이인 max_len을 기준으로 모두 0으로 채워진 크기가 (max_len, n_features)인 텐서 생성\n",
    "        position = torch.arange(0, n_input, dtype=torch.float).unsqueeze(1) # position 텐서 생성\n",
    "\n",
    "        # div_term을 계산하는 방식 수정\n",
    "        div_term = torch.exp(torch.arange(0, n_features, 2).float() * (-math.log(10000.0) / n_features)) # div_term 계산\n",
    "        \n",
    "        # div_term의 길이를 n_features의 절반으로 조정\n",
    "        div_term = div_term.repeat_interleave(2)[:n_features] # div_term 텐서 생성\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2]) # 짝수 인덱스에는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2]) # 홀수 인덱스에는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0) # pe = [bs, seq_len, n_feautres]\n",
    "        self.register_buffer('pe', pe) # pe 텐서를 모델의 버퍼로 등록\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.pe.shape)\n",
    "        x = x + self.pe[:, :x.size(1), :] # 입력에 위치 인코딩을 더함\n",
    "        return x\n",
    "\n",
    "# Transformer Block 정의\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(n_features, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_features, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "# 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout, n_input):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(n_features, num_heads, ff_dim, dropout) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        # 첫 번째 nn.Linear 층의 입력 차원을 n_features로 설정\n",
    "        self.layers.add_module(\"dense_0\", nn.Linear(n_features, mlp_units[0]))\n",
    "        self.layers.add_module(\"relu_0\", nn.ReLU())\n",
    "        self.layers.add_module(\"dropout_0\", nn.Dropout(dropout))\n",
    "        self.layers.add_module(\"norm_0\", nn.LayerNorm(mlp_units[0]))\n",
    "\n",
    "        # 이후 층들에 대한 설정\n",
    "        for i in range(1, len(mlp_units)):\n",
    "            self.layers.add_module(f\"dense_{i}\", nn.Linear(mlp_units[i-1], mlp_units[i]))\n",
    "            self.layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            self.layers.add_module(f\"dropout_{i}\", nn.Dropout(dropout))\n",
    "            self.layers.add_module(f\"norm_{i}\", nn.LayerNorm(mlp_units[i]))\n",
    "\n",
    "        # 최종 출력 층\n",
    "        self.out = nn.Linear(mlp_units[-1], n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = TransformerModel(n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout_rate, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "x = torch.rand(128, 30, 63)\n",
    "y = pos_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'm_avg_PelvisRotX', 'm_avg_PelvisRotY', 'm_avg_PelvisRotZ',\n",
    "    'm_avg_L_HipRotX', 'm_avg_L_HipRotY', 'm_avg_L_HipRotZ',\n",
    "    'm_avg_L_KneeRotX', 'm_avg_L_KneeRotY', 'm_avg_L_KneeRotZ',\n",
    "    'm_avg_L_AnkleRotX', 'm_avg_L_AnkleRotY', 'm_avg_L_AnkleRotZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "    'm_avg_R_HipRotX', 'm_avg_R_HipRotY', 'm_avg_R_HipRotZ',\n",
    "    'm_avg_R_KneeRotX', 'm_avg_R_KneeRotY', 'm_avg_R_KneeRotZ',\n",
    "    'm_avg_R_AnkleRotX', 'm_avg_R_AnkleRotY', 'm_avg_R_AnkleRotZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "    'm_avg_Spine1RotX', 'm_avg_Spine1RotY', 'm_avg_Spine1RotZ',\n",
    "    'm_avg_Spine2RotX', 'm_avg_Spine2RotY', 'm_avg_Spine2RotZ',\n",
    "    'm_avg_L_CollarRotX', 'm_avg_L_CollarRotY', 'm_avg_L_CollarRotZ',\n",
    "    'm_avg_L_ShoulderRotX', 'm_avg_L_ShoulderRotY', 'm_avg_L_ShoulderRotZ',\n",
    "    'm_avg_L_ElbowRotX', 'm_avg_L_ElbowRotY', 'm_avg_L_ElbowRotZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_NeckRotX', 'm_avg_NeckRotY', 'm_avg_NeckRotZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_R_CollarRotX', 'm_avg_R_CollarRotY', 'm_avg_R_CollarRotZ',\n",
    "    'm_avg_R_ShoulderRotX', 'm_avg_R_ShoulderRotY', 'm_avg_R_ShoulderRotZ',\n",
    "    'm_avg_R_ElbowRotX', 'm_avg_R_ElbowRotY', 'm_avg_R_ElbowRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 43, 44, 48, 49, 50, 60, 61, 62]\n"
     ]
    }
   ],
   "source": [
    "column_names = [\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ'\n",
    "]\n",
    "\n",
    "weighted_columns_indices = [column_order.index(name) for name in column_names]\n",
    "print(weighted_columns_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weighted_columns_indices, weight_for_weighted_columns):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weighted_columns_indices = torch.tensor(weighted_columns_indices)\n",
    "        self.weight_for_weighted_columns = weight_for_weighted_columns\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # y_pred의 마지막 feature를 rot_diff_category로 분리\n",
    "        y_pred_values = y_pred[:, :]\n",
    "\n",
    "        # MSE 계산\n",
    "        mse = F.mse_loss(y_true[:, :], y_pred_values, reduction='none')\n",
    "        mse = mse.mean(axis=-1)\n",
    "\n",
    "        # 특정 joint rotation에 대한 가중치 적용\n",
    "        weighted_mse = y_pred_values[:, self.weighted_columns_indices]\n",
    "        weighted_mse = (weighted_mse ** 2) * self.weight_for_weighted_columns\n",
    "        mse += weighted_mse.mean(axis=-1)\n",
    "\n",
    "        return mse.mean()  # 전체 배치에 대한 평균 손실 반환\n",
    "\n",
    "# 가중치를 적용할 열 인덱스와 가중치 값\n",
    "weighted_columns_indices = [42, 43, 44, 48, 49, 50, 60, 61, 62] \n",
    "weight_for_weighted_columns = 2.0\n",
    "\n",
    "# CustomLoss 인스턴스 생성\n",
    "custom_loss_instance = CustomLoss(\n",
    "    weighted_columns_indices=weighted_columns_indices,\n",
    "    weight_for_weighted_columns=weight_for_weighted_columns,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화기와 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = custom_loss_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "epochs = 50\n",
    "patience = 7  # Early Stopping patience\n",
    "best_loss = np.inf\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=7, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Current device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "PositionalEncoding-1               [-1, 30, 63]               0\n",
      "MultiheadAttention-2  [[-1, 30, 63], [-1, 2, 2]]               0\n",
      "           Dropout-3               [-1, 30, 63]               0\n",
      "         LayerNorm-4               [-1, 30, 63]             126\n",
      "            Linear-5              [-1, 30, 512]          32,768\n",
      "              ReLU-6              [-1, 30, 512]               0\n",
      "            Linear-7               [-1, 30, 63]          32,319\n",
      "           Dropout-8               [-1, 30, 63]               0\n",
      "         LayerNorm-9               [-1, 30, 63]             126\n",
      " TransformerBlock-10               [-1, 30, 63]               0\n",
      "MultiheadAttention-11  [[-1, 30, 63], [-1, 2, 2]]               0\n",
      "          Dropout-12               [-1, 30, 63]               0\n",
      "        LayerNorm-13               [-1, 30, 63]             126\n",
      "           Linear-14              [-1, 30, 512]          32,768\n",
      "             ReLU-15              [-1, 30, 512]               0\n",
      "           Linear-16               [-1, 30, 63]          32,319\n",
      "          Dropout-17               [-1, 30, 63]               0\n",
      "        LayerNorm-18               [-1, 30, 63]             126\n",
      " TransformerBlock-19               [-1, 30, 63]               0\n",
      "MultiheadAttention-20  [[-1, 30, 63], [-1, 2, 2]]               0\n",
      "          Dropout-21               [-1, 30, 63]               0\n",
      "        LayerNorm-22               [-1, 30, 63]             126\n",
      "           Linear-23              [-1, 30, 512]          32,768\n",
      "             ReLU-24              [-1, 30, 512]               0\n",
      "           Linear-25               [-1, 30, 63]          32,319\n",
      "          Dropout-26               [-1, 30, 63]               0\n",
      "        LayerNorm-27               [-1, 30, 63]             126\n",
      " TransformerBlock-28               [-1, 30, 63]               0\n",
      "MultiheadAttention-29  [[-1, 30, 63], [-1, 2, 2]]               0\n",
      "          Dropout-30               [-1, 30, 63]               0\n",
      "        LayerNorm-31               [-1, 30, 63]             126\n",
      "           Linear-32              [-1, 30, 512]          32,768\n",
      "             ReLU-33              [-1, 30, 512]               0\n",
      "           Linear-34               [-1, 30, 63]          32,319\n",
      "          Dropout-35               [-1, 30, 63]               0\n",
      "        LayerNorm-36               [-1, 30, 63]             126\n",
      " TransformerBlock-37               [-1, 30, 63]               0\n",
      "           Linear-38                  [-1, 512]          32,768\n",
      "             ReLU-39                  [-1, 512]               0\n",
      "          Dropout-40                  [-1, 512]               0\n",
      "        LayerNorm-41                  [-1, 512]           1,024\n",
      "           Linear-42                  [-1, 256]         131,328\n",
      "             ReLU-43                  [-1, 256]               0\n",
      "          Dropout-44                  [-1, 256]               0\n",
      "        LayerNorm-45                  [-1, 256]             512\n",
      "           Linear-46                  [-1, 128]          32,896\n",
      "             ReLU-47                  [-1, 128]               0\n",
      "          Dropout-48                  [-1, 128]               0\n",
      "        LayerNorm-49                  [-1, 128]             256\n",
      "           Linear-50                   [-1, 63]           8,127\n",
      "================================================================\n",
      "Total params: 468,267\n",
      "Trainable params: 468,267\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.10\n",
      "Params size (MB): 1.79\n",
      "Estimated Total Size (MB): 2.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(n_input, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 5611/5611 [00:33<00:00, 165.49batch/s, train_loss=5.78e+3]\n",
      "Validation Epoch 1/50: 100%|██████████| 1403/1403 [00:04<00:00, 334.82batch/s, val_loss=2.54e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 9256734220.392578 \t Validation Loss: 2295748468.054688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 5611/5611 [00:34<00:00, 164.39batch/s, train_loss=5.75e+3]\n",
      "Validation Epoch 2/50: 100%|██████████| 1403/1403 [00:04<00:00, 331.51batch/s, val_loss=2.43e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t Training Loss: 8730143192.529297 \t Validation Loss: 2197030078.701172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 5611/5611 [00:34<00:00, 163.43batch/s, train_loss=5.72e+3]\n",
      "Validation Epoch 3/50: 100%|██████████| 1403/1403 [00:04<00:00, 328.60batch/s, val_loss=2.37e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t Training Loss: 8450650085.692383 \t Validation Loss: 2155210687.085938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.21batch/s, train_loss=5.67e+3]\n",
      "Validation Epoch 4/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.61batch/s, val_loss=2.34e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t Training Loss: 8285319311.539062 \t Validation Loss: 2136748699.439453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 5611/5611 [00:34<00:00, 162.09batch/s, train_loss=5.63e+3]\n",
      "Validation Epoch 5/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.36batch/s, val_loss=2.33e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t Training Loss: 8181037038.313477 \t Validation Loss: 2112895842.546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 5611/5611 [00:35<00:00, 155.97batch/s, train_loss=5.62e+3]\n",
      "Validation Epoch 6/50: 100%|██████████| 1403/1403 [00:04<00:00, 319.66batch/s, val_loss=2.31e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 \t Training Loss: 8106934569.359375 \t Validation Loss: 2096960015.523438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 5611/5611 [00:35<00:00, 156.65batch/s, train_loss=5.59e+3]\n",
      "Validation Epoch 7/50: 100%|██████████| 1403/1403 [00:04<00:00, 320.08batch/s, val_loss=2.27e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t Training Loss: 8046376078.333008 \t Validation Loss: 2082502663.779297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 5611/5611 [00:36<00:00, 152.94batch/s, train_loss=5.57e+3]\n",
      "Validation Epoch 8/50: 100%|██████████| 1403/1403 [00:04<00:00, 311.44batch/s, val_loss=2.25e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 \t Training Loss: 7992929510.982422 \t Validation Loss: 2068205958.517578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.39batch/s, train_loss=5.56e+3]\n",
      "Validation Epoch 9/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.15batch/s, val_loss=2.24e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 \t Training Loss: 7944746530.980469 \t Validation Loss: 2056624627.753906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.10batch/s, train_loss=5.55e+3]\n",
      "Validation Epoch 10/50: 100%|██████████| 1403/1403 [00:04<00:00, 327.30batch/s, val_loss=2.22e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Training Loss: 7900517650.628906 \t Validation Loss: 2048139048.681641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.32batch/s, train_loss=5.54e+3]\n",
      "Validation Epoch 11/50: 100%|██████████| 1403/1403 [00:04<00:00, 327.10batch/s, val_loss=2.21e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 \t Training Loss: 7858945834.587891 \t Validation Loss: 2043393982.875000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.72batch/s, train_loss=5.53e+3]\n",
      "Validation Epoch 12/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.04batch/s, val_loss=2.21e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 \t Training Loss: 7819775842.837891 \t Validation Loss: 2031653500.537109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 5611/5611 [00:35<00:00, 160.03batch/s, train_loss=5.54e+3]\n",
      "Validation Epoch 13/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.56batch/s, val_loss=2.17e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 \t Training Loss: 7784691336.599609 \t Validation Loss: 2019197046.371094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.46batch/s, train_loss=5.51e+3]\n",
      "Validation Epoch 14/50: 100%|██████████| 1403/1403 [00:04<00:00, 311.10batch/s, val_loss=2.18e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 \t Training Loss: 7753618570.902344 \t Validation Loss: 2022063253.296875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 5611/5611 [00:39<00:00, 142.86batch/s, train_loss=5.51e+3]\n",
      "Validation Epoch 15/50: 100%|██████████| 1403/1403 [00:04<00:00, 300.34batch/s, val_loss=2.16e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t Training Loss: 7725184106.321289 \t Validation Loss: 2006508694.269531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 5611/5611 [00:35<00:00, 157.90batch/s, train_loss=5.51e+3]\n",
      "Validation Epoch 16/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.55batch/s, val_loss=2.15e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 \t Training Loss: 7700914348.765625 \t Validation Loss: 2007181093.667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.48batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 17/50: 100%|██████████| 1403/1403 [00:04<00:00, 305.31batch/s, val_loss=2.15e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 \t Training Loss: 7680109057.798828 \t Validation Loss: 2008511882.298828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.92batch/s, train_loss=5.5e+3] \n",
      "Validation Epoch 18/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.76batch/s, val_loss=2.16e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 \t Training Loss: 7661617509.012695 \t Validation Loss: 1998298419.667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.26batch/s, train_loss=5.5e+3] \n",
      "Validation Epoch 19/50: 100%|██████████| 1403/1403 [00:04<00:00, 327.33batch/s, val_loss=2.15e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 \t Training Loss: 7645714520.533203 \t Validation Loss: 2002161200.507812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.44batch/s, train_loss=5.51e+3]\n",
      "Validation Epoch 20/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.14batch/s, val_loss=2.14e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t Training Loss: 7629356723.478516 \t Validation Loss: 2001210313.152344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.40batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 21/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.47batch/s, val_loss=2.14e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 \t Training Loss: 7615502845.166016 \t Validation Loss: 1994826028.496094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.46batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 22/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.46batch/s, val_loss=2.15e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 \t Training Loss: 7603283155.664062 \t Validation Loss: 1987756834.355469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 5611/5611 [00:34<00:00, 161.09batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 23/50: 100%|██████████| 1403/1403 [00:04<00:00, 327.00batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 \t Training Loss: 7591532709.171875 \t Validation Loss: 1986612626.138672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.79batch/s, train_loss=5495.5] \n",
      "Validation Epoch 24/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.01batch/s, val_loss=2.16e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 \t Training Loss: 7579028549.207031 \t Validation Loss: 1983261913.517578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.73batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 25/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.62batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 \t Training Loss: 7567409801.888672 \t Validation Loss: 1980177961.316406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 5611/5611 [00:36<00:00, 153.65batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 26/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.90batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 \t Training Loss: 7556917517.285156 \t Validation Loss: 1982856766.416016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.40batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 27/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.01batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 \t Training Loss: 7545715504.312500 \t Validation Loss: 1974799303.248047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.96batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 28/50: 100%|██████████| 1403/1403 [00:04<00:00, 322.69batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 \t Training Loss: 7534762359.703125 \t Validation Loss: 1977094396.710938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.91batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 29/50: 100%|██████████| 1403/1403 [00:04<00:00, 328.08batch/s, val_loss=2.1e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 \t Training Loss: 7524181558.933594 \t Validation Loss: 1967447054.347656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.86batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 30/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.27batch/s, val_loss=2.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 \t Training Loss: 7513978239.703125 \t Validation Loss: 1967705041.115234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.81batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 31/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.43batch/s, val_loss=2.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 \t Training Loss: 7503829812.105469 \t Validation Loss: 1967322586.962891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.60batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 32/50: 100%|██████████| 1403/1403 [00:04<00:00, 320.81batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 \t Training Loss: 7493607569.859375 \t Validation Loss: 1967347670.667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.42batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 33/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.94batch/s, val_loss=2.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 \t Training Loss: 7484331983.738281 \t Validation Loss: 1959333555.916016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.51batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 34/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.26batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 \t Training Loss: 7475801175.748047 \t Validation Loss: 1964300327.289062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.66batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 35/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.86batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 \t Training Loss: 7467692047.886719 \t Validation Loss: 1956881319.796875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 5611/5611 [00:35<00:00, 158.87batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 36/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.85batch/s, val_loss=2.1e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 \t Training Loss: 7460261568.947266 \t Validation Loss: 1958333510.912109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 5611/5611 [00:35<00:00, 160.06batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 37/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.79batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 \t Training Loss: 7453677100.523438 \t Validation Loss: 1956583641.402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 5611/5611 [00:35<00:00, 160.16batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 38/50: 100%|██████████| 1403/1403 [00:04<00:00, 322.89batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 \t Training Loss: 7445926707.500000 \t Validation Loss: 1954708698.974609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.45batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 39/50: 100%|██████████| 1403/1403 [00:04<00:00, 326.02batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 \t Training Loss: 7440872381.781250 \t Validation Loss: 1953207224.453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.77batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 40/50: 100%|██████████| 1403/1403 [00:04<00:00, 318.82batch/s, val_loss=2.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 \t Training Loss: 7434917351.289062 \t Validation Loss: 1954929347.902344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.51batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 41/50: 100%|██████████| 1403/1403 [00:04<00:00, 322.66batch/s, val_loss=2.1e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 \t Training Loss: 7430181922.365234 \t Validation Loss: 1951726055.767578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.35batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 42/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.57batch/s, val_loss=2.13e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 \t Training Loss: 7425800962.333984 \t Validation Loss: 1953036712.908203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.62batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 43/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.64batch/s, val_loss=2.09e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 \t Training Loss: 7419729704.003906 \t Validation Loss: 1949863476.744141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.40batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 44/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.99batch/s, val_loss=2.12e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 \t Training Loss: 7415542365.550781 \t Validation Loss: 1951310097.855469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|██████████| 5611/5611 [00:35<00:00, 158.68batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 45/50: 100%|██████████| 1403/1403 [00:04<00:00, 325.53batch/s, val_loss=2.1e+4] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 \t Training Loss: 7412298024.492188 \t Validation Loss: 1954367581.408203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.46batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 46/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.51batch/s, val_loss=2.09e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 \t Training Loss: 7408273024.304688 \t Validation Loss: 1949422819.154297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.65batch/s, train_loss=5.49e+3]\n",
      "Validation Epoch 47/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.79batch/s, val_loss=2.07e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 \t Training Loss: 7403522589.369141 \t Validation Loss: 1947029920.330078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.38batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 48/50: 100%|██████████| 1403/1403 [00:04<00:00, 323.11batch/s, val_loss=2.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 \t Training Loss: 7400103514.699219 \t Validation Loss: 1949740780.662109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|██████████| 5611/5611 [00:34<00:00, 160.76batch/s, train_loss=5.47e+3]\n",
      "Validation Epoch 49/50: 100%|██████████| 1403/1403 [00:04<00:00, 317.55batch/s, val_loss=2.09e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 \t Training Loss: 7396674419.429688 \t Validation Loss: 1950270612.726562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|██████████| 5611/5611 [00:35<00:00, 159.92batch/s, train_loss=5.48e+3]\n",
      "Validation Epoch 50/50: 100%|██████████| 1403/1403 [00:04<00:00, 324.77batch/s, val_loss=2.12e+4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 \t Training Loss: 7394409866.531250 \t Validation Loss: 1952232346.138672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 루프\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # 훈련 데이터 로더에 대한 프로그레스 바 추가\n",
    "    train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    for data, target in train_progress_bar:\n",
    "        data, target = data.to(current_device), target.to(current_device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        # 프로그레스 바 업데이트\n",
    "        train_progress_bar.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    # 검증 데이터 로더에 대한 프로그레스 바 추가\n",
    "    val_progress_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_progress_bar:\n",
    "            data, target = data.to(current_device), target.to(current_device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            # 프로그레스 바 업데이트\n",
    "            val_progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader.dataset))\n",
    "    print(f'Epoch {epoch+1} \\t Training Loss: {train_loss:.6f} \\t Validation Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Learning Rate Scheduler 업데이트\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 가장 좋은 모델 가중치 로드\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model_Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./B01_TransformData_FinalAvatar_20230922_171230.csv').iloc[300:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Time</th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>301</td>\n",
       "      <td>3.516298</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>351.9312</td>\n",
       "      <td>358.1726</td>\n",
       "      <td>282.4885</td>\n",
       "      <td>0.484731</td>\n",
       "      <td>0.254242</td>\n",
       "      <td>1.712456</td>\n",
       "      <td>339.4850</td>\n",
       "      <td>296.8258</td>\n",
       "      <td>305.4126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>302</td>\n",
       "      <td>3.527282</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>350.7178</td>\n",
       "      <td>357.2758</td>\n",
       "      <td>281.4112</td>\n",
       "      <td>0.473791</td>\n",
       "      <td>0.253382</td>\n",
       "      <td>1.724783</td>\n",
       "      <td>337.8150</td>\n",
       "      <td>292.9910</td>\n",
       "      <td>307.2033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>3.538654</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>349.4878</td>\n",
       "      <td>356.3683</td>\n",
       "      <td>280.3726</td>\n",
       "      <td>0.462770</td>\n",
       "      <td>0.253029</td>\n",
       "      <td>1.736980</td>\n",
       "      <td>336.0336</td>\n",
       "      <td>289.2522</td>\n",
       "      <td>308.9090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>304</td>\n",
       "      <td>3.549650</td>\n",
       "      <td>0.169145</td>\n",
       "      <td>0.316027</td>\n",
       "      <td>1.705714</td>\n",
       "      <td>357.288</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>0.112322</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>...</td>\n",
       "      <td>348.2825</td>\n",
       "      <td>355.5042</td>\n",
       "      <td>279.3825</td>\n",
       "      <td>0.452501</td>\n",
       "      <td>0.258765</td>\n",
       "      <td>1.747534</td>\n",
       "      <td>334.1286</td>\n",
       "      <td>285.3405</td>\n",
       "      <td>310.8063</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>3.561161</td>\n",
       "      <td>0.169145</td>\n",
       "      <td>0.316027</td>\n",
       "      <td>1.705714</td>\n",
       "      <td>357.288</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>0.112322</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>...</td>\n",
       "      <td>346.9774</td>\n",
       "      <td>354.4922</td>\n",
       "      <td>278.4650</td>\n",
       "      <td>0.441532</td>\n",
       "      <td>0.259532</td>\n",
       "      <td>1.759929</td>\n",
       "      <td>332.4753</td>\n",
       "      <td>281.4930</td>\n",
       "      <td>312.3235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Frame      Time  m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  \\\n",
       "300    301  3.516298          0.167637          0.309281          1.707612   \n",
       "301    302  3.527282          0.167637          0.309281          1.707612   \n",
       "302    303  3.538654          0.167637          0.309281          1.707612   \n",
       "303    304  3.549650          0.169145          0.316027          1.705714   \n",
       "304    305  3.561161          0.169145          0.316027          1.705714   \n",
       "\n",
       "     m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  m_avg_L_HipPosX  \\\n",
       "300           357.083          356.5699          359.1780         0.110527   \n",
       "301           357.083          356.5699          359.1780         0.110527   \n",
       "302           357.083          356.5699          359.1780         0.110527   \n",
       "303           357.288          355.9712          359.2451         0.112322   \n",
       "304           357.288          355.9712          359.2451         0.112322   \n",
       "\n",
       "     m_avg_L_HipPosY  ...  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "300         0.229474  ...           351.9312           358.1726   \n",
       "301         0.229474  ...           350.7178           357.2758   \n",
       "302         0.229474  ...           349.4878           356.3683   \n",
       "303         0.236200  ...           348.2825           355.5042   \n",
       "304         0.236200  ...           346.9774           354.4922   \n",
       "\n",
       "     m_avg_R_ElbowRotZ  m_avg_R_WristPosX  m_avg_R_WristPosY  \\\n",
       "300           282.4885           0.484731           0.254242   \n",
       "301           281.4112           0.473791           0.253382   \n",
       "302           280.3726           0.462770           0.253029   \n",
       "303           279.3825           0.452501           0.258765   \n",
       "304           278.4650           0.441532           0.259532   \n",
       "\n",
       "     m_avg_R_WristPosZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "300           1.712456           339.4850           296.8258   \n",
       "301           1.724783           337.8150           292.9910   \n",
       "302           1.736980           336.0336           289.2522   \n",
       "303           1.747534           334.1286           285.3405   \n",
       "304           1.759929           332.4753           281.4930   \n",
       "\n",
       "     m_avg_R_WristRotZ  Unnamed: 128  \n",
       "300           305.4126           NaN  \n",
       "301           307.2033           NaN  \n",
       "302           308.9090           NaN  \n",
       "303           310.8063           NaN  \n",
       "304           312.3235           NaN  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rotation_columns = [col for col in test_df.columns if 'Rot' in col]\n",
    "test_rotation_df = test_df[test_rotation_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>m_avg_L_HipRotY</th>\n",
       "      <th>m_avg_L_HipRotZ</th>\n",
       "      <th>m_avg_L_KneeRotX</th>\n",
       "      <th>m_avg_L_KneeRotY</th>\n",
       "      <th>m_avg_L_KneeRotZ</th>\n",
       "      <th>m_avg_L_AnkleRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>357.083000</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>3.120952</td>\n",
       "      <td>2.614980</td>\n",
       "      <td>1.605597</td>\n",
       "      <td>357.2850</td>\n",
       "      <td>2.783687</td>\n",
       "      <td>0.762435</td>\n",
       "      <td>354.665700</td>\n",
       "      <td>...</td>\n",
       "      <td>350.0468</td>\n",
       "      <td>346.61580</td>\n",
       "      <td>356.47380</td>\n",
       "      <td>287.389800</td>\n",
       "      <td>351.93120</td>\n",
       "      <td>358.17260</td>\n",
       "      <td>282.488500</td>\n",
       "      <td>339.4850</td>\n",
       "      <td>296.8258</td>\n",
       "      <td>305.41260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>357.083000</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>3.120952</td>\n",
       "      <td>2.614980</td>\n",
       "      <td>1.605597</td>\n",
       "      <td>357.2850</td>\n",
       "      <td>2.783687</td>\n",
       "      <td>0.762435</td>\n",
       "      <td>354.665700</td>\n",
       "      <td>...</td>\n",
       "      <td>350.0468</td>\n",
       "      <td>345.37230</td>\n",
       "      <td>355.67140</td>\n",
       "      <td>286.328600</td>\n",
       "      <td>350.71780</td>\n",
       "      <td>357.27580</td>\n",
       "      <td>281.411200</td>\n",
       "      <td>337.8150</td>\n",
       "      <td>292.9910</td>\n",
       "      <td>307.20330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>357.083000</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>3.120952</td>\n",
       "      <td>2.614980</td>\n",
       "      <td>1.605597</td>\n",
       "      <td>357.2850</td>\n",
       "      <td>2.783687</td>\n",
       "      <td>0.762435</td>\n",
       "      <td>354.665700</td>\n",
       "      <td>...</td>\n",
       "      <td>350.0468</td>\n",
       "      <td>344.11480</td>\n",
       "      <td>354.85520</td>\n",
       "      <td>285.303200</td>\n",
       "      <td>349.48780</td>\n",
       "      <td>356.36830</td>\n",
       "      <td>280.372600</td>\n",
       "      <td>336.0336</td>\n",
       "      <td>289.2522</td>\n",
       "      <td>308.90900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>357.288000</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>1.685574</td>\n",
       "      <td>5.019152</td>\n",
       "      <td>2.186052</td>\n",
       "      <td>356.7885</td>\n",
       "      <td>5.105362</td>\n",
       "      <td>1.477256</td>\n",
       "      <td>4.062734</td>\n",
       "      <td>...</td>\n",
       "      <td>349.6448</td>\n",
       "      <td>342.88500</td>\n",
       "      <td>354.07880</td>\n",
       "      <td>284.322500</td>\n",
       "      <td>348.28250</td>\n",
       "      <td>355.50420</td>\n",
       "      <td>279.382500</td>\n",
       "      <td>334.1286</td>\n",
       "      <td>285.3405</td>\n",
       "      <td>310.80630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>357.288000</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>1.685574</td>\n",
       "      <td>5.019152</td>\n",
       "      <td>2.186052</td>\n",
       "      <td>356.7885</td>\n",
       "      <td>5.105362</td>\n",
       "      <td>1.477256</td>\n",
       "      <td>4.062734</td>\n",
       "      <td>...</td>\n",
       "      <td>349.6448</td>\n",
       "      <td>341.55870</td>\n",
       "      <td>353.14730</td>\n",
       "      <td>283.414800</td>\n",
       "      <td>346.97740</td>\n",
       "      <td>354.49220</td>\n",
       "      <td>278.465000</td>\n",
       "      <td>332.4753</td>\n",
       "      <td>281.4930</td>\n",
       "      <td>312.32350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>0.638496</td>\n",
       "      <td>355.0141</td>\n",
       "      <td>356.6961</td>\n",
       "      <td>357.393600</td>\n",
       "      <td>4.837928</td>\n",
       "      <td>3.512594</td>\n",
       "      <td>349.2009</td>\n",
       "      <td>4.826385</td>\n",
       "      <td>2.328943</td>\n",
       "      <td>349.104800</td>\n",
       "      <td>...</td>\n",
       "      <td>353.4269</td>\n",
       "      <td>10.94913</td>\n",
       "      <td>21.90927</td>\n",
       "      <td>6.278744</td>\n",
       "      <td>10.26911</td>\n",
       "      <td>27.53492</td>\n",
       "      <td>2.730536</td>\n",
       "      <td>355.6536</td>\n",
       "      <td>355.0251</td>\n",
       "      <td>20.78619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>0.638496</td>\n",
       "      <td>355.0141</td>\n",
       "      <td>356.6961</td>\n",
       "      <td>357.393600</td>\n",
       "      <td>4.837928</td>\n",
       "      <td>3.512594</td>\n",
       "      <td>349.2009</td>\n",
       "      <td>4.826385</td>\n",
       "      <td>2.328943</td>\n",
       "      <td>349.104800</td>\n",
       "      <td>...</td>\n",
       "      <td>353.4269</td>\n",
       "      <td>10.96714</td>\n",
       "      <td>21.80633</td>\n",
       "      <td>6.167922</td>\n",
       "      <td>10.29771</td>\n",
       "      <td>27.43369</td>\n",
       "      <td>2.622310</td>\n",
       "      <td>355.6207</td>\n",
       "      <td>354.8966</td>\n",
       "      <td>20.48832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>0.638496</td>\n",
       "      <td>355.0141</td>\n",
       "      <td>356.6961</td>\n",
       "      <td>357.393600</td>\n",
       "      <td>4.837928</td>\n",
       "      <td>3.512594</td>\n",
       "      <td>349.2009</td>\n",
       "      <td>4.826385</td>\n",
       "      <td>2.328943</td>\n",
       "      <td>349.104800</td>\n",
       "      <td>...</td>\n",
       "      <td>353.4269</td>\n",
       "      <td>10.97428</td>\n",
       "      <td>21.70186</td>\n",
       "      <td>6.112121</td>\n",
       "      <td>10.31019</td>\n",
       "      <td>27.33001</td>\n",
       "      <td>2.567600</td>\n",
       "      <td>355.6002</td>\n",
       "      <td>354.7376</td>\n",
       "      <td>20.17733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>0.646919</td>\n",
       "      <td>355.0108</td>\n",
       "      <td>356.7127</td>\n",
       "      <td>357.546200</td>\n",
       "      <td>5.342571</td>\n",
       "      <td>3.862668</td>\n",
       "      <td>349.4238</td>\n",
       "      <td>5.280097</td>\n",
       "      <td>2.694932</td>\n",
       "      <td>349.240000</td>\n",
       "      <td>...</td>\n",
       "      <td>353.2370</td>\n",
       "      <td>10.99195</td>\n",
       "      <td>21.59484</td>\n",
       "      <td>6.029818</td>\n",
       "      <td>10.33570</td>\n",
       "      <td>27.22439</td>\n",
       "      <td>2.487622</td>\n",
       "      <td>355.6598</td>\n",
       "      <td>354.5097</td>\n",
       "      <td>19.92392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0.646919</td>\n",
       "      <td>355.0108</td>\n",
       "      <td>356.7127</td>\n",
       "      <td>357.546200</td>\n",
       "      <td>5.342571</td>\n",
       "      <td>3.862668</td>\n",
       "      <td>349.4238</td>\n",
       "      <td>5.280097</td>\n",
       "      <td>2.694932</td>\n",
       "      <td>349.240000</td>\n",
       "      <td>...</td>\n",
       "      <td>353.2370</td>\n",
       "      <td>10.99784</td>\n",
       "      <td>21.51480</td>\n",
       "      <td>5.988278</td>\n",
       "      <td>10.34555</td>\n",
       "      <td>27.14502</td>\n",
       "      <td>2.446991</td>\n",
       "      <td>355.7557</td>\n",
       "      <td>354.3958</td>\n",
       "      <td>19.77342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4376 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  m_avg_L_HipRotX  \\\n",
       "300         357.083000          356.5699          359.1780         3.120952   \n",
       "301         357.083000          356.5699          359.1780         3.120952   \n",
       "302         357.083000          356.5699          359.1780         3.120952   \n",
       "303         357.288000          355.9712          359.2451         1.685574   \n",
       "304         357.288000          355.9712          359.2451         1.685574   \n",
       "...                ...               ...               ...              ...   \n",
       "4671          0.638496          355.0141          356.6961       357.393600   \n",
       "4672          0.638496          355.0141          356.6961       357.393600   \n",
       "4673          0.638496          355.0141          356.6961       357.393600   \n",
       "4674          0.646919          355.0108          356.7127       357.546200   \n",
       "4675          0.646919          355.0108          356.7127       357.546200   \n",
       "\n",
       "      m_avg_L_HipRotY  m_avg_L_HipRotZ  m_avg_L_KneeRotX  m_avg_L_KneeRotY  \\\n",
       "300          2.614980         1.605597          357.2850          2.783687   \n",
       "301          2.614980         1.605597          357.2850          2.783687   \n",
       "302          2.614980         1.605597          357.2850          2.783687   \n",
       "303          5.019152         2.186052          356.7885          5.105362   \n",
       "304          5.019152         2.186052          356.7885          5.105362   \n",
       "...               ...              ...               ...               ...   \n",
       "4671         4.837928         3.512594          349.2009          4.826385   \n",
       "4672         4.837928         3.512594          349.2009          4.826385   \n",
       "4673         4.837928         3.512594          349.2009          4.826385   \n",
       "4674         5.342571         3.862668          349.4238          5.280097   \n",
       "4675         5.342571         3.862668          349.4238          5.280097   \n",
       "\n",
       "      m_avg_L_KneeRotZ  m_avg_L_AnkleRotX  ...  m_avg_R_CollarRotZ  \\\n",
       "300           0.762435         354.665700  ...            350.0468   \n",
       "301           0.762435         354.665700  ...            350.0468   \n",
       "302           0.762435         354.665700  ...            350.0468   \n",
       "303           1.477256           4.062734  ...            349.6448   \n",
       "304           1.477256           4.062734  ...            349.6448   \n",
       "...                ...                ...  ...                 ...   \n",
       "4671          2.328943         349.104800  ...            353.4269   \n",
       "4672          2.328943         349.104800  ...            353.4269   \n",
       "4673          2.328943         349.104800  ...            353.4269   \n",
       "4674          2.694932         349.240000  ...            353.2370   \n",
       "4675          2.694932         349.240000  ...            353.2370   \n",
       "\n",
       "      m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "300              346.61580             356.47380            287.389800   \n",
       "301              345.37230             355.67140            286.328600   \n",
       "302              344.11480             354.85520            285.303200   \n",
       "303              342.88500             354.07880            284.322500   \n",
       "304              341.55870             353.14730            283.414800   \n",
       "...                    ...                   ...                   ...   \n",
       "4671              10.94913              21.90927              6.278744   \n",
       "4672              10.96714              21.80633              6.167922   \n",
       "4673              10.97428              21.70186              6.112121   \n",
       "4674              10.99195              21.59484              6.029818   \n",
       "4675              10.99784              21.51480              5.988278   \n",
       "\n",
       "      m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "300           351.93120          358.17260         282.488500   \n",
       "301           350.71780          357.27580         281.411200   \n",
       "302           349.48780          356.36830         280.372600   \n",
       "303           348.28250          355.50420         279.382500   \n",
       "304           346.97740          354.49220         278.465000   \n",
       "...                 ...                ...                ...   \n",
       "4671           10.26911           27.53492           2.730536   \n",
       "4672           10.29771           27.43369           2.622310   \n",
       "4673           10.31019           27.33001           2.567600   \n",
       "4674           10.33570           27.22439           2.487622   \n",
       "4675           10.34555           27.14502           2.446991   \n",
       "\n",
       "      m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "300            339.4850           296.8258          305.41260  \n",
       "301            337.8150           292.9910          307.20330  \n",
       "302            336.0336           289.2522          308.90900  \n",
       "303            334.1286           285.3405          310.80630  \n",
       "304            332.4753           281.4930          312.32350  \n",
       "...                 ...                ...                ...  \n",
       "4671           355.6536           355.0251           20.78619  \n",
       "4672           355.6207           354.8966           20.48832  \n",
       "4673           355.6002           354.7376           20.17733  \n",
       "4674           355.6598           354.5097           19.92392  \n",
       "4675           355.7557           354.3958           19.77342  \n",
       "\n",
       "[4376 rows x 63 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-180~180 사이로 정규화\n",
    "def normalize_angle(x):\n",
    "    x = np.where(x > 180, x - 360, x)\n",
    "    x = np.where(x < -180, x + 360, x)\n",
    "    return x\n",
    "test_df[test_rotation_columns] = test_df[test_rotation_columns].apply(normalize_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_rotation_columns].map(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>m_avg_L_HipRotY</th>\n",
       "      <th>m_avg_L_HipRotZ</th>\n",
       "      <th>m_avg_L_KneeRotX</th>\n",
       "      <th>m_avg_L_KneeRotY</th>\n",
       "      <th>m_avg_L_KneeRotZ</th>\n",
       "      <th>m_avg_L_AnkleRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>-3.43</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-13.38</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-72.61</td>\n",
       "      <td>-8.07</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-77.51</td>\n",
       "      <td>-20.51</td>\n",
       "      <td>-63.17</td>\n",
       "      <td>-54.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>-3.43</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-14.63</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>-73.67</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-78.59</td>\n",
       "      <td>-22.19</td>\n",
       "      <td>-67.01</td>\n",
       "      <td>-52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>-2.92</td>\n",
       "      <td>-3.43</td>\n",
       "      <td>-0.82</td>\n",
       "      <td>3.12</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.61</td>\n",
       "      <td>-2.71</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.76</td>\n",
       "      <td>-5.33</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-15.89</td>\n",
       "      <td>-5.14</td>\n",
       "      <td>-74.70</td>\n",
       "      <td>-10.51</td>\n",
       "      <td>-3.63</td>\n",
       "      <td>-79.63</td>\n",
       "      <td>-23.97</td>\n",
       "      <td>-70.75</td>\n",
       "      <td>-51.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>-2.71</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.69</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-3.21</td>\n",
       "      <td>5.11</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.36</td>\n",
       "      <td>-17.12</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>-75.68</td>\n",
       "      <td>-11.72</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>-80.62</td>\n",
       "      <td>-25.87</td>\n",
       "      <td>-74.66</td>\n",
       "      <td>-49.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-2.71</td>\n",
       "      <td>-4.03</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>1.69</td>\n",
       "      <td>5.02</td>\n",
       "      <td>2.19</td>\n",
       "      <td>-3.21</td>\n",
       "      <td>5.11</td>\n",
       "      <td>1.48</td>\n",
       "      <td>4.06</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.36</td>\n",
       "      <td>-18.44</td>\n",
       "      <td>-6.85</td>\n",
       "      <td>-76.59</td>\n",
       "      <td>-13.02</td>\n",
       "      <td>-5.51</td>\n",
       "      <td>-81.54</td>\n",
       "      <td>-27.52</td>\n",
       "      <td>-78.51</td>\n",
       "      <td>-47.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>3.51</td>\n",
       "      <td>-10.80</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-10.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.95</td>\n",
       "      <td>21.91</td>\n",
       "      <td>6.28</td>\n",
       "      <td>10.27</td>\n",
       "      <td>27.53</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-4.35</td>\n",
       "      <td>-4.97</td>\n",
       "      <td>20.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>3.51</td>\n",
       "      <td>-10.80</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-10.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.97</td>\n",
       "      <td>21.81</td>\n",
       "      <td>6.17</td>\n",
       "      <td>10.30</td>\n",
       "      <td>27.43</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>-5.10</td>\n",
       "      <td>20.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>0.64</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.30</td>\n",
       "      <td>-2.61</td>\n",
       "      <td>4.84</td>\n",
       "      <td>3.51</td>\n",
       "      <td>-10.80</td>\n",
       "      <td>4.83</td>\n",
       "      <td>2.33</td>\n",
       "      <td>-10.90</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.97</td>\n",
       "      <td>21.70</td>\n",
       "      <td>6.11</td>\n",
       "      <td>10.31</td>\n",
       "      <td>27.33</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>20.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>0.65</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>5.34</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-10.58</td>\n",
       "      <td>5.28</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-10.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.76</td>\n",
       "      <td>10.99</td>\n",
       "      <td>21.59</td>\n",
       "      <td>6.03</td>\n",
       "      <td>10.34</td>\n",
       "      <td>27.22</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-4.34</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>19.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0.65</td>\n",
       "      <td>-4.99</td>\n",
       "      <td>-3.29</td>\n",
       "      <td>-2.45</td>\n",
       "      <td>5.34</td>\n",
       "      <td>3.86</td>\n",
       "      <td>-10.58</td>\n",
       "      <td>5.28</td>\n",
       "      <td>2.69</td>\n",
       "      <td>-10.76</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.76</td>\n",
       "      <td>11.00</td>\n",
       "      <td>21.51</td>\n",
       "      <td>5.99</td>\n",
       "      <td>10.35</td>\n",
       "      <td>27.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4376 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  m_avg_L_HipRotX  \\\n",
       "300              -2.92             -3.43             -0.82             3.12   \n",
       "301              -2.92             -3.43             -0.82             3.12   \n",
       "302              -2.92             -3.43             -0.82             3.12   \n",
       "303              -2.71             -4.03             -0.75             1.69   \n",
       "304              -2.71             -4.03             -0.75             1.69   \n",
       "...                ...               ...               ...              ...   \n",
       "4671              0.64             -4.99             -3.30            -2.61   \n",
       "4672              0.64             -4.99             -3.30            -2.61   \n",
       "4673              0.64             -4.99             -3.30            -2.61   \n",
       "4674              0.65             -4.99             -3.29            -2.45   \n",
       "4675              0.65             -4.99             -3.29            -2.45   \n",
       "\n",
       "      m_avg_L_HipRotY  m_avg_L_HipRotZ  m_avg_L_KneeRotX  m_avg_L_KneeRotY  \\\n",
       "300              2.61             1.61             -2.71              2.78   \n",
       "301              2.61             1.61             -2.71              2.78   \n",
       "302              2.61             1.61             -2.71              2.78   \n",
       "303              5.02             2.19             -3.21              5.11   \n",
       "304              5.02             2.19             -3.21              5.11   \n",
       "...               ...              ...               ...               ...   \n",
       "4671             4.84             3.51            -10.80              4.83   \n",
       "4672             4.84             3.51            -10.80              4.83   \n",
       "4673             4.84             3.51            -10.80              4.83   \n",
       "4674             5.34             3.86            -10.58              5.28   \n",
       "4675             5.34             3.86            -10.58              5.28   \n",
       "\n",
       "      m_avg_L_KneeRotZ  m_avg_L_AnkleRotX  ...  m_avg_R_CollarRotZ  \\\n",
       "300               0.76              -5.33  ...               -9.95   \n",
       "301               0.76              -5.33  ...               -9.95   \n",
       "302               0.76              -5.33  ...               -9.95   \n",
       "303               1.48               4.06  ...              -10.36   \n",
       "304               1.48               4.06  ...              -10.36   \n",
       "...                ...                ...  ...                 ...   \n",
       "4671              2.33             -10.90  ...               -6.57   \n",
       "4672              2.33             -10.90  ...               -6.57   \n",
       "4673              2.33             -10.90  ...               -6.57   \n",
       "4674              2.69             -10.76  ...               -6.76   \n",
       "4675              2.69             -10.76  ...               -6.76   \n",
       "\n",
       "      m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "300                 -13.38                 -3.53                -72.61   \n",
       "301                 -14.63                 -4.33                -73.67   \n",
       "302                 -15.89                 -5.14                -74.70   \n",
       "303                 -17.12                 -5.92                -75.68   \n",
       "304                 -18.44                 -6.85                -76.59   \n",
       "...                    ...                   ...                   ...   \n",
       "4671                 10.95                 21.91                  6.28   \n",
       "4672                 10.97                 21.81                  6.17   \n",
       "4673                 10.97                 21.70                  6.11   \n",
       "4674                 10.99                 21.59                  6.03   \n",
       "4675                 11.00                 21.51                  5.99   \n",
       "\n",
       "      m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "300               -8.07              -1.83             -77.51   \n",
       "301               -9.28              -2.72             -78.59   \n",
       "302              -10.51              -3.63             -79.63   \n",
       "303              -11.72              -4.50             -80.62   \n",
       "304              -13.02              -5.51             -81.54   \n",
       "...                 ...                ...                ...   \n",
       "4671              10.27              27.53               2.73   \n",
       "4672              10.30              27.43               2.62   \n",
       "4673              10.31              27.33               2.57   \n",
       "4674              10.34              27.22               2.49   \n",
       "4675              10.35              27.15               2.45   \n",
       "\n",
       "      m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "300              -20.51             -63.17             -54.59  \n",
       "301              -22.19             -67.01             -52.80  \n",
       "302              -23.97             -70.75             -51.09  \n",
       "303              -25.87             -74.66             -49.19  \n",
       "304              -27.52             -78.51             -47.68  \n",
       "...                 ...                ...                ...  \n",
       "4671              -4.35              -4.97              20.79  \n",
       "4672              -4.38              -5.10              20.49  \n",
       "4673              -4.40              -5.26              20.18  \n",
       "4674              -4.34              -5.49              19.92  \n",
       "4675              -4.24              -5.60              19.77  \n",
       "\n",
       "[4376 rows x 63 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어나는 값이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = (test_df > 180).sum().sum() + (test_df < -180).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4376, 63)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 새로운 CSV 파일로 저장합니다.\n",
    "test_df.to_csv('./test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 값을 넣을 빈 리스트\n",
    "test_predictions = []\n",
    "\n",
    "# 훈련 데이터셋에서 마지막 입력 개수의 값을 가져온 후\n",
    "current_batch = torch.from_numpy(test_df[-n_input:].values.astype(np.float32)).reshape((1, n_input, n_features))\n",
    "\n",
    "# 모델이 사용하는 디바이스를 확인하고 데이터를 해당 디바이스로 옮깁니다.\n",
    "current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "current_batch = current_batch.to(current_device)\n",
    "\n",
    "# 예측 과정 반복\n",
    "with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "    for i in range(1):\n",
    "        # 현재 배치에서 다음 포인트를 예측\n",
    "        current_pred = model(current_batch).cpu().numpy()[0]  # 마지막 시퀀스 포인트 예측\n",
    "        current_pred = np.array([normalize_angle(y) for y in current_pred])  # 예측값 정규화\n",
    "\n",
    "        # 예측된 마지막 프레임을 리스트에 추가\n",
    "        test_predictions.append(current_pred)\n",
    "\n",
    "        # 새로운 배치 생성: 마지막 시퀀스 제외하고 예측값 추가\n",
    "        current_batch = np.roll(current_batch.cpu().numpy(), -1, axis=1)\n",
    "        current_batch[:, -1, :] = current_pred\n",
    "        current_batch = torch.from_numpy(current_batch).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ -8.36271858,   1.52798653,  -1.83110571,  -3.74713039,\n",
       "         12.55899525,   0.33136725,  -9.40380287,  10.47586155,\n",
       "          1.12912226,  -2.07923746,  -0.13644487,   1.17281783,\n",
       "         -1.43221653, -10.75444126,  15.16151237,  -6.55329561,\n",
       "        -12.99492455,  -1.58172917,  -7.76797438, -13.77939224,\n",
       "         -2.51179767,  -0.82390171,  13.75622082,  -3.02825952,\n",
       "          2.84747791,   9.16064453,  -1.314327  ,  -7.43386412,\n",
       "          1.3347106 ,  -2.05864644,  -0.15528074,   4.75243902,\n",
       "         -1.83560479,  -2.52064061,   8.74391842,   4.30780983,\n",
       "         -3.98666906,  23.04104805,  -2.93955469,  -4.88390303,\n",
       "         16.15159798, -12.23924923,  -6.18827677,  24.94114494,\n",
       "         17.30404854,  -4.27114582,   4.57458782,  -1.15341043,\n",
       "          5.21155787,   1.88073635,  -2.44791222,  -3.30967641,\n",
       "        -11.17351341,  -9.02195168,  -0.69701171, -17.34079552,\n",
       "         -5.11593199,  -2.41322565,   0.56683505,  -0.31204009,\n",
       "         -2.93524289, -14.80333996,  21.59170723])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_array = np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 리스트를 데이터프레임으로 변환합니다.\n",
    "test_predictions = pd.DataFrame(test_predictions_array)\n",
    "\n",
    "# test_predictions 데이터프레임을 CSV 파일로 저장합니다.\n",
    "test_predictions.to_csv('./test_predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-8.362719</td>\n",
       "      <td>1.527987</td>\n",
       "      <td>-1.831106</td>\n",
       "      <td>-3.74713</td>\n",
       "      <td>12.558995</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>-9.403803</td>\n",
       "      <td>10.475862</td>\n",
       "      <td>1.129122</td>\n",
       "      <td>-2.079237</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.021952</td>\n",
       "      <td>-0.697012</td>\n",
       "      <td>-17.340796</td>\n",
       "      <td>-5.115932</td>\n",
       "      <td>-2.413226</td>\n",
       "      <td>0.566835</td>\n",
       "      <td>-0.31204</td>\n",
       "      <td>-2.935243</td>\n",
       "      <td>-14.80334</td>\n",
       "      <td>21.591707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2        3          4         5         6   \\\n",
       "0 -8.362719  1.527987 -1.831106 -3.74713  12.558995  0.331367 -9.403803   \n",
       "\n",
       "          7         8         9   ...        53        54         55  \\\n",
       "0  10.475862  1.129122 -2.079237  ... -9.021952 -0.697012 -17.340796   \n",
       "\n",
       "         56        57        58       59        60        61         62  \n",
       "0 -5.115932 -2.413226  0.566835 -0.31204 -2.935243 -14.80334  21.591707  \n",
       "\n",
       "[1 rows x 63 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 데이터(test_df)의 마지막 30 프레임과 \n",
    "last_inputs_df = test_df.iloc[-30:][column_order].reset_index(drop=True)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "test_combined_df = pd.concat([last_inputs_df, test_predictions_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>m_avg_L_HipRotY</th>\n",
       "      <th>m_avg_L_HipRotZ</th>\n",
       "      <th>m_avg_L_KneeRotX</th>\n",
       "      <th>m_avg_L_KneeRotY</th>\n",
       "      <th>m_avg_L_KneeRotZ</th>\n",
       "      <th>m_avg_L_AnkleRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.290000</td>\n",
       "      <td>-2.860000</td>\n",
       "      <td>-1.520000</td>\n",
       "      <td>-2.88000</td>\n",
       "      <td>9.890000</td>\n",
       "      <td>4.190000</td>\n",
       "      <td>-19.270000</td>\n",
       "      <td>9.840000</td>\n",
       "      <td>1.820000</td>\n",
       "      <td>-23.310000</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.950000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>9.240000</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>5.80000</td>\n",
       "      <td>-1.590000</td>\n",
       "      <td>-1.08000</td>\n",
       "      <td>28.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-10.840000</td>\n",
       "      <td>-133.450000</td>\n",
       "      <td>-3.480000</td>\n",
       "      <td>-10.59000</td>\n",
       "      <td>-83.650000</td>\n",
       "      <td>19.510000</td>\n",
       "      <td>-38.960000</td>\n",
       "      <td>-92.770000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>-37.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.190000</td>\n",
       "      <td>25.240000</td>\n",
       "      <td>9.640000</td>\n",
       "      <td>9.190000</td>\n",
       "      <td>30.800000</td>\n",
       "      <td>5.99000</td>\n",
       "      <td>-1.950000</td>\n",
       "      <td>-1.22000</td>\n",
       "      <td>28.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-10.840000</td>\n",
       "      <td>-133.450000</td>\n",
       "      <td>-3.480000</td>\n",
       "      <td>-10.59000</td>\n",
       "      <td>-83.650000</td>\n",
       "      <td>19.510000</td>\n",
       "      <td>-38.960000</td>\n",
       "      <td>-92.770000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>-37.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.210000</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>30.790000</td>\n",
       "      <td>6.05000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>-1.39000</td>\n",
       "      <td>28.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-10.840000</td>\n",
       "      <td>-133.450000</td>\n",
       "      <td>-3.480000</td>\n",
       "      <td>-10.59000</td>\n",
       "      <td>-83.650000</td>\n",
       "      <td>19.510000</td>\n",
       "      <td>-38.960000</td>\n",
       "      <td>-92.770000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>-37.730000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>25.190000</td>\n",
       "      <td>9.740000</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>6.09000</td>\n",
       "      <td>-2.550000</td>\n",
       "      <td>-1.55000</td>\n",
       "      <td>28.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.070000</td>\n",
       "      <td>-4.730000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.37000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>-20.960000</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-21.270000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>9.240000</td>\n",
       "      <td>30.680000</td>\n",
       "      <td>6.06000</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>-1.71000</td>\n",
       "      <td>28.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.070000</td>\n",
       "      <td>-4.730000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.37000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>-20.960000</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-21.270000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>25.040000</td>\n",
       "      <td>9.660000</td>\n",
       "      <td>9.270000</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>6.02000</td>\n",
       "      <td>-3.260000</td>\n",
       "      <td>-1.87000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.070000</td>\n",
       "      <td>-4.730000</td>\n",
       "      <td>-2.910000</td>\n",
       "      <td>-1.37000</td>\n",
       "      <td>2.790000</td>\n",
       "      <td>3.020000</td>\n",
       "      <td>-20.960000</td>\n",
       "      <td>3.240000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-21.270000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>24.950000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>9.310000</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>5.94000</td>\n",
       "      <td>-3.590000</td>\n",
       "      <td>-2.08000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>-4.650000</td>\n",
       "      <td>-3.010000</td>\n",
       "      <td>-1.52000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>-17.180000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>-13.930000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.350000</td>\n",
       "      <td>24.770000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>9.370000</td>\n",
       "      <td>30.330000</td>\n",
       "      <td>5.77000</td>\n",
       "      <td>-3.910000</td>\n",
       "      <td>-2.33000</td>\n",
       "      <td>28.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>-4.650000</td>\n",
       "      <td>-3.010000</td>\n",
       "      <td>-1.52000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>-17.180000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>-13.930000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>24.610000</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>5.61000</td>\n",
       "      <td>-4.220000</td>\n",
       "      <td>-2.60000</td>\n",
       "      <td>27.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.830000</td>\n",
       "      <td>-4.650000</td>\n",
       "      <td>-3.010000</td>\n",
       "      <td>-1.52000</td>\n",
       "      <td>3.960000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>-17.180000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>-13.930000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.440000</td>\n",
       "      <td>24.460000</td>\n",
       "      <td>9.050000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>30.030000</td>\n",
       "      <td>5.43000</td>\n",
       "      <td>-4.470000</td>\n",
       "      <td>-2.94000</td>\n",
       "      <td>27.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.790000</td>\n",
       "      <td>-1.87000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>-14.320000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>-11.220000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.510000</td>\n",
       "      <td>24.260000</td>\n",
       "      <td>8.810000</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>5.20000</td>\n",
       "      <td>-4.780000</td>\n",
       "      <td>-3.22000</td>\n",
       "      <td>26.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.790000</td>\n",
       "      <td>-1.87000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>-14.320000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>-11.220000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.560000</td>\n",
       "      <td>24.080000</td>\n",
       "      <td>8.630000</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>5.02000</td>\n",
       "      <td>-5.040000</td>\n",
       "      <td>-3.48000</td>\n",
       "      <td>25.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.770000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.790000</td>\n",
       "      <td>-1.87000</td>\n",
       "      <td>4.040000</td>\n",
       "      <td>2.920000</td>\n",
       "      <td>-14.320000</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>1.090000</td>\n",
       "      <td>-11.220000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.610000</td>\n",
       "      <td>23.890000</td>\n",
       "      <td>8.380000</td>\n",
       "      <td>9.730000</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>4.79000</td>\n",
       "      <td>-5.260000</td>\n",
       "      <td>-3.68000</td>\n",
       "      <td>25.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.590000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.26000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.140000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>-9.720000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.660000</td>\n",
       "      <td>23.700000</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>29.300000</td>\n",
       "      <td>4.57000</td>\n",
       "      <td>-5.220000</td>\n",
       "      <td>-3.75000</td>\n",
       "      <td>24.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.590000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.26000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.140000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>-9.720000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>9.860000</td>\n",
       "      <td>29.110000</td>\n",
       "      <td>4.35000</td>\n",
       "      <td>-5.230000</td>\n",
       "      <td>-3.91000</td>\n",
       "      <td>24.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.590000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.26000</td>\n",
       "      <td>4.020000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-13.000000</td>\n",
       "      <td>4.140000</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>-9.720000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>23.330000</td>\n",
       "      <td>7.720000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>28.930000</td>\n",
       "      <td>4.14000</td>\n",
       "      <td>-5.030000</td>\n",
       "      <td>-4.06000</td>\n",
       "      <td>23.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.28000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-12.880000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>-9.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.770000</td>\n",
       "      <td>23.140000</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>9.970000</td>\n",
       "      <td>28.740000</td>\n",
       "      <td>3.94000</td>\n",
       "      <td>-4.810000</td>\n",
       "      <td>-4.18000</td>\n",
       "      <td>23.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.28000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-12.880000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>-9.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>10.020000</td>\n",
       "      <td>28.570000</td>\n",
       "      <td>3.76000</td>\n",
       "      <td>-4.670000</td>\n",
       "      <td>-4.34000</td>\n",
       "      <td>22.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.720000</td>\n",
       "      <td>-4.600000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.28000</td>\n",
       "      <td>4.010000</td>\n",
       "      <td>2.960000</td>\n",
       "      <td>-12.880000</td>\n",
       "      <td>4.130000</td>\n",
       "      <td>1.410000</td>\n",
       "      <td>-9.580000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.830000</td>\n",
       "      <td>22.780000</td>\n",
       "      <td>7.150000</td>\n",
       "      <td>10.070000</td>\n",
       "      <td>28.390000</td>\n",
       "      <td>3.59000</td>\n",
       "      <td>-4.520000</td>\n",
       "      <td>-4.50000</td>\n",
       "      <td>22.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.710000</td>\n",
       "      <td>-4.610000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.29000</td>\n",
       "      <td>4.030000</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>-12.850000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>-9.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.860000</td>\n",
       "      <td>22.620000</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>10.110000</td>\n",
       "      <td>28.230000</td>\n",
       "      <td>3.43000</td>\n",
       "      <td>-4.450000</td>\n",
       "      <td>-4.62000</td>\n",
       "      <td>21.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.710000</td>\n",
       "      <td>-4.610000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.29000</td>\n",
       "      <td>4.030000</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>-12.850000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>-9.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.880000</td>\n",
       "      <td>22.460000</td>\n",
       "      <td>6.840000</td>\n",
       "      <td>10.150000</td>\n",
       "      <td>28.080000</td>\n",
       "      <td>3.28000</td>\n",
       "      <td>-4.400000</td>\n",
       "      <td>-4.69000</td>\n",
       "      <td>21.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.710000</td>\n",
       "      <td>-4.610000</td>\n",
       "      <td>-2.690000</td>\n",
       "      <td>-2.29000</td>\n",
       "      <td>4.030000</td>\n",
       "      <td>2.970000</td>\n",
       "      <td>-12.850000</td>\n",
       "      <td>4.150000</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>-9.550000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>6.710000</td>\n",
       "      <td>10.180000</td>\n",
       "      <td>27.940000</td>\n",
       "      <td>3.15000</td>\n",
       "      <td>-4.330000</td>\n",
       "      <td>-4.75000</td>\n",
       "      <td>21.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>-5.080000</td>\n",
       "      <td>-3.150000</td>\n",
       "      <td>-2.55000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>-11.410000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.910000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>27.820000</td>\n",
       "      <td>3.03000</td>\n",
       "      <td>-4.290000</td>\n",
       "      <td>-4.81000</td>\n",
       "      <td>20.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>-5.080000</td>\n",
       "      <td>-3.150000</td>\n",
       "      <td>-2.55000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>-11.410000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.930000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>6.460000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>27.730000</td>\n",
       "      <td>2.91000</td>\n",
       "      <td>-4.300000</td>\n",
       "      <td>-4.87000</td>\n",
       "      <td>20.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.670000</td>\n",
       "      <td>-5.080000</td>\n",
       "      <td>-3.150000</td>\n",
       "      <td>-2.55000</td>\n",
       "      <td>4.250000</td>\n",
       "      <td>3.190000</td>\n",
       "      <td>-11.410000</td>\n",
       "      <td>4.290000</td>\n",
       "      <td>1.910000</td>\n",
       "      <td>-10.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.940000</td>\n",
       "      <td>22.020000</td>\n",
       "      <td>6.360000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>27.640000</td>\n",
       "      <td>2.81000</td>\n",
       "      <td>-4.320000</td>\n",
       "      <td>-4.91000</td>\n",
       "      <td>20.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>-4.990000</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>-2.61000</td>\n",
       "      <td>4.840000</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>-10.800000</td>\n",
       "      <td>4.830000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>-10.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.950000</td>\n",
       "      <td>21.910000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>27.530000</td>\n",
       "      <td>2.73000</td>\n",
       "      <td>-4.350000</td>\n",
       "      <td>-4.97000</td>\n",
       "      <td>20.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>-4.990000</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>-2.61000</td>\n",
       "      <td>4.840000</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>-10.800000</td>\n",
       "      <td>4.830000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>-10.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.970000</td>\n",
       "      <td>21.810000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>27.430000</td>\n",
       "      <td>2.62000</td>\n",
       "      <td>-4.380000</td>\n",
       "      <td>-5.10000</td>\n",
       "      <td>20.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.640000</td>\n",
       "      <td>-4.990000</td>\n",
       "      <td>-3.300000</td>\n",
       "      <td>-2.61000</td>\n",
       "      <td>4.840000</td>\n",
       "      <td>3.510000</td>\n",
       "      <td>-10.800000</td>\n",
       "      <td>4.830000</td>\n",
       "      <td>2.330000</td>\n",
       "      <td>-10.900000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.970000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>6.110000</td>\n",
       "      <td>10.310000</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>2.57000</td>\n",
       "      <td>-4.400000</td>\n",
       "      <td>-5.26000</td>\n",
       "      <td>20.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>-4.990000</td>\n",
       "      <td>-3.290000</td>\n",
       "      <td>-2.45000</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>-10.580000</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>-10.760000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.760000</td>\n",
       "      <td>10.990000</td>\n",
       "      <td>21.590000</td>\n",
       "      <td>6.030000</td>\n",
       "      <td>10.340000</td>\n",
       "      <td>27.220000</td>\n",
       "      <td>2.49000</td>\n",
       "      <td>-4.340000</td>\n",
       "      <td>-5.49000</td>\n",
       "      <td>19.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.650000</td>\n",
       "      <td>-4.990000</td>\n",
       "      <td>-3.290000</td>\n",
       "      <td>-2.45000</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>3.860000</td>\n",
       "      <td>-10.580000</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>2.690000</td>\n",
       "      <td>-10.760000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.760000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>21.510000</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>10.350000</td>\n",
       "      <td>27.150000</td>\n",
       "      <td>2.45000</td>\n",
       "      <td>-4.240000</td>\n",
       "      <td>-5.60000</td>\n",
       "      <td>19.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-8.362719</td>\n",
       "      <td>1.527987</td>\n",
       "      <td>-1.831106</td>\n",
       "      <td>-3.74713</td>\n",
       "      <td>12.558995</td>\n",
       "      <td>0.331367</td>\n",
       "      <td>-9.403803</td>\n",
       "      <td>10.475862</td>\n",
       "      <td>1.129122</td>\n",
       "      <td>-2.079237</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.021952</td>\n",
       "      <td>-0.697012</td>\n",
       "      <td>-17.340796</td>\n",
       "      <td>-5.115932</td>\n",
       "      <td>-2.413226</td>\n",
       "      <td>0.566835</td>\n",
       "      <td>-0.31204</td>\n",
       "      <td>-2.935243</td>\n",
       "      <td>-14.80334</td>\n",
       "      <td>21.591707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 63 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  m_avg_L_HipRotX  \\\n",
       "0           1.290000         -2.860000         -1.520000         -2.88000   \n",
       "1         -10.840000       -133.450000         -3.480000        -10.59000   \n",
       "2         -10.840000       -133.450000         -3.480000        -10.59000   \n",
       "3         -10.840000       -133.450000         -3.480000        -10.59000   \n",
       "4           1.070000         -4.730000         -2.910000         -1.37000   \n",
       "5           1.070000         -4.730000         -2.910000         -1.37000   \n",
       "6           1.070000         -4.730000         -2.910000         -1.37000   \n",
       "7           0.830000         -4.650000         -3.010000         -1.52000   \n",
       "8           0.830000         -4.650000         -3.010000         -1.52000   \n",
       "9           0.830000         -4.650000         -3.010000         -1.52000   \n",
       "10          0.770000         -4.600000         -2.790000         -1.87000   \n",
       "11          0.770000         -4.600000         -2.790000         -1.87000   \n",
       "12          0.770000         -4.600000         -2.790000         -1.87000   \n",
       "13          0.720000         -4.590000         -2.690000         -2.26000   \n",
       "14          0.720000         -4.590000         -2.690000         -2.26000   \n",
       "15          0.720000         -4.590000         -2.690000         -2.26000   \n",
       "16          0.720000         -4.600000         -2.690000         -2.28000   \n",
       "17          0.720000         -4.600000         -2.690000         -2.28000   \n",
       "18          0.720000         -4.600000         -2.690000         -2.28000   \n",
       "19          0.710000         -4.610000         -2.690000         -2.29000   \n",
       "20          0.710000         -4.610000         -2.690000         -2.29000   \n",
       "21          0.710000         -4.610000         -2.690000         -2.29000   \n",
       "22          0.670000         -5.080000         -3.150000         -2.55000   \n",
       "23          0.670000         -5.080000         -3.150000         -2.55000   \n",
       "24          0.670000         -5.080000         -3.150000         -2.55000   \n",
       "25          0.640000         -4.990000         -3.300000         -2.61000   \n",
       "26          0.640000         -4.990000         -3.300000         -2.61000   \n",
       "27          0.640000         -4.990000         -3.300000         -2.61000   \n",
       "28          0.650000         -4.990000         -3.290000         -2.45000   \n",
       "29          0.650000         -4.990000         -3.290000         -2.45000   \n",
       "30         -8.362719          1.527987         -1.831106         -3.74713   \n",
       "\n",
       "    m_avg_L_HipRotY  m_avg_L_HipRotZ  m_avg_L_KneeRotX  m_avg_L_KneeRotY  \\\n",
       "0          9.890000         4.190000        -19.270000          9.840000   \n",
       "1        -83.650000        19.510000        -38.960000        -92.770000   \n",
       "2        -83.650000        19.510000        -38.960000        -92.770000   \n",
       "3        -83.650000        19.510000        -38.960000        -92.770000   \n",
       "4          2.790000         3.020000        -20.960000          3.240000   \n",
       "5          2.790000         3.020000        -20.960000          3.240000   \n",
       "6          2.790000         3.020000        -20.960000          3.240000   \n",
       "7          3.960000         2.810000        -17.180000          4.290000   \n",
       "8          3.960000         2.810000        -17.180000          4.290000   \n",
       "9          3.960000         2.810000        -17.180000          4.290000   \n",
       "10         4.040000         2.920000        -14.320000          4.220000   \n",
       "11         4.040000         2.920000        -14.320000          4.220000   \n",
       "12         4.040000         2.920000        -14.320000          4.220000   \n",
       "13         4.020000         2.960000        -13.000000          4.140000   \n",
       "14         4.020000         2.960000        -13.000000          4.140000   \n",
       "15         4.020000         2.960000        -13.000000          4.140000   \n",
       "16         4.010000         2.960000        -12.880000          4.130000   \n",
       "17         4.010000         2.960000        -12.880000          4.130000   \n",
       "18         4.010000         2.960000        -12.880000          4.130000   \n",
       "19         4.030000         2.970000        -12.850000          4.150000   \n",
       "20         4.030000         2.970000        -12.850000          4.150000   \n",
       "21         4.030000         2.970000        -12.850000          4.150000   \n",
       "22         4.250000         3.190000        -11.410000          4.290000   \n",
       "23         4.250000         3.190000        -11.410000          4.290000   \n",
       "24         4.250000         3.190000        -11.410000          4.290000   \n",
       "25         4.840000         3.510000        -10.800000          4.830000   \n",
       "26         4.840000         3.510000        -10.800000          4.830000   \n",
       "27         4.840000         3.510000        -10.800000          4.830000   \n",
       "28         5.340000         3.860000        -10.580000          5.280000   \n",
       "29         5.340000         3.860000        -10.580000          5.280000   \n",
       "30        12.558995         0.331367         -9.403803         10.475862   \n",
       "\n",
       "    m_avg_L_KneeRotZ  m_avg_L_AnkleRotX  ...  m_avg_R_CollarRotZ  \\\n",
       "0           1.820000         -23.310000  ...          -22.950000   \n",
       "1          19.100000         -37.730000  ...           -9.760000   \n",
       "2          19.100000         -37.730000  ...           -9.760000   \n",
       "3          19.100000         -37.730000  ...           -9.760000   \n",
       "4           0.080000         -21.270000  ...          -17.430000   \n",
       "5           0.080000         -21.270000  ...          -17.430000   \n",
       "6           0.080000         -21.270000  ...          -17.430000   \n",
       "7           0.480000         -13.930000  ...          -12.610000   \n",
       "8           0.480000         -13.930000  ...          -12.610000   \n",
       "9           0.480000         -13.930000  ...          -12.610000   \n",
       "10          1.090000         -11.220000  ...          -12.110000   \n",
       "11          1.090000         -11.220000  ...          -12.110000   \n",
       "12          1.090000         -11.220000  ...          -12.110000   \n",
       "13          1.390000          -9.720000  ...          -11.560000   \n",
       "14          1.390000          -9.720000  ...          -11.560000   \n",
       "15          1.390000          -9.720000  ...          -11.560000   \n",
       "16          1.410000          -9.580000  ...          -11.550000   \n",
       "17          1.410000          -9.580000  ...          -11.550000   \n",
       "18          1.410000          -9.580000  ...          -11.550000   \n",
       "19          1.420000          -9.550000  ...          -11.540000   \n",
       "20          1.420000          -9.550000  ...          -11.540000   \n",
       "21          1.420000          -9.550000  ...          -11.540000   \n",
       "22          1.910000         -10.600000  ...           -7.650000   \n",
       "23          1.910000         -10.600000  ...           -7.650000   \n",
       "24          1.910000         -10.600000  ...           -7.650000   \n",
       "25          2.330000         -10.900000  ...           -6.570000   \n",
       "26          2.330000         -10.900000  ...           -6.570000   \n",
       "27          2.330000         -10.900000  ...           -6.570000   \n",
       "28          2.690000         -10.760000  ...           -6.760000   \n",
       "29          2.690000         -10.760000  ...           -6.760000   \n",
       "30          1.129122          -2.079237  ...           -9.021952   \n",
       "\n",
       "    m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "0              10.220000             25.170000              9.440000   \n",
       "1              10.190000             25.240000              9.640000   \n",
       "2              10.210000             25.230000              9.700000   \n",
       "3              10.220000             25.190000              9.740000   \n",
       "4              10.250000             25.120000              9.710000   \n",
       "5              10.270000             25.040000              9.660000   \n",
       "6              10.300000             24.950000              9.570000   \n",
       "7              10.350000             24.770000              9.400000   \n",
       "8              10.400000             24.610000              9.230000   \n",
       "9              10.440000             24.460000              9.050000   \n",
       "10             10.510000             24.260000              8.810000   \n",
       "11             10.560000             24.080000              8.630000   \n",
       "12             10.610000             23.890000              8.380000   \n",
       "13             10.660000             23.700000              8.160000   \n",
       "14             10.700000             23.520000              7.930000   \n",
       "15             10.740000             23.330000              7.720000   \n",
       "16             10.770000             23.140000              7.520000   \n",
       "17             10.800000             22.960000              7.330000   \n",
       "18             10.830000             22.780000              7.150000   \n",
       "19             10.860000             22.620000              6.990000   \n",
       "20             10.880000             22.460000              6.840000   \n",
       "21             10.900000             22.330000              6.710000   \n",
       "22             10.910000             22.200000              6.580000   \n",
       "23             10.930000             22.100000              6.460000   \n",
       "24             10.940000             22.020000              6.360000   \n",
       "25             10.950000             21.910000              6.280000   \n",
       "26             10.970000             21.810000              6.170000   \n",
       "27             10.970000             21.700000              6.110000   \n",
       "28             10.990000             21.590000              6.030000   \n",
       "29             11.000000             21.510000              5.990000   \n",
       "30             -0.697012            -17.340796             -5.115932   \n",
       "\n",
       "    m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "0            9.240000          30.740000            5.80000   \n",
       "1            9.190000          30.800000            5.99000   \n",
       "2            9.200000          30.790000            6.05000   \n",
       "3            9.210000          30.750000            6.09000   \n",
       "4            9.240000          30.680000            6.06000   \n",
       "5            9.270000          30.600000            6.02000   \n",
       "6            9.310000          30.510000            5.94000   \n",
       "7            9.370000          30.330000            5.77000   \n",
       "8            9.440000          30.180000            5.61000   \n",
       "9            9.500000          30.030000            5.43000   \n",
       "10           9.590000          29.840000            5.20000   \n",
       "11           9.650000          29.670000            5.02000   \n",
       "12           9.730000          29.480000            4.79000   \n",
       "13           9.800000          29.300000            4.57000   \n",
       "14           9.860000          29.110000            4.35000   \n",
       "15           9.920000          28.930000            4.14000   \n",
       "16           9.970000          28.740000            3.94000   \n",
       "17          10.020000          28.570000            3.76000   \n",
       "18          10.070000          28.390000            3.59000   \n",
       "19          10.110000          28.230000            3.43000   \n",
       "20          10.150000          28.080000            3.28000   \n",
       "21          10.180000          27.940000            3.15000   \n",
       "22          10.200000          27.820000            3.03000   \n",
       "23          10.230000          27.730000            2.91000   \n",
       "24          10.250000          27.640000            2.81000   \n",
       "25          10.270000          27.530000            2.73000   \n",
       "26          10.300000          27.430000            2.62000   \n",
       "27          10.310000          27.330000            2.57000   \n",
       "28          10.340000          27.220000            2.49000   \n",
       "29          10.350000          27.150000            2.45000   \n",
       "30          -2.413226           0.566835           -0.31204   \n",
       "\n",
       "    m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "0           -1.590000           -1.08000          28.350000  \n",
       "1           -1.950000           -1.22000          28.450000  \n",
       "2           -2.250000           -1.39000          28.470000  \n",
       "3           -2.550000           -1.55000          28.440000  \n",
       "4           -2.900000           -1.71000          28.450000  \n",
       "5           -3.260000           -1.87000          28.500000  \n",
       "6           -3.590000           -2.08000          28.500000  \n",
       "7           -3.910000           -2.33000          28.320000  \n",
       "8           -4.220000           -2.60000          27.990000  \n",
       "9           -4.470000           -2.94000          27.070000  \n",
       "10          -4.780000           -3.22000          26.340000  \n",
       "11          -5.040000           -3.48000          25.640000  \n",
       "12          -5.260000           -3.68000          25.050000  \n",
       "13          -5.220000           -3.75000          24.580000  \n",
       "14          -5.230000           -3.91000          24.100000  \n",
       "15          -5.030000           -4.06000          23.690000  \n",
       "16          -4.810000           -4.18000          23.170000  \n",
       "17          -4.670000           -4.34000          22.580000  \n",
       "18          -4.520000           -4.50000          22.040000  \n",
       "19          -4.450000           -4.62000          21.660000  \n",
       "20          -4.400000           -4.69000          21.350000  \n",
       "21          -4.330000           -4.75000          21.050000  \n",
       "22          -4.290000           -4.81000          20.760000  \n",
       "23          -4.300000           -4.87000          20.830000  \n",
       "24          -4.320000           -4.91000          20.900000  \n",
       "25          -4.350000           -4.97000          20.790000  \n",
       "26          -4.380000           -5.10000          20.490000  \n",
       "27          -4.400000           -5.26000          20.180000  \n",
       "28          -4.340000           -5.49000          19.920000  \n",
       "29          -4.240000           -5.60000          19.770000  \n",
       "30          -2.935243          -14.80334          21.591707  \n",
       "\n",
       "[31 rows x 63 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df.to_csv('./test_combined_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
