{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './csvFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 목록을 랜덤하게 섞습니다.\n",
    "random.seed(42)  # 재현 가능한 결과를 위해 시드 설정\n",
    "random.shuffle(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 파일에서 컬럼 이름을 가져옵니다.\n",
    "first_file_path = os.path.join(directory, csv_files[0])\n",
    "first_df = pd.read_csv(first_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator_token = 999\n",
    "separator = pd.DataFrame({col: separator_token for col in first_df.columns}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.iloc[300:-100]\n",
    "    df_list.append(df)\n",
    "    df_list.append(separator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Time</th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>3.454812</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>352.2222</td>\n",
       "      <td>350.739100</td>\n",
       "      <td>0.733168</td>\n",
       "      <td>0.943708</td>\n",
       "      <td>0.631755</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>359.55840</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>302</td>\n",
       "      <td>3.465498</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>352.2729</td>\n",
       "      <td>350.772300</td>\n",
       "      <td>0.733238</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.631620</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>359.53570</td>\n",
       "      <td>0.884086</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>303</td>\n",
       "      <td>3.476308</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>352.2918</td>\n",
       "      <td>350.806000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944234</td>\n",
       "      <td>0.631682</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>359.52710</td>\n",
       "      <td>0.923961</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>3.487449</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>352.2754</td>\n",
       "      <td>350.814000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944235</td>\n",
       "      <td>0.631826</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>359.53850</td>\n",
       "      <td>0.959352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>305</td>\n",
       "      <td>3.498579</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>352.2285</td>\n",
       "      <td>350.846900</td>\n",
       "      <td>0.733279</td>\n",
       "      <td>0.944438</td>\n",
       "      <td>0.631913</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>359.57180</td>\n",
       "      <td>0.958712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250495</th>\n",
       "      <td>4600</td>\n",
       "      <td>51.260610</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>198.3722</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>-1.323386</td>\n",
       "      <td>1.040075</td>\n",
       "      <td>0.924556</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250496</th>\n",
       "      <td>4601</td>\n",
       "      <td>51.271360</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>198.4202</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>-1.323311</td>\n",
       "      <td>1.040272</td>\n",
       "      <td>0.924719</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250497</th>\n",
       "      <td>4602</td>\n",
       "      <td>51.283000</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>198.4699</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>-1.323236</td>\n",
       "      <td>1.040406</td>\n",
       "      <td>0.924903</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250498</th>\n",
       "      <td>4603</td>\n",
       "      <td>51.293670</td>\n",
       "      <td>-0.674747</td>\n",
       "      <td>0.459201</td>\n",
       "      <td>0.823021</td>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>357.4926</td>\n",
       "      <td>-0.614688</td>\n",
       "      <td>0.391303</td>\n",
       "      <td>...</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>198.5145</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>-1.322936</td>\n",
       "      <td>1.038698</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250499</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250500 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Frame        Time  m_avg_PelvisPosX  m_avg_PelvisPosY  \\\n",
       "0          301    3.454812          0.021056          0.537794   \n",
       "1          302    3.465498          0.021056          0.537794   \n",
       "2          303    3.476308          0.021056          0.537794   \n",
       "3          304    3.487449          0.021056          0.537794   \n",
       "4          305    3.498579          0.021056          0.537794   \n",
       "...        ...         ...               ...               ...   \n",
       "1250495   4600   51.260610         -0.674628          0.459754   \n",
       "1250496   4601   51.271360         -0.674628          0.459754   \n",
       "1250497   4602   51.283000         -0.674628          0.459754   \n",
       "1250498   4603   51.293670         -0.674747          0.459201   \n",
       "1250499    999  999.000000        999.000000        999.000000   \n",
       "\n",
       "         m_avg_PelvisPosZ  m_avg_PelvisRotX  m_avg_PelvisRotY  \\\n",
       "0                0.689871         358.82500          0.679502   \n",
       "1                0.689871         358.82500          0.679502   \n",
       "2                0.689871         358.82500          0.679502   \n",
       "3                0.689871         358.82500          0.679492   \n",
       "4                0.689871         358.82500          0.679492   \n",
       "...                   ...               ...               ...   \n",
       "1250495          0.821685          18.35563        179.608500   \n",
       "1250496          0.821685          18.35563        179.608500   \n",
       "1250497          0.821685          18.35563        179.608500   \n",
       "1250498          0.823021          18.44855        179.665600   \n",
       "1250499        999.000000         999.00000        999.000000   \n",
       "\n",
       "         m_avg_PelvisRotZ  m_avg_L_HipPosX  m_avg_L_HipPosY  ...  \\\n",
       "0                359.8412        -0.037938         0.455332  ...   \n",
       "1                359.8412        -0.037938         0.455332  ...   \n",
       "2                359.8412        -0.037938         0.455332  ...   \n",
       "3                359.8412        -0.037938         0.455332  ...   \n",
       "4                359.8412        -0.037938         0.455332  ...   \n",
       "...                   ...              ...              ...  ...   \n",
       "1250495          357.6249        -0.614788         0.391658  ...   \n",
       "1250496          357.6249        -0.614788         0.391658  ...   \n",
       "1250497          357.6249        -0.614788         0.391658  ...   \n",
       "1250498          357.4926        -0.614688         0.391303  ...   \n",
       "1250499          999.0000       999.000000       999.000000  ...   \n",
       "\n",
       "         m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "0                 8.262754           352.2222         350.739100   \n",
       "1                 8.274371           352.2729         350.772300   \n",
       "2                 8.273871           352.2918         350.806000   \n",
       "3                 8.267890           352.2754         350.814000   \n",
       "4                 8.257694           352.2285         350.846900   \n",
       "...                    ...                ...                ...   \n",
       "1250495          11.131550           198.3722           3.791916   \n",
       "1250496          11.133720           198.4202           3.836214   \n",
       "1250497          11.135700           198.4699           3.866319   \n",
       "1250498          11.140010           198.5145           3.896140   \n",
       "1250499         999.000000           999.0000         999.000000   \n",
       "\n",
       "         m_avg_R_WristPosX  m_avg_R_WristPosY  m_avg_R_WristPosZ  \\\n",
       "0                 0.733168           0.943708           0.631755   \n",
       "1                 0.733238           0.943977           0.631620   \n",
       "2                 0.733297           0.944234           0.631682   \n",
       "3                 0.733297           0.944235           0.631826   \n",
       "4                 0.733279           0.944438           0.631913   \n",
       "...                    ...                ...                ...   \n",
       "1250495          -1.323386           1.040075           0.924556   \n",
       "1250496          -1.323311           1.040272           0.924719   \n",
       "1250497          -1.323236           1.040406           0.924903   \n",
       "1250498          -1.322936           1.038698           0.926400   \n",
       "1250499         999.000000         999.000000         999.000000   \n",
       "\n",
       "         m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  Unnamed: 128  \n",
       "0                 0.200809          359.55840           0.846429           NaN  \n",
       "1                 0.114945          359.53570           0.884086           NaN  \n",
       "2                 0.151095          359.52710           0.923961           NaN  \n",
       "3                 0.239347          359.53850           0.959352           NaN  \n",
       "4                 0.385812          359.57180           0.958712           NaN  \n",
       "...                    ...                ...                ...           ...  \n",
       "1250495          47.211770           37.32508         178.617100           NaN  \n",
       "1250496          47.288450           37.23001         178.460700           NaN  \n",
       "1250497          47.363370           37.13769         178.286900           NaN  \n",
       "1250498          47.438840           37.06465         178.126200           NaN  \n",
       "1250499         999.000000          999.00000         999.000000         999.0  \n",
       "\n",
       "[1250500 rows x 129 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation 컬럼만 선택\n",
    "rotation_columns = [col for col in combined_df.columns if 'Rot' in col]\n",
    "\n",
    "# Position 컬럼만 선택\n",
    "position_columns = [col for col in combined_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "rotation_df = combined_df[rotation_columns]\n",
    "position_df = combined_df[position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "rotation_df = rotation_df.apply(lambda col: col.apply(normalize_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어나는 값이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = rotation_df.apply(lambda col: col.apply(lambda x: x != 999 and (x > 180 or x < -180))).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position 컬럼은 그대로 두고, 다시 합치기\n",
    "posrot_df = pd.concat([position_df, rotation_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "posrot_df.to_csv('./posrot_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 데이터를 DataFrame으로 변환\n",
    "posrot_df = pd.DataFrame(posrot_df, columns=posrot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "posrot_df = posrot_df.map(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_KneePosX</th>\n",
       "      <th>m_avg_L_KneePosY</th>\n",
       "      <th>m_avg_L_KneePosZ</th>\n",
       "      <th>m_avg_L_AnklePosX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.14</td>\n",
       "      <td>-0.34</td>\n",
       "      <td>-6.66</td>\n",
       "      <td>8.26</td>\n",
       "      <td>-7.78</td>\n",
       "      <td>-9.26</td>\n",
       "      <td>0.20</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.14</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-6.63</td>\n",
       "      <td>8.27</td>\n",
       "      <td>-7.73</td>\n",
       "      <td>-9.23</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.13</td>\n",
       "      <td>-0.37</td>\n",
       "      <td>-6.61</td>\n",
       "      <td>8.27</td>\n",
       "      <td>-7.71</td>\n",
       "      <td>-9.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.12</td>\n",
       "      <td>-0.39</td>\n",
       "      <td>-6.62</td>\n",
       "      <td>8.27</td>\n",
       "      <td>-7.72</td>\n",
       "      <td>-9.19</td>\n",
       "      <td>0.24</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.02</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.69</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.67</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>1.80</td>\n",
       "      <td>9.12</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-6.60</td>\n",
       "      <td>8.26</td>\n",
       "      <td>-7.77</td>\n",
       "      <td>-9.15</td>\n",
       "      <td>0.39</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250495</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>21.79</td>\n",
       "      <td>11.64</td>\n",
       "      <td>-178.00</td>\n",
       "      <td>15.93</td>\n",
       "      <td>11.13</td>\n",
       "      <td>-161.63</td>\n",
       "      <td>3.79</td>\n",
       "      <td>47.21</td>\n",
       "      <td>37.33</td>\n",
       "      <td>178.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250496</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>21.79</td>\n",
       "      <td>11.64</td>\n",
       "      <td>-178.00</td>\n",
       "      <td>15.93</td>\n",
       "      <td>11.13</td>\n",
       "      <td>-161.58</td>\n",
       "      <td>3.84</td>\n",
       "      <td>47.29</td>\n",
       "      <td>37.23</td>\n",
       "      <td>178.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250497</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>21.79</td>\n",
       "      <td>11.64</td>\n",
       "      <td>-178.00</td>\n",
       "      <td>15.93</td>\n",
       "      <td>11.14</td>\n",
       "      <td>-161.53</td>\n",
       "      <td>3.87</td>\n",
       "      <td>47.36</td>\n",
       "      <td>37.14</td>\n",
       "      <td>178.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250498</th>\n",
       "      <td>-0.67</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.82</td>\n",
       "      <td>-0.61</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.47</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.49</td>\n",
       "      <td>...</td>\n",
       "      <td>22.25</td>\n",
       "      <td>12.38</td>\n",
       "      <td>-177.68</td>\n",
       "      <td>15.54</td>\n",
       "      <td>11.14</td>\n",
       "      <td>-161.49</td>\n",
       "      <td>3.90</td>\n",
       "      <td>47.44</td>\n",
       "      <td>37.06</td>\n",
       "      <td>178.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250499</th>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>...</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "      <td>999.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250500 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  \\\n",
       "0                    0.02              0.54              0.69   \n",
       "1                    0.02              0.54              0.69   \n",
       "2                    0.02              0.54              0.69   \n",
       "3                    0.02              0.54              0.69   \n",
       "4                    0.02              0.54              0.69   \n",
       "...                   ...               ...               ...   \n",
       "1250495             -0.67              0.46              0.82   \n",
       "1250496             -0.67              0.46              0.82   \n",
       "1250497             -0.67              0.46              0.82   \n",
       "1250498             -0.67              0.46              0.82   \n",
       "1250499            999.00            999.00            999.00   \n",
       "\n",
       "         m_avg_L_HipPosX  m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_KneePosX  \\\n",
       "0                  -0.04             0.46             0.67             -0.08   \n",
       "1                  -0.04             0.46             0.67             -0.08   \n",
       "2                  -0.04             0.46             0.67             -0.08   \n",
       "3                  -0.04             0.46             0.67             -0.08   \n",
       "4                  -0.04             0.46             0.67             -0.08   \n",
       "...                  ...              ...              ...               ...   \n",
       "1250495            -0.61             0.39             0.86             -0.47   \n",
       "1250496            -0.61             0.39             0.86             -0.47   \n",
       "1250497            -0.61             0.39             0.86             -0.47   \n",
       "1250498            -0.61             0.39             0.86             -0.47   \n",
       "1250499           999.00           999.00           999.00            999.00   \n",
       "\n",
       "         m_avg_L_KneePosY  m_avg_L_KneePosZ  m_avg_L_AnklePosX  ...  \\\n",
       "0                    0.07              0.68              -0.07  ...   \n",
       "1                    0.07              0.68              -0.07  ...   \n",
       "2                    0.07              0.68              -0.07  ...   \n",
       "3                    0.07              0.68              -0.07  ...   \n",
       "4                    0.07              0.68              -0.07  ...   \n",
       "...                   ...               ...                ...  ...   \n",
       "1250495              0.04              0.86              -0.49  ...   \n",
       "1250496              0.04              0.86              -0.49  ...   \n",
       "1250497              0.04              0.86              -0.49  ...   \n",
       "1250498              0.04              0.85              -0.49  ...   \n",
       "1250499            999.00            999.00             999.00  ...   \n",
       "\n",
       "         m_avg_R_CollarRotZ  m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  \\\n",
       "0                      1.80                  9.14                 -0.34   \n",
       "1                      1.80                  9.14                 -0.35   \n",
       "2                      1.80                  9.13                 -0.37   \n",
       "3                      1.80                  9.12                 -0.39   \n",
       "4                      1.80                  9.12                 -0.35   \n",
       "...                     ...                   ...                   ...   \n",
       "1250495               21.79                 11.64               -178.00   \n",
       "1250496               21.79                 11.64               -178.00   \n",
       "1250497               21.79                 11.64               -178.00   \n",
       "1250498               22.25                 12.38               -177.68   \n",
       "1250499              999.00                999.00                999.00   \n",
       "\n",
       "         m_avg_R_ShoulderRotZ  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "0                       -6.66               8.26              -7.78   \n",
       "1                       -6.63               8.27              -7.73   \n",
       "2                       -6.61               8.27              -7.71   \n",
       "3                       -6.62               8.27              -7.72   \n",
       "4                       -6.60               8.26              -7.77   \n",
       "...                       ...                ...                ...   \n",
       "1250495                 15.93              11.13            -161.63   \n",
       "1250496                 15.93              11.13            -161.58   \n",
       "1250497                 15.93              11.14            -161.53   \n",
       "1250498                 15.54              11.14            -161.49   \n",
       "1250499                999.00             999.00             999.00   \n",
       "\n",
       "         m_avg_R_ElbowRotZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "0                    -9.26               0.20              -0.44   \n",
       "1                    -9.23               0.11              -0.46   \n",
       "2                    -9.19               0.15              -0.47   \n",
       "3                    -9.19               0.24              -0.46   \n",
       "4                    -9.15               0.39              -0.43   \n",
       "...                    ...                ...                ...   \n",
       "1250495               3.79              47.21              37.33   \n",
       "1250496               3.84              47.29              37.23   \n",
       "1250497               3.87              47.36              37.14   \n",
       "1250498               3.90              47.44              37.06   \n",
       "1250499             999.00             999.00             999.00   \n",
       "\n",
       "         m_avg_R_WristRotZ  \n",
       "0                     0.85  \n",
       "1                     0.88  \n",
       "2                     0.92  \n",
       "3                     0.96  \n",
       "4                     0.96  \n",
       "...                    ...  \n",
       "1250495             178.62  \n",
       "1250496             178.46  \n",
       "1250497             178.29  \n",
       "1250498             178.13  \n",
       "1250499             999.00  \n",
       "\n",
       "[1250500 rows x 126 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = posrot_df.iloc[:1000400]\n",
    "test = posrot_df.iloc[1000400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30  # Sequence length\n",
    "n_features = 126  # Number of features\n",
    "output_units = (21 * 3)  # Output shape\n",
    "head_size = 256  # Size of attention head\n",
    "num_heads = 7  # Number of attention heads\n",
    "ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "num_blocks = 4  # Number of transformer blocks\n",
    "mlp_units = [512, 256, 128]  # Size of the dense layers of the final classifier\n",
    "dropout_rate = 0.3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.sequence_length] # : 입력 시퀀스 (30개의 데이터)\n",
    "        y = self.data[index+self.sequence_length-1] # : 예측 시퀀스 (1개의 데이터)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "# 데이터셋과 데이터 로더를 생성\n",
    "train_dataset = TimeseriesDataset(X_train.values, n_input)\n",
    "val_dataset = TimeseriesDataset(X_val.values, n_input)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([128, 30, 126])\n",
      "y batch shape: torch.Size([128, 126])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로더에서 첫 번째 배치를 가져와서 형태를 확인\n",
    "first_batch = next(iter(train_loader))\n",
    "X, y = first_batch\n",
    "\n",
    "print(\"X batch shape:\", X.shape)\n",
    "print(\"y batch shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_features, n_input):\n",
    "        super(PositionalEncoding, self).__init__() # 상속받은 nn.Module 클래스의 __init__() 메서드 호출\n",
    "        pe = torch.zeros(n_input, n_features) # 가장 큰 시퀀스 길이인 max_len을 기준으로 모두 0으로 채워진 크기가 (max_len, n_features)인 텐서 생성\n",
    "        position = torch.arange(0, n_input, dtype=torch.float).unsqueeze(1) # position 텐서 생성\n",
    "\n",
    "        # div_term을 계산하는 방식 수정\n",
    "        div_term = torch.exp(torch.arange(0, n_features, 2).float() * (-math.log(10000.0) / n_features)) # div_term 계산\n",
    "        \n",
    "        # div_term의 길이를 n_features의 절반으로 조정\n",
    "        div_term = div_term.repeat_interleave(2)[:n_features] # div_term 텐서 생성\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2]) # 짝수 인덱스에는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2]) # 홀수 인덱스에는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0) # pe = [bs, seq_len, n_feautres]\n",
    "        self.register_buffer('pe', pe) # pe 텐서를 모델의 버퍼로 등록\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(self.pe.shape)\n",
    "        x = x + self.pe[:, :x.size(1), :] # 입력에 위치 인코딩을 더함\n",
    "        return x\n",
    "\n",
    "# Transformer Block 정의\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(n_features, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_features, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "# 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout, n_input):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(n_features, num_heads, ff_dim, dropout) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        # 첫 번째 nn.Linear 층의 입력 차원을 n_features로 설정\n",
    "        self.layers.add_module(\"dense_0\", nn.Linear(n_features, mlp_units[0]))\n",
    "        self.layers.add_module(\"relu_0\", nn.ReLU())\n",
    "        self.layers.add_module(\"dropout_0\", nn.Dropout(dropout))\n",
    "        self.layers.add_module(\"norm_0\", nn.LayerNorm(mlp_units[0]))\n",
    "\n",
    "        # 이후 층들에 대한 설정\n",
    "        for i in range(1, len(mlp_units)):\n",
    "            self.layers.add_module(f\"dense_{i}\", nn.Linear(mlp_units[i-1], mlp_units[i]))\n",
    "            self.layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            self.layers.add_module(f\"dropout_{i}\", nn.Dropout(dropout))\n",
    "            self.layers.add_module(f\"norm_{i}\", nn.LayerNorm(mlp_units[i]))\n",
    "\n",
    "        # 최종 출력 층\n",
    "        self.out = nn.Linear(mlp_units[-1], n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.out(x)\n",
    "\n",
    "model = TransformerModel(n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout_rate, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "x = torch.rand(128, 30, 126)\n",
    "y = pos_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'm_avg_PelvisPosX', 'm_avg_PelvisPosY', 'm_avg_PelvisPosZ',\n",
    "    'm_avg_L_HipPosX', 'm_avg_L_HipPosY', 'm_avg_L_HipPosZ',\n",
    "    'm_avg_L_KneePosX', 'm_avg_L_KneePosY', 'm_avg_L_KneePosZ',\n",
    "    'm_avg_L_AnklePosX', 'm_avg_L_AnklePosY', 'm_avg_L_AnklePosZ',\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_R_HipPosX', 'm_avg_R_HipPosY', 'm_avg_R_HipPosZ',\n",
    "    'm_avg_R_KneePosX', 'm_avg_R_KneePosY', 'm_avg_R_KneePosZ',\n",
    "    'm_avg_R_AnklePosX', 'm_avg_R_AnklePosY', 'm_avg_R_AnklePosZ',\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_Spine1PosX', 'm_avg_Spine1PosY', 'm_avg_Spine1PosZ',\n",
    "    'm_avg_Spine2PosX', 'm_avg_Spine2PosY', 'm_avg_Spine2PosZ',\n",
    "    'm_avg_L_CollarPosX', 'm_avg_L_CollarPosY', 'm_avg_L_CollarPosZ',\n",
    "    'm_avg_L_ShoulderPosX', 'm_avg_L_ShoulderPosY', 'm_avg_L_ShoulderPosZ',\n",
    "    'm_avg_L_ElbowPosX', 'm_avg_L_ElbowPosY', 'm_avg_L_ElbowPosZ',\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_NeckPosX', 'm_avg_NeckPosY', 'm_avg_NeckPosZ',\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_R_CollarPosX', 'm_avg_R_CollarPosY', 'm_avg_R_CollarPosZ',\n",
    "    'm_avg_R_ShoulderPosX', 'm_avg_R_ShoulderPosY', 'm_avg_R_ShoulderPosZ',\n",
    "    'm_avg_R_ElbowPosX', 'm_avg_R_ElbowPosY', 'm_avg_R_ElbowPosZ',\n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_PelvisRotX', 'm_avg_PelvisRotY', 'm_avg_PelvisRotZ',\n",
    "    'm_avg_L_HipRotX', 'm_avg_L_HipRotY', 'm_avg_L_HipRotZ',\n",
    "    'm_avg_L_KneeRotX', 'm_avg_L_KneeRotY', 'm_avg_L_KneeRotZ',\n",
    "    'm_avg_L_AnkleRotX', 'm_avg_L_AnkleRotY', 'm_avg_L_AnkleRotZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "    'm_avg_R_HipRotX', 'm_avg_R_HipRotY', 'm_avg_R_HipRotZ',\n",
    "    'm_avg_R_KneeRotX', 'm_avg_R_KneeRotY', 'm_avg_R_KneeRotZ',\n",
    "    'm_avg_R_AnkleRotX', 'm_avg_R_AnkleRotY', 'm_avg_R_AnkleRotZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "    'm_avg_Spine1RotX', 'm_avg_Spine1RotY', 'm_avg_Spine1RotZ',\n",
    "    'm_avg_Spine2RotX', 'm_avg_Spine2RotY', 'm_avg_Spine2RotZ',\n",
    "    'm_avg_L_CollarRotX', 'm_avg_L_CollarRotY', 'm_avg_L_CollarRotZ',\n",
    "    'm_avg_L_ShoulderRotX', 'm_avg_L_ShoulderRotY', 'm_avg_L_ShoulderRotZ',\n",
    "    'm_avg_L_ElbowRotX', 'm_avg_L_ElbowRotY', 'm_avg_L_ElbowRotZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_NeckRotX', 'm_avg_NeckRotY', 'm_avg_NeckRotZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_R_CollarRotX', 'm_avg_R_CollarRotY', 'm_avg_R_CollarRotZ',\n",
    "    'm_avg_R_ShoulderRotX', 'm_avg_R_ShoulderRotY', 'm_avg_R_ShoulderRotZ',\n",
    "    'm_avg_R_ElbowRotX', 'm_avg_R_ElbowRotY', 'm_avg_R_ElbowRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ'\n",
    "    ]\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48, 49, 50, 42, 43, 44, 60, 61, 62, 12, 13, 14, 24, 25, 26, 111, 112, 113, 105, 106, 107, 123, 124, 125, 75, 76, 77, 87, 88, 89]\n"
     ]
    }
   ],
   "source": [
    "column_names = [\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "]\n",
    "\n",
    "weighted_columns_indices = [column_order.index(name) for name in column_names]\n",
    "print(weighted_columns_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weighted_columns_indices, weight_for_weighted_columns, threshold, penalty_weight):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weighted_columns_indices = torch.tensor(weighted_columns_indices)\n",
    "        self.weight_for_weighted_columns = weight_for_weighted_columns\n",
    "        self.threshold = threshold  # 신체 움직임의 임계값\n",
    "        self.penalty_weight = penalty_weight  # 비정상적 움직임에 대한 패널티 가중치\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # y_pred의 마지막 feature를 rot_diff_category로 분리\n",
    "        y_pred_values = y_pred[:, :]\n",
    "\n",
    "        # MSE 계산\n",
    "        mse = F.mse_loss(y_true[:, :], y_pred_values, reduction='none')\n",
    "        mse = mse.mean(axis=-1)\n",
    "\n",
    "        # 특정 joint에 대한 가중치 적용\n",
    "        weighted_mse = y_pred[:, self.weighted_columns_indices]\n",
    "        weighted_mse = (weighted_mse ** 2) * self.weight_for_weighted_columns\n",
    "        mse += weighted_mse.mean(axis=-1)\n",
    "\n",
    "        # 과도한 움직임에 대한 패널티 적용\n",
    "        excessive_movement_penalty = (y_pred - y_true).abs() > self.threshold\n",
    "        penalty = excessive_movement_penalty.type(torch.float32) * self.penalty_weight\n",
    "        mse += penalty.mean(axis=-1)\n",
    "\n",
    "        return mse.mean()  # 전체 배치에 대한 평균 손실 반환\n",
    "\n",
    "# 가중치를 적용할 열 인덱스와 가중치 값\n",
    "weighted_columns_indices = weighted_columns_indices\n",
    "weight_for_weighted_columns = 2.0\n",
    "\n",
    "# 임계값과 패널티 가중치 설정\n",
    "threshold = 10.0  # 예시 임계값\n",
    "penalty_weight = 1.0  # 예시 패널티 가중치\n",
    "\n",
    "# CustomLoss 인스턴스 생성\n",
    "custom_loss_instance = CustomLoss(\n",
    "    weighted_columns_indices=weighted_columns_indices,\n",
    "    weight_for_weighted_columns=weight_for_weighted_columns,\n",
    "    threshold=threshold,\n",
    "    penalty_weight=penalty_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화기와 손실 함수\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = custom_loss_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "epochs = 100\n",
    "patience = 7  # Early Stopping patience\n",
    "best_loss = np.inf\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=7, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Current device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "PositionalEncoding-1              [-1, 30, 126]               0\n",
      "MultiheadAttention-2  [[-1, 30, 126], [-1, 2, 2]]               0\n",
      "           Dropout-3              [-1, 30, 126]               0\n",
      "         LayerNorm-4              [-1, 30, 126]             252\n",
      "            Linear-5              [-1, 30, 512]          65,024\n",
      "              ReLU-6              [-1, 30, 512]               0\n",
      "            Linear-7              [-1, 30, 126]          64,638\n",
      "           Dropout-8              [-1, 30, 126]               0\n",
      "         LayerNorm-9              [-1, 30, 126]             252\n",
      " TransformerBlock-10              [-1, 30, 126]               0\n",
      "MultiheadAttention-11  [[-1, 30, 126], [-1, 2, 2]]               0\n",
      "          Dropout-12              [-1, 30, 126]               0\n",
      "        LayerNorm-13              [-1, 30, 126]             252\n",
      "           Linear-14              [-1, 30, 512]          65,024\n",
      "             ReLU-15              [-1, 30, 512]               0\n",
      "           Linear-16              [-1, 30, 126]          64,638\n",
      "          Dropout-17              [-1, 30, 126]               0\n",
      "        LayerNorm-18              [-1, 30, 126]             252\n",
      " TransformerBlock-19              [-1, 30, 126]               0\n",
      "MultiheadAttention-20  [[-1, 30, 126], [-1, 2, 2]]               0\n",
      "          Dropout-21              [-1, 30, 126]               0\n",
      "        LayerNorm-22              [-1, 30, 126]             252\n",
      "           Linear-23              [-1, 30, 512]          65,024\n",
      "             ReLU-24              [-1, 30, 512]               0\n",
      "           Linear-25              [-1, 30, 126]          64,638\n",
      "          Dropout-26              [-1, 30, 126]               0\n",
      "        LayerNorm-27              [-1, 30, 126]             252\n",
      " TransformerBlock-28              [-1, 30, 126]               0\n",
      "MultiheadAttention-29  [[-1, 30, 126], [-1, 2, 2]]               0\n",
      "          Dropout-30              [-1, 30, 126]               0\n",
      "        LayerNorm-31              [-1, 30, 126]             252\n",
      "           Linear-32              [-1, 30, 512]          65,024\n",
      "             ReLU-33              [-1, 30, 512]               0\n",
      "           Linear-34              [-1, 30, 126]          64,638\n",
      "          Dropout-35              [-1, 30, 126]               0\n",
      "        LayerNorm-36              [-1, 30, 126]             252\n",
      " TransformerBlock-37              [-1, 30, 126]               0\n",
      "           Linear-38                  [-1, 512]          65,024\n",
      "             ReLU-39                  [-1, 512]               0\n",
      "          Dropout-40                  [-1, 512]               0\n",
      "        LayerNorm-41                  [-1, 512]           1,024\n",
      "           Linear-42                  [-1, 256]         131,328\n",
      "             ReLU-43                  [-1, 256]               0\n",
      "          Dropout-44                  [-1, 256]               0\n",
      "        LayerNorm-45                  [-1, 256]             512\n",
      "           Linear-46                  [-1, 128]          32,896\n",
      "             ReLU-47                  [-1, 128]               0\n",
      "          Dropout-48                  [-1, 128]               0\n",
      "        LayerNorm-49                  [-1, 128]             256\n",
      "           Linear-50                  [-1, 126]          16,254\n",
      "================================================================\n",
      "Total params: 767,958\n",
      "Trainable params: 767,958\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 1.23\n",
      "Params size (MB): 2.93\n",
      "Estimated Total Size (MB): 4.17\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(n_input, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 6253/6253 [00:44<00:00, 139.39batch/s, train_loss=2.72e+3]\n",
      "Validation Epoch 1/100: 100%|██████████| 1563/1563 [00:05<00:00, 261.04batch/s, val_loss=1.56e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 \t Training Loss: 5433308393.257568 \t Validation Loss: 1595696599.440552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 6253/6253 [00:45<00:00, 137.24batch/s, train_loss=2.73e+3]\n",
      "Validation Epoch 2/100: 100%|██████████| 1563/1563 [00:06<00:00, 257.93batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 \t Training Loss: 5126411010.807129 \t Validation Loss: 1535969693.170898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 6253/6253 [00:45<00:00, 138.22batch/s, train_loss=2.69e+3]\n",
      "Validation Epoch 3/100: 100%|██████████| 1563/1563 [00:06<00:00, 252.20batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 \t Training Loss: 4978193536.576904 \t Validation Loss: 1512654703.211548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 6253/6253 [00:45<00:00, 137.44batch/s, train_loss=2.7e+3] \n",
      "Validation Epoch 4/100: 100%|██████████| 1563/1563 [00:06<00:00, 257.20batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 \t Training Loss: 4894321297.650146 \t Validation Loss: 1503027026.713135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 6253/6253 [00:45<00:00, 136.08batch/s, train_loss=2.66e+3]\n",
      "Validation Epoch 5/100: 100%|██████████| 1563/1563 [00:06<00:00, 255.37batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t Training Loss: 4839461441.876953 \t Validation Loss: 1485417551.337891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 6253/6253 [00:45<00:00, 138.47batch/s, train_loss=2.64e+3]\n",
      "Validation Epoch 6/100: 100%|██████████| 1563/1563 [00:06<00:00, 251.64batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 \t Training Loss: 4798136170.328613 \t Validation Loss: 1472891314.621704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 6253/6253 [00:50<00:00, 123.15batch/s, train_loss=2.64e+3]\n",
      "Validation Epoch 7/100: 100%|██████████| 1563/1563 [00:06<00:00, 227.54batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 \t Training Loss: 4764212110.210938 \t Validation Loss: 1462284860.656372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 6253/6253 [00:48<00:00, 127.98batch/s, train_loss=2.62e+3]\n",
      "Validation Epoch 8/100: 100%|██████████| 1563/1563 [00:06<00:00, 233.07batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 \t Training Loss: 4735526704.660156 \t Validation Loss: 1456951830.192627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 6253/6253 [00:52<00:00, 119.29batch/s, train_loss=2.62e+3]\n",
      "Validation Epoch 9/100: 100%|██████████| 1563/1563 [00:06<00:00, 239.84batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 \t Training Loss: 4709721001.416016 \t Validation Loss: 1448138153.205811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 6253/6253 [00:46<00:00, 134.20batch/s, train_loss=2.57e+3]\n",
      "Validation Epoch 10/100: 100%|██████████| 1563/1563 [00:06<00:00, 251.29batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Training Loss: 4686447945.499023 \t Validation Loss: 1437912321.899292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 6253/6253 [00:47<00:00, 132.69batch/s, train_loss=2.58e+3]\n",
      "Validation Epoch 11/100: 100%|██████████| 1563/1563 [00:06<00:00, 235.58batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 \t Training Loss: 4662726160.644531 \t Validation Loss: 1435773708.073608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 6253/6253 [00:46<00:00, 133.79batch/s, train_loss=2.59e+3]\n",
      "Validation Epoch 12/100: 100%|██████████| 1563/1563 [00:06<00:00, 257.37batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 \t Training Loss: 4640852463.078369 \t Validation Loss: 1418573406.522827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 6253/6253 [00:45<00:00, 136.25batch/s, train_loss=2.52e+3]\n",
      "Validation Epoch 13/100: 100%|██████████| 1563/1563 [00:05<00:00, 264.95batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 \t Training Loss: 4619468577.898682 \t Validation Loss: 1413740915.587280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: 100%|██████████| 6253/6253 [00:44<00:00, 140.58batch/s, train_loss=2.36e+3]\n",
      "Validation Epoch 14/100: 100%|██████████| 1563/1563 [00:05<00:00, 265.33batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 \t Training Loss: 4596710656.885254 \t Validation Loss: 1408981641.650757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: 100%|██████████| 6253/6253 [00:46<00:00, 133.72batch/s, train_loss=2.23e+3]\n",
      "Validation Epoch 15/100: 100%|██████████| 1563/1563 [00:06<00:00, 245.65batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t Training Loss: 4546946206.431885 \t Validation Loss: 1377638163.299683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: 100%|██████████| 6253/6253 [00:44<00:00, 139.01batch/s, train_loss=2.11e+3]\n",
      "Validation Epoch 16/100: 100%|██████████| 1563/1563 [00:06<00:00, 248.15batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 \t Training Loss: 4508101042.648682 \t Validation Loss: 1367679502.764160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: 100%|██████████| 6253/6253 [00:47<00:00, 130.71batch/s, train_loss=2.02e+3]\n",
      "Validation Epoch 17/100: 100%|██████████| 1563/1563 [00:06<00:00, 228.41batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 \t Training Loss: 4483589803.496948 \t Validation Loss: 1358477975.148438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: 100%|██████████| 6253/6253 [00:44<00:00, 139.12batch/s, train_loss=1.99e+3]\n",
      "Validation Epoch 18/100: 100%|██████████| 1563/1563 [00:06<00:00, 252.04batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 \t Training Loss: 4463028678.121948 \t Validation Loss: 1342255783.238770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: 100%|██████████| 6253/6253 [00:44<00:00, 139.14batch/s, train_loss=1.98e+3]\n",
      "Validation Epoch 19/100: 100%|██████████| 1563/1563 [00:06<00:00, 249.77batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 \t Training Loss: 4442840808.968872 \t Validation Loss: 1350449151.352661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: 100%|██████████| 6253/6253 [00:45<00:00, 136.42batch/s, train_loss=1.88e+3]\n",
      "Validation Epoch 20/100: 100%|██████████| 1563/1563 [00:06<00:00, 247.89batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t Training Loss: 4423718474.856445 \t Validation Loss: 1335156940.508789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: 100%|██████████| 6253/6253 [00:47<00:00, 131.78batch/s, train_loss=1.87e+3]\n",
      "Validation Epoch 21/100: 100%|██████████| 1563/1563 [00:06<00:00, 246.41batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 \t Training Loss: 4407351952.839844 \t Validation Loss: 1326396643.998779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: 100%|██████████| 6253/6253 [00:46<00:00, 134.28batch/s, train_loss=1.77e+3]\n",
      "Validation Epoch 22/100: 100%|██████████| 1563/1563 [00:06<00:00, 256.60batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 \t Training Loss: 4389890971.521851 \t Validation Loss: 1329941252.847412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: 100%|██████████| 6253/6253 [00:46<00:00, 135.47batch/s, train_loss=1.74e+3]\n",
      "Validation Epoch 23/100: 100%|██████████| 1563/1563 [00:06<00:00, 256.43batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 \t Training Loss: 4373492166.589844 \t Validation Loss: 1366873275.085571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: 100%|██████████| 6253/6253 [00:44<00:00, 142.11batch/s, train_loss=1.72e+3]\n",
      "Validation Epoch 24/100: 100%|██████████| 1563/1563 [00:05<00:00, 263.03batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24 \t Training Loss: 4360065787.054077 \t Validation Loss: 1383602984.176880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: 100%|██████████| 6253/6253 [00:44<00:00, 141.67batch/s, train_loss=1.69e+3]\n",
      "Validation Epoch 25/100: 100%|██████████| 1563/1563 [00:06<00:00, 260.00batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 \t Training Loss: 4344819643.692993 \t Validation Loss: 1304643460.405151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.73batch/s, train_loss=1.64e+3]\n",
      "Validation Epoch 26/100: 100%|██████████| 1563/1563 [00:05<00:00, 264.74batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26 \t Training Loss: 4338375788.117065 \t Validation Loss: 1315257076.981323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: 100%|██████████| 6253/6253 [00:45<00:00, 137.14batch/s, train_loss=1.58e+3]\n",
      "Validation Epoch 27/100: 100%|██████████| 1563/1563 [00:06<00:00, 254.09batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 \t Training Loss: 4320726570.254517 \t Validation Loss: 1317913292.426270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: 100%|██████████| 6253/6253 [00:44<00:00, 140.25batch/s, train_loss=1.59e+3]\n",
      "Validation Epoch 28/100: 100%|██████████| 1563/1563 [00:05<00:00, 261.81batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28 \t Training Loss: 4313436265.590576 \t Validation Loss: 1322602198.458130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: 100%|██████████| 6253/6253 [00:44<00:00, 140.85batch/s, train_loss=1.57e+3]\n",
      "Validation Epoch 29/100: 100%|██████████| 1563/1563 [00:05<00:00, 263.76batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29 \t Training Loss: 4304779124.364624 \t Validation Loss: 1317941168.340210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.12batch/s, train_loss=1.59e+3]\n",
      "Validation Epoch 30/100: 100%|██████████| 1563/1563 [00:05<00:00, 267.05batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 \t Training Loss: 4299056684.152466 \t Validation Loss: 1335127239.422363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.36batch/s, train_loss=1.6e+3] \n",
      "Validation Epoch 31/100: 100%|██████████| 1563/1563 [00:05<00:00, 268.32batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 \t Training Loss: 4294949635.025513 \t Validation Loss: 1305839684.263672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.36batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 32/100: 100%|██████████| 1563/1563 [00:05<00:00, 266.95batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32 \t Training Loss: 4287456276.565918 \t Validation Loss: 1274071478.654175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.33batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 33/100: 100%|██████████| 1563/1563 [00:05<00:00, 267.86batch/s, val_loss=1.51e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 \t Training Loss: 4289738772.755981 \t Validation Loss: 1274592092.742554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.67batch/s, train_loss=1.57e+3]\n",
      "Validation Epoch 34/100: 100%|██████████| 1563/1563 [00:05<00:00, 271.56batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34 \t Training Loss: 4278820670.609741 \t Validation Loss: 1274694248.794434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.57batch/s, train_loss=1.57e+3]\n",
      "Validation Epoch 35/100: 100%|██████████| 1563/1563 [00:05<00:00, 270.81batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 \t Training Loss: 4280279039.632812 \t Validation Loss: 1296587483.369019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/100: 100%|██████████| 6253/6253 [00:43<00:00, 142.67batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 36/100: 100%|██████████| 1563/1563 [00:05<00:00, 269.91batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 \t Training Loss: 4281941003.660522 \t Validation Loss: 1278833438.077759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.56batch/s, train_loss=1.55e+3]\n",
      "Validation Epoch 37/100: 100%|██████████| 1563/1563 [00:05<00:00, 270.12batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37 \t Training Loss: 4272798162.338501 \t Validation Loss: 1263103320.040039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.72batch/s, train_loss=1.55e+3]\n",
      "Validation Epoch 38/100: 100%|██████████| 1563/1563 [00:05<00:00, 269.58batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38 \t Training Loss: 4265066908.573120 \t Validation Loss: 1263341518.820801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.78batch/s, train_loss=1.56e+3]\n",
      "Validation Epoch 39/100: 100%|██████████| 1563/1563 [00:05<00:00, 271.83batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39 \t Training Loss: 4264018024.607422 \t Validation Loss: 1264868781.198242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/100: 100%|██████████| 6253/6253 [00:44<00:00, 141.66batch/s, train_loss=1.56e+3]\n",
      "Validation Epoch 40/100: 100%|██████████| 1563/1563 [00:05<00:00, 272.04batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 \t Training Loss: 4256577120.874146 \t Validation Loss: 2270831442.241089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/100: 100%|██████████| 6253/6253 [00:43<00:00, 144.00batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 41/100: 100%|██████████| 1563/1563 [00:05<00:00, 271.58batch/s, val_loss=1.52e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41 \t Training Loss: 4263923809.105713 \t Validation Loss: 1266762587.172241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.85batch/s, train_loss=1.55e+3]\n",
      "Validation Epoch 42/100: 100%|██████████| 1563/1563 [00:05<00:00, 272.26batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42 \t Training Loss: 4255990423.899536 \t Validation Loss: 1260973830.707764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.80batch/s, train_loss=1.51e+3]\n",
      "Validation Epoch 43/100: 100%|██████████| 1563/1563 [00:05<00:00, 261.89batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43 \t Training Loss: 4252436590.461304 \t Validation Loss: 1259951656.921021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/100: 100%|██████████| 6253/6253 [00:44<00:00, 142.05batch/s, train_loss=1.57e+3]\n",
      "Validation Epoch 44/100: 100%|██████████| 1563/1563 [00:05<00:00, 272.30batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44 \t Training Loss: 4268206771.606445 \t Validation Loss: 1268087694.780762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/100: 100%|██████████| 6253/6253 [00:43<00:00, 144.99batch/s, train_loss=1.57e+3]\n",
      "Validation Epoch 45/100: 100%|██████████| 1563/1563 [00:05<00:00, 269.28batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 \t Training Loss: 4252098486.196289 \t Validation Loss: 1268946701.235596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/100: 100%|██████████| 6253/6253 [00:43<00:00, 143.67batch/s, train_loss=1.6e+3] \n",
      "Validation Epoch 46/100: 100%|██████████| 1563/1563 [00:05<00:00, 269.99batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 \t Training Loss: 4249892812.903687 \t Validation Loss: 1265704071.408325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/100: 100%|██████████| 6253/6253 [00:47<00:00, 130.92batch/s, train_loss=1.59e+3]\n",
      "Validation Epoch 47/100: 100%|██████████| 1563/1563 [00:06<00:00, 241.06batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47 \t Training Loss: 4253695765.851562 \t Validation Loss: 1268089491.527222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/100: 100%|██████████| 6253/6253 [00:48<00:00, 129.37batch/s, train_loss=1.53e+3]\n",
      "Validation Epoch 48/100: 100%|██████████| 1563/1563 [00:05<00:00, 266.52batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48 \t Training Loss: 4245349599.224731 \t Validation Loss: 1278782869.134277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/100: 100%|██████████| 6253/6253 [00:44<00:00, 141.77batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 49/100: 100%|██████████| 1563/1563 [00:05<00:00, 263.61batch/s, val_loss=1.53e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 \t Training Loss: 4256821593.574585 \t Validation Loss: 1266191734.325684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/100: 100%|██████████| 6253/6253 [00:47<00:00, 132.83batch/s, train_loss=1.61e+3]\n",
      "Validation Epoch 50/100: 100%|██████████| 1563/1563 [00:06<00:00, 245.07batch/s, val_loss=1.54e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50 \t Training Loss: 4234193713.785889 \t Validation Loss: 1259076668.411011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/100: 100%|██████████| 6253/6253 [00:46<00:00, 133.44batch/s, train_loss=1.66e+3]\n",
      "Validation Epoch 51/100: 100%|██████████| 1563/1563 [00:06<00:00, 241.84batch/s, val_loss=1.54e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51 \t Training Loss: 4245969447.734863 \t Validation Loss: 1277661328.177734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/100: 100%|██████████| 6253/6253 [00:47<00:00, 132.86batch/s, train_loss=1.54e+3]\n",
      "Validation Epoch 52/100: 100%|██████████| 1563/1563 [00:06<00:00, 256.86batch/s, val_loss=1.54e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52 \t Training Loss: 4239993200.533569 \t Validation Loss: 1262300400.225586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/100: 100%|██████████| 6253/6253 [00:46<00:00, 133.64batch/s, train_loss=1.63e+3]\n",
      "Validation Epoch 53/100: 100%|██████████| 1563/1563 [00:06<00:00, 239.54batch/s, val_loss=1.54e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53 \t Training Loss: 4230331136.244629 \t Validation Loss: 1288091459.992554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/100: 100%|██████████| 6253/6253 [00:47<00:00, 131.21batch/s, train_loss=1.64e+3]\n",
      "Validation Epoch 54/100: 100%|██████████| 1563/1563 [00:06<00:00, 237.96batch/s, val_loss=1.55e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54 \t Training Loss: 4245763891.185913 \t Validation Loss: 1263448548.032471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/100: 100%|██████████| 6253/6253 [00:47<00:00, 130.81batch/s, train_loss=1.65e+3]\n",
      "Validation Epoch 55/100: 100%|██████████| 1563/1563 [00:06<00:00, 240.17batch/s, val_loss=1.55e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55 \t Training Loss: 4238151346.670898 \t Validation Loss: 1264634872.462769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/100: 100%|██████████| 6253/6253 [00:47<00:00, 131.80batch/s, train_loss=1.56e+3]\n",
      "Validation Epoch 56/100: 100%|██████████| 1563/1563 [00:06<00:00, 240.62batch/s, val_loss=1.55e+3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56 \t Training Loss: 4233743860.497437 \t Validation Loss: 1278507903.523071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/100: 100%|██████████| 6253/6253 [00:47<00:00, 130.61batch/s, train_loss=1.58e+3]\n",
      "Validation Epoch 57/100: 100%|██████████| 1563/1563 [00:06<00:00, 239.34batch/s, val_loss=1.55e+3]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57 \t Training Loss: 4247348807.530151 \t Validation Loss: 1269764739.228271\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 루프\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # 훈련 데이터 로더에 대한 프로그레스 바 추가\n",
    "    train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    for data, target in train_progress_bar:\n",
    "        data, target = data.to(current_device), target.to(current_device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "        # 프로그레스 바 업데이트\n",
    "        train_progress_bar.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    # 검증 데이터 로더에 대한 프로그레스 바 추가\n",
    "    val_progress_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_progress_bar:\n",
    "            data, target = data.to(current_device), target.to(current_device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "            # 프로그레스 바 업데이트\n",
    "            val_progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader.dataset))\n",
    "    print(f'Epoch {epoch+1} \\t Training Loss: {train_loss:.6f} \\t Validation Loss: {val_loss:.6f}')\n",
    "    \n",
    "    # Learning Rate Scheduler 업데이트\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 가장 좋은 모델 가중치 로드\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model_Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./B01_TransformData_FinalAvatar_20230922_171230.csv').iloc[300:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Time</th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>301</td>\n",
       "      <td>3.516298</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>351.9312</td>\n",
       "      <td>358.1726</td>\n",
       "      <td>282.4885</td>\n",
       "      <td>0.484731</td>\n",
       "      <td>0.254242</td>\n",
       "      <td>1.712456</td>\n",
       "      <td>339.4850</td>\n",
       "      <td>296.8258</td>\n",
       "      <td>305.4126</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>302</td>\n",
       "      <td>3.527282</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>350.7178</td>\n",
       "      <td>357.2758</td>\n",
       "      <td>281.4112</td>\n",
       "      <td>0.473791</td>\n",
       "      <td>0.253382</td>\n",
       "      <td>1.724783</td>\n",
       "      <td>337.8150</td>\n",
       "      <td>292.9910</td>\n",
       "      <td>307.2033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>3.538654</td>\n",
       "      <td>0.167637</td>\n",
       "      <td>0.309281</td>\n",
       "      <td>1.707612</td>\n",
       "      <td>357.083</td>\n",
       "      <td>356.5699</td>\n",
       "      <td>359.1780</td>\n",
       "      <td>0.110527</td>\n",
       "      <td>0.229474</td>\n",
       "      <td>...</td>\n",
       "      <td>349.4878</td>\n",
       "      <td>356.3683</td>\n",
       "      <td>280.3726</td>\n",
       "      <td>0.462770</td>\n",
       "      <td>0.253029</td>\n",
       "      <td>1.736980</td>\n",
       "      <td>336.0336</td>\n",
       "      <td>289.2522</td>\n",
       "      <td>308.9090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>304</td>\n",
       "      <td>3.549650</td>\n",
       "      <td>0.169145</td>\n",
       "      <td>0.316027</td>\n",
       "      <td>1.705714</td>\n",
       "      <td>357.288</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>0.112322</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>...</td>\n",
       "      <td>348.2825</td>\n",
       "      <td>355.5042</td>\n",
       "      <td>279.3825</td>\n",
       "      <td>0.452501</td>\n",
       "      <td>0.258765</td>\n",
       "      <td>1.747534</td>\n",
       "      <td>334.1286</td>\n",
       "      <td>285.3405</td>\n",
       "      <td>310.8063</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>305</td>\n",
       "      <td>3.561161</td>\n",
       "      <td>0.169145</td>\n",
       "      <td>0.316027</td>\n",
       "      <td>1.705714</td>\n",
       "      <td>357.288</td>\n",
       "      <td>355.9712</td>\n",
       "      <td>359.2451</td>\n",
       "      <td>0.112322</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>...</td>\n",
       "      <td>346.9774</td>\n",
       "      <td>354.4922</td>\n",
       "      <td>278.4650</td>\n",
       "      <td>0.441532</td>\n",
       "      <td>0.259532</td>\n",
       "      <td>1.759929</td>\n",
       "      <td>332.4753</td>\n",
       "      <td>281.4930</td>\n",
       "      <td>312.3235</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Frame      Time  m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  \\\n",
       "300    301  3.516298          0.167637          0.309281          1.707612   \n",
       "301    302  3.527282          0.167637          0.309281          1.707612   \n",
       "302    303  3.538654          0.167637          0.309281          1.707612   \n",
       "303    304  3.549650          0.169145          0.316027          1.705714   \n",
       "304    305  3.561161          0.169145          0.316027          1.705714   \n",
       "\n",
       "     m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  m_avg_L_HipPosX  \\\n",
       "300           357.083          356.5699          359.1780         0.110527   \n",
       "301           357.083          356.5699          359.1780         0.110527   \n",
       "302           357.083          356.5699          359.1780         0.110527   \n",
       "303           357.288          355.9712          359.2451         0.112322   \n",
       "304           357.288          355.9712          359.2451         0.112322   \n",
       "\n",
       "     m_avg_L_HipPosY  ...  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "300         0.229474  ...           351.9312           358.1726   \n",
       "301         0.229474  ...           350.7178           357.2758   \n",
       "302         0.229474  ...           349.4878           356.3683   \n",
       "303         0.236200  ...           348.2825           355.5042   \n",
       "304         0.236200  ...           346.9774           354.4922   \n",
       "\n",
       "     m_avg_R_ElbowRotZ  m_avg_R_WristPosX  m_avg_R_WristPosY  \\\n",
       "300           282.4885           0.484731           0.254242   \n",
       "301           281.4112           0.473791           0.253382   \n",
       "302           280.3726           0.462770           0.253029   \n",
       "303           279.3825           0.452501           0.258765   \n",
       "304           278.4650           0.441532           0.259532   \n",
       "\n",
       "     m_avg_R_WristPosZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "300           1.712456           339.4850           296.8258   \n",
       "301           1.724783           337.8150           292.9910   \n",
       "302           1.736980           336.0336           289.2522   \n",
       "303           1.747534           334.1286           285.3405   \n",
       "304           1.759929           332.4753           281.4930   \n",
       "\n",
       "     m_avg_R_WristRotZ  Unnamed: 128  \n",
       "300           305.4126           NaN  \n",
       "301           307.2033           NaN  \n",
       "302           308.9090           NaN  \n",
       "303           310.8063           NaN  \n",
       "304           312.3235           NaN  \n",
       "\n",
       "[5 rows x 129 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rotation_columns = [col for col in test_df.columns if 'Rot' in col]\n",
    "\n",
    "test_position_columns = [col for col in test_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "test_rotation_df = test_df[test_rotation_columns]\n",
    "test_position_df = test_df[test_position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "test_rotation_df = test_rotation_df.apply(lambda col: col.apply(normalize_angle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어나는 값이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = (test_rotation_df > 180).sum().sum() + (test_df < -180).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position 컬럼은 그대로 두고, 다시 합치기\n",
    "test_posrot_df = pd.concat([test_position_df, test_rotation_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posrot_df = test_posrot_df.map(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_KneePosX</th>\n",
       "      <th>m_avg_L_KneePosY</th>\n",
       "      <th>m_avg_L_KneePosZ</th>\n",
       "      <th>m_avg_L_AnklePosX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-13.38</td>\n",
       "      <td>-3.53</td>\n",
       "      <td>-72.61</td>\n",
       "      <td>-8.07</td>\n",
       "      <td>-1.83</td>\n",
       "      <td>-77.51</td>\n",
       "      <td>-20.51</td>\n",
       "      <td>-63.17</td>\n",
       "      <td>-54.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-14.63</td>\n",
       "      <td>-4.33</td>\n",
       "      <td>-73.67</td>\n",
       "      <td>-9.28</td>\n",
       "      <td>-2.72</td>\n",
       "      <td>-78.59</td>\n",
       "      <td>-22.19</td>\n",
       "      <td>-67.01</td>\n",
       "      <td>-52.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.23</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.95</td>\n",
       "      <td>-15.89</td>\n",
       "      <td>-5.14</td>\n",
       "      <td>-74.70</td>\n",
       "      <td>-10.51</td>\n",
       "      <td>-3.63</td>\n",
       "      <td>-79.63</td>\n",
       "      <td>-23.97</td>\n",
       "      <td>-70.75</td>\n",
       "      <td>-51.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.36</td>\n",
       "      <td>-17.12</td>\n",
       "      <td>-5.92</td>\n",
       "      <td>-75.68</td>\n",
       "      <td>-11.72</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>-80.62</td>\n",
       "      <td>-25.87</td>\n",
       "      <td>-74.66</td>\n",
       "      <td>-49.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1.71</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.11</td>\n",
       "      <td>...</td>\n",
       "      <td>-10.36</td>\n",
       "      <td>-18.44</td>\n",
       "      <td>-6.85</td>\n",
       "      <td>-76.59</td>\n",
       "      <td>-13.02</td>\n",
       "      <td>-5.51</td>\n",
       "      <td>-81.54</td>\n",
       "      <td>-27.52</td>\n",
       "      <td>-78.51</td>\n",
       "      <td>-47.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.95</td>\n",
       "      <td>21.91</td>\n",
       "      <td>6.28</td>\n",
       "      <td>10.27</td>\n",
       "      <td>27.53</td>\n",
       "      <td>2.73</td>\n",
       "      <td>-4.35</td>\n",
       "      <td>-4.97</td>\n",
       "      <td>20.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4672</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.97</td>\n",
       "      <td>21.81</td>\n",
       "      <td>6.17</td>\n",
       "      <td>10.30</td>\n",
       "      <td>27.43</td>\n",
       "      <td>2.62</td>\n",
       "      <td>-4.38</td>\n",
       "      <td>-5.10</td>\n",
       "      <td>20.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.57</td>\n",
       "      <td>10.97</td>\n",
       "      <td>21.70</td>\n",
       "      <td>6.11</td>\n",
       "      <td>10.31</td>\n",
       "      <td>27.33</td>\n",
       "      <td>2.57</td>\n",
       "      <td>-4.40</td>\n",
       "      <td>-5.26</td>\n",
       "      <td>20.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4674</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.76</td>\n",
       "      <td>10.99</td>\n",
       "      <td>21.59</td>\n",
       "      <td>6.03</td>\n",
       "      <td>10.34</td>\n",
       "      <td>27.22</td>\n",
       "      <td>2.49</td>\n",
       "      <td>-4.34</td>\n",
       "      <td>-5.49</td>\n",
       "      <td>19.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4675</th>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.11</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.23</td>\n",
       "      <td>2.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.76</td>\n",
       "      <td>11.00</td>\n",
       "      <td>21.51</td>\n",
       "      <td>5.99</td>\n",
       "      <td>10.35</td>\n",
       "      <td>27.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>-4.24</td>\n",
       "      <td>-5.60</td>\n",
       "      <td>19.77</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4376 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  m_avg_L_HipPosX  \\\n",
       "300               0.17              0.31              1.71             0.11   \n",
       "301               0.17              0.31              1.71             0.11   \n",
       "302               0.17              0.31              1.71             0.11   \n",
       "303               0.17              0.32              1.71             0.11   \n",
       "304               0.17              0.32              1.71             0.11   \n",
       "...                ...               ...               ...              ...   \n",
       "4671              0.13              0.22              2.13             0.07   \n",
       "4672              0.13              0.22              2.13             0.07   \n",
       "4673              0.13              0.22              2.13             0.07   \n",
       "4674              0.13              0.22              2.13             0.07   \n",
       "4675              0.13              0.22              2.13             0.07   \n",
       "\n",
       "      m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_KneePosX  m_avg_L_KneePosY  \\\n",
       "300              0.23             1.69              0.08             -0.15   \n",
       "301              0.23             1.69              0.08             -0.15   \n",
       "302              0.23             1.69              0.08             -0.15   \n",
       "303              0.24             1.69              0.08             -0.14   \n",
       "304              0.24             1.69              0.08             -0.14   \n",
       "...               ...              ...               ...               ...   \n",
       "4671             0.14             2.11              0.05             -0.23   \n",
       "4672             0.14             2.11              0.05             -0.23   \n",
       "4673             0.14             2.11              0.05             -0.23   \n",
       "4674             0.14             2.11              0.05             -0.23   \n",
       "4675             0.14             2.11              0.05             -0.23   \n",
       "\n",
       "      m_avg_L_KneePosZ  m_avg_L_AnklePosX  ...  m_avg_R_CollarRotZ  \\\n",
       "300               1.68               0.10  ...               -9.95   \n",
       "301               1.68               0.10  ...               -9.95   \n",
       "302               1.68               0.10  ...               -9.95   \n",
       "303               1.69               0.11  ...              -10.36   \n",
       "304               1.69               0.11  ...              -10.36   \n",
       "...                ...                ...  ...                 ...   \n",
       "4671              2.14               0.08  ...               -6.57   \n",
       "4672              2.14               0.08  ...               -6.57   \n",
       "4673              2.14               0.08  ...               -6.57   \n",
       "4674              2.13               0.09  ...               -6.76   \n",
       "4675              2.13               0.09  ...               -6.76   \n",
       "\n",
       "      m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "300                 -13.38                 -3.53                -72.61   \n",
       "301                 -14.63                 -4.33                -73.67   \n",
       "302                 -15.89                 -5.14                -74.70   \n",
       "303                 -17.12                 -5.92                -75.68   \n",
       "304                 -18.44                 -6.85                -76.59   \n",
       "...                    ...                   ...                   ...   \n",
       "4671                 10.95                 21.91                  6.28   \n",
       "4672                 10.97                 21.81                  6.17   \n",
       "4673                 10.97                 21.70                  6.11   \n",
       "4674                 10.99                 21.59                  6.03   \n",
       "4675                 11.00                 21.51                  5.99   \n",
       "\n",
       "      m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "300               -8.07              -1.83             -77.51   \n",
       "301               -9.28              -2.72             -78.59   \n",
       "302              -10.51              -3.63             -79.63   \n",
       "303              -11.72              -4.50             -80.62   \n",
       "304              -13.02              -5.51             -81.54   \n",
       "...                 ...                ...                ...   \n",
       "4671              10.27              27.53               2.73   \n",
       "4672              10.30              27.43               2.62   \n",
       "4673              10.31              27.33               2.57   \n",
       "4674              10.34              27.22               2.49   \n",
       "4675              10.35              27.15               2.45   \n",
       "\n",
       "      m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "300              -20.51             -63.17             -54.59  \n",
       "301              -22.19             -67.01             -52.80  \n",
       "302              -23.97             -70.75             -51.09  \n",
       "303              -25.87             -74.66             -49.19  \n",
       "304              -27.52             -78.51             -47.68  \n",
       "...                 ...                ...                ...  \n",
       "4671              -4.35              -4.97              20.79  \n",
       "4672              -4.38              -5.10              20.49  \n",
       "4673              -4.40              -5.26              20.18  \n",
       "4674              -4.34              -5.49              19.92  \n",
       "4675              -4.24              -5.60              19.77  \n",
       "\n",
       "[4376 rows x 126 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 새로운 CSV 파일로 저장합니다.\n",
    "test_posrot_df.to_csv('./test_posrot_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = TransformerModel(\n",
    "    n_features=n_features,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_blocks=num_blocks,\n",
    "    mlp_units=mlp_units,\n",
    "    dropout=dropout_rate,\n",
    "    n_input=n_input\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('final_model_Transformer.pth'))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 값을 넣을 빈 리스트\n",
    "test_predictions = []\n",
    "\n",
    "# 훈련 데이터셋에서 마지막 입력 개수의 값을 가져온 후\n",
    "current_batch = torch.from_numpy(test_posrot_df[-n_input:].values.astype(np.float32)).reshape((1, n_input, n_features))\n",
    "\n",
    "# 모델이 사용하는 디바이스를 확인하고 데이터를 해당 디바이스로 옮깁니다.\n",
    "current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(current_device)\n",
    "current_batch = current_batch.to(current_device)\n",
    "\n",
    "# 예측 과정 반복\n",
    "with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "    for i in range(1):\n",
    "        # 현재 배치에서 다음 포인트를 예측\n",
    "        current_pred = model(current_batch).cpu().numpy()[0]  # 마지막 시퀀스 포인트 예측\n",
    "        current_pred = np.array([normalize_angle(y) for y in current_pred])  # 예측값 정규화\n",
    "\n",
    "        # 예측된 마지막 프레임을 리스트에 추가\n",
    "        test_predictions.append(current_pred)\n",
    "\n",
    "        # 새로운 배치 생성: 마지막 시퀀스 제외하고 예측값 추가\n",
    "        current_batch = np.roll(current_batch.cpu().numpy(), -1, axis=1)\n",
    "        current_batch[:, -1, :] = current_pred\n",
    "        current_batch = torch.from_numpy(current_batch).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_array = np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 배열을 데이터프레임으로 변환합니다. 이때 column_order 리스트를 열 이름으로 사용합니다.\n",
    "test_predictions = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "# test_predictions 데이터프레임을 CSV 파일로 저장합니다.\n",
    "test_predictions.to_csv('./test_predictions.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_KneePosX</th>\n",
       "      <th>m_avg_L_KneePosY</th>\n",
       "      <th>m_avg_L_KneePosZ</th>\n",
       "      <th>m_avg_L_AnklePosX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.92031</td>\n",
       "      <td>-3.534008</td>\n",
       "      <td>-2.632143</td>\n",
       "      <td>-3.997756</td>\n",
       "      <td>-3.614383</td>\n",
       "      <td>-2.648881</td>\n",
       "      <td>-4.036597</td>\n",
       "      <td>-3.976387</td>\n",
       "      <td>-2.598897</td>\n",
       "      <td>-4.010202</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.678426</td>\n",
       "      <td>-5.941551</td>\n",
       "      <td>-4.171821</td>\n",
       "      <td>-15.924047</td>\n",
       "      <td>-5.421728</td>\n",
       "      <td>32.368019</td>\n",
       "      <td>-20.490168</td>\n",
       "      <td>-6.409177</td>\n",
       "      <td>-39.958096</td>\n",
       "      <td>16.002148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  m_avg_L_HipPosX  \\\n",
       "0          -3.92031         -3.534008         -2.632143        -3.997756   \n",
       "\n",
       "   m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_KneePosX  m_avg_L_KneePosY  \\\n",
       "0        -3.614383        -2.648881         -4.036597         -3.976387   \n",
       "\n",
       "   m_avg_L_KneePosZ  m_avg_L_AnklePosX  ...  m_avg_R_CollarRotZ  \\\n",
       "0         -2.598897          -4.010202  ...          -12.678426   \n",
       "\n",
       "   m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "0             -5.941551             -4.171821            -15.924047   \n",
       "\n",
       "   m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  m_avg_R_WristRotX  \\\n",
       "0          -5.421728          32.368019         -20.490168          -6.409177   \n",
       "\n",
       "   m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "0         -39.958096          16.002148  \n",
       "\n",
       "[1 rows x 126 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 데이터(test_df)의 마지막 30 프레임과 \n",
    "last_inputs_df = test_posrot_df.iloc[-30:][column_order].reset_index(drop=True)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "test_combined_df = pd.concat([last_inputs_df, test_predictions_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_KneePosX</th>\n",
       "      <th>m_avg_L_KneePosY</th>\n",
       "      <th>m_avg_L_KneePosZ</th>\n",
       "      <th>m_avg_L_AnklePosX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_CollarRotZ</th>\n",
       "      <th>m_avg_R_ShoulderRotX</th>\n",
       "      <th>m_avg_R_ShoulderRotY</th>\n",
       "      <th>m_avg_R_ShoulderRotZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.14000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>-0.210000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>...</td>\n",
       "      <td>-22.950000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>25.170000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>9.240000</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>-1.590000</td>\n",
       "      <td>-1.080000</td>\n",
       "      <td>28.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.190000</td>\n",
       "      <td>25.240000</td>\n",
       "      <td>9.640000</td>\n",
       "      <td>9.190000</td>\n",
       "      <td>30.800000</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>-1.950000</td>\n",
       "      <td>-1.220000</td>\n",
       "      <td>28.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.210000</td>\n",
       "      <td>25.230000</td>\n",
       "      <td>9.700000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>30.790000</td>\n",
       "      <td>6.050000</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>-1.390000</td>\n",
       "      <td>28.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.10000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.190000</td>\n",
       "      <td>-0.150000</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.760000</td>\n",
       "      <td>10.220000</td>\n",
       "      <td>25.190000</td>\n",
       "      <td>9.740000</td>\n",
       "      <td>9.210000</td>\n",
       "      <td>30.750000</td>\n",
       "      <td>6.090000</td>\n",
       "      <td>-2.550000</td>\n",
       "      <td>-1.550000</td>\n",
       "      <td>28.440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>25.120000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>9.240000</td>\n",
       "      <td>30.680000</td>\n",
       "      <td>6.060000</td>\n",
       "      <td>-2.900000</td>\n",
       "      <td>-1.710000</td>\n",
       "      <td>28.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>25.040000</td>\n",
       "      <td>9.660000</td>\n",
       "      <td>9.270000</td>\n",
       "      <td>30.600000</td>\n",
       "      <td>6.020000</td>\n",
       "      <td>-3.260000</td>\n",
       "      <td>-1.870000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>2.120000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-17.430000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>24.950000</td>\n",
       "      <td>9.570000</td>\n",
       "      <td>9.310000</td>\n",
       "      <td>30.510000</td>\n",
       "      <td>5.940000</td>\n",
       "      <td>-3.590000</td>\n",
       "      <td>-2.080000</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.350000</td>\n",
       "      <td>24.770000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>9.370000</td>\n",
       "      <td>30.330000</td>\n",
       "      <td>5.770000</td>\n",
       "      <td>-3.910000</td>\n",
       "      <td>-2.330000</td>\n",
       "      <td>28.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.400000</td>\n",
       "      <td>24.610000</td>\n",
       "      <td>9.230000</td>\n",
       "      <td>9.440000</td>\n",
       "      <td>30.180000</td>\n",
       "      <td>5.610000</td>\n",
       "      <td>-4.220000</td>\n",
       "      <td>-2.600000</td>\n",
       "      <td>27.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.610000</td>\n",
       "      <td>10.440000</td>\n",
       "      <td>24.460000</td>\n",
       "      <td>9.050000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>30.030000</td>\n",
       "      <td>5.430000</td>\n",
       "      <td>-4.470000</td>\n",
       "      <td>-2.940000</td>\n",
       "      <td>27.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.510000</td>\n",
       "      <td>24.260000</td>\n",
       "      <td>8.810000</td>\n",
       "      <td>9.590000</td>\n",
       "      <td>29.840000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>-4.780000</td>\n",
       "      <td>-3.220000</td>\n",
       "      <td>26.340000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.560000</td>\n",
       "      <td>24.080000</td>\n",
       "      <td>8.630000</td>\n",
       "      <td>9.650000</td>\n",
       "      <td>29.670000</td>\n",
       "      <td>5.020000</td>\n",
       "      <td>-5.040000</td>\n",
       "      <td>-3.480000</td>\n",
       "      <td>25.640000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.110000</td>\n",
       "      <td>10.610000</td>\n",
       "      <td>23.890000</td>\n",
       "      <td>8.380000</td>\n",
       "      <td>9.730000</td>\n",
       "      <td>29.480000</td>\n",
       "      <td>4.790000</td>\n",
       "      <td>-5.260000</td>\n",
       "      <td>-3.680000</td>\n",
       "      <td>25.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.660000</td>\n",
       "      <td>23.700000</td>\n",
       "      <td>8.160000</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>29.300000</td>\n",
       "      <td>4.570000</td>\n",
       "      <td>-5.220000</td>\n",
       "      <td>-3.750000</td>\n",
       "      <td>24.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.700000</td>\n",
       "      <td>23.520000</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>9.860000</td>\n",
       "      <td>29.110000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>-5.230000</td>\n",
       "      <td>-3.910000</td>\n",
       "      <td>24.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.560000</td>\n",
       "      <td>10.740000</td>\n",
       "      <td>23.330000</td>\n",
       "      <td>7.720000</td>\n",
       "      <td>9.920000</td>\n",
       "      <td>28.930000</td>\n",
       "      <td>4.140000</td>\n",
       "      <td>-5.030000</td>\n",
       "      <td>-4.060000</td>\n",
       "      <td>23.690000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.770000</td>\n",
       "      <td>23.140000</td>\n",
       "      <td>7.520000</td>\n",
       "      <td>9.970000</td>\n",
       "      <td>28.740000</td>\n",
       "      <td>3.940000</td>\n",
       "      <td>-4.810000</td>\n",
       "      <td>-4.180000</td>\n",
       "      <td>23.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.960000</td>\n",
       "      <td>7.330000</td>\n",
       "      <td>10.020000</td>\n",
       "      <td>28.570000</td>\n",
       "      <td>3.760000</td>\n",
       "      <td>-4.670000</td>\n",
       "      <td>-4.340000</td>\n",
       "      <td>22.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.550000</td>\n",
       "      <td>10.830000</td>\n",
       "      <td>22.780000</td>\n",
       "      <td>7.150000</td>\n",
       "      <td>10.070000</td>\n",
       "      <td>28.390000</td>\n",
       "      <td>3.590000</td>\n",
       "      <td>-4.520000</td>\n",
       "      <td>-4.500000</td>\n",
       "      <td>22.040000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.860000</td>\n",
       "      <td>22.620000</td>\n",
       "      <td>6.990000</td>\n",
       "      <td>10.110000</td>\n",
       "      <td>28.230000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>-4.450000</td>\n",
       "      <td>-4.620000</td>\n",
       "      <td>21.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.880000</td>\n",
       "      <td>22.460000</td>\n",
       "      <td>6.840000</td>\n",
       "      <td>10.150000</td>\n",
       "      <td>28.080000</td>\n",
       "      <td>3.280000</td>\n",
       "      <td>-4.400000</td>\n",
       "      <td>-4.690000</td>\n",
       "      <td>21.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.540000</td>\n",
       "      <td>10.900000</td>\n",
       "      <td>22.330000</td>\n",
       "      <td>6.710000</td>\n",
       "      <td>10.180000</td>\n",
       "      <td>27.940000</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>-4.330000</td>\n",
       "      <td>-4.750000</td>\n",
       "      <td>21.050000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.910000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>6.580000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>27.820000</td>\n",
       "      <td>3.030000</td>\n",
       "      <td>-4.290000</td>\n",
       "      <td>-4.810000</td>\n",
       "      <td>20.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.930000</td>\n",
       "      <td>22.100000</td>\n",
       "      <td>6.460000</td>\n",
       "      <td>10.230000</td>\n",
       "      <td>27.730000</td>\n",
       "      <td>2.910000</td>\n",
       "      <td>-4.300000</td>\n",
       "      <td>-4.870000</td>\n",
       "      <td>20.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.650000</td>\n",
       "      <td>10.940000</td>\n",
       "      <td>22.020000</td>\n",
       "      <td>6.360000</td>\n",
       "      <td>10.250000</td>\n",
       "      <td>27.640000</td>\n",
       "      <td>2.810000</td>\n",
       "      <td>-4.320000</td>\n",
       "      <td>-4.910000</td>\n",
       "      <td>20.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.950000</td>\n",
       "      <td>21.910000</td>\n",
       "      <td>6.280000</td>\n",
       "      <td>10.270000</td>\n",
       "      <td>27.530000</td>\n",
       "      <td>2.730000</td>\n",
       "      <td>-4.350000</td>\n",
       "      <td>-4.970000</td>\n",
       "      <td>20.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.970000</td>\n",
       "      <td>21.810000</td>\n",
       "      <td>6.170000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>27.430000</td>\n",
       "      <td>2.620000</td>\n",
       "      <td>-4.380000</td>\n",
       "      <td>-5.100000</td>\n",
       "      <td>20.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.570000</td>\n",
       "      <td>10.970000</td>\n",
       "      <td>21.700000</td>\n",
       "      <td>6.110000</td>\n",
       "      <td>10.310000</td>\n",
       "      <td>27.330000</td>\n",
       "      <td>2.570000</td>\n",
       "      <td>-4.400000</td>\n",
       "      <td>-5.260000</td>\n",
       "      <td>20.180000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.760000</td>\n",
       "      <td>10.990000</td>\n",
       "      <td>21.590000</td>\n",
       "      <td>6.030000</td>\n",
       "      <td>10.340000</td>\n",
       "      <td>27.220000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>-4.340000</td>\n",
       "      <td>-5.490000</td>\n",
       "      <td>19.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.13000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>2.110000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>-0.230000</td>\n",
       "      <td>2.130000</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.760000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>21.510000</td>\n",
       "      <td>5.990000</td>\n",
       "      <td>10.350000</td>\n",
       "      <td>27.150000</td>\n",
       "      <td>2.450000</td>\n",
       "      <td>-4.240000</td>\n",
       "      <td>-5.600000</td>\n",
       "      <td>19.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>-3.92031</td>\n",
       "      <td>-3.534008</td>\n",
       "      <td>-2.632143</td>\n",
       "      <td>-3.997756</td>\n",
       "      <td>-3.614383</td>\n",
       "      <td>-2.648881</td>\n",
       "      <td>-4.036597</td>\n",
       "      <td>-3.976387</td>\n",
       "      <td>-2.598897</td>\n",
       "      <td>-4.010202</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.678426</td>\n",
       "      <td>-5.941551</td>\n",
       "      <td>-4.171821</td>\n",
       "      <td>-15.924047</td>\n",
       "      <td>-5.421728</td>\n",
       "      <td>32.368019</td>\n",
       "      <td>-20.490168</td>\n",
       "      <td>-6.409177</td>\n",
       "      <td>-39.958096</td>\n",
       "      <td>16.002148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  m_avg_L_HipPosX  \\\n",
       "0            0.14000          0.240000          2.130000         0.080000   \n",
       "1            0.10000          0.200000          2.140000         0.140000   \n",
       "2            0.10000          0.200000          2.140000         0.140000   \n",
       "3            0.10000          0.200000          2.140000         0.140000   \n",
       "4            0.13000          0.210000          2.130000         0.070000   \n",
       "5            0.13000          0.210000          2.130000         0.070000   \n",
       "6            0.13000          0.210000          2.130000         0.070000   \n",
       "7            0.13000          0.220000          2.130000         0.070000   \n",
       "8            0.13000          0.220000          2.130000         0.070000   \n",
       "9            0.13000          0.220000          2.130000         0.070000   \n",
       "10           0.13000          0.220000          2.130000         0.070000   \n",
       "11           0.13000          0.220000          2.130000         0.070000   \n",
       "12           0.13000          0.220000          2.130000         0.070000   \n",
       "13           0.13000          0.220000          2.130000         0.070000   \n",
       "14           0.13000          0.220000          2.130000         0.070000   \n",
       "15           0.13000          0.220000          2.130000         0.070000   \n",
       "16           0.13000          0.220000          2.130000         0.070000   \n",
       "17           0.13000          0.220000          2.130000         0.070000   \n",
       "18           0.13000          0.220000          2.130000         0.070000   \n",
       "19           0.13000          0.220000          2.130000         0.070000   \n",
       "20           0.13000          0.220000          2.130000         0.070000   \n",
       "21           0.13000          0.220000          2.130000         0.070000   \n",
       "22           0.13000          0.220000          2.130000         0.070000   \n",
       "23           0.13000          0.220000          2.130000         0.070000   \n",
       "24           0.13000          0.220000          2.130000         0.070000   \n",
       "25           0.13000          0.220000          2.130000         0.070000   \n",
       "26           0.13000          0.220000          2.130000         0.070000   \n",
       "27           0.13000          0.220000          2.130000         0.070000   \n",
       "28           0.13000          0.220000          2.130000         0.070000   \n",
       "29           0.13000          0.220000          2.130000         0.070000   \n",
       "30          -3.92031         -3.534008         -2.632143        -3.997756   \n",
       "\n",
       "    m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_KneePosX  m_avg_L_KneePosY  \\\n",
       "0          0.160000         2.110000          0.070000         -0.210000   \n",
       "1          0.120000         2.100000          0.080000         -0.240000   \n",
       "2          0.120000         2.100000          0.080000         -0.240000   \n",
       "3          0.120000         2.100000          0.080000         -0.240000   \n",
       "4          0.130000         2.110000          0.050000         -0.240000   \n",
       "5          0.130000         2.110000          0.050000         -0.240000   \n",
       "6          0.130000         2.110000          0.050000         -0.240000   \n",
       "7          0.140000         2.110000          0.050000         -0.230000   \n",
       "8          0.140000         2.110000          0.050000         -0.230000   \n",
       "9          0.140000         2.110000          0.050000         -0.230000   \n",
       "10         0.140000         2.110000          0.050000         -0.230000   \n",
       "11         0.140000         2.110000          0.050000         -0.230000   \n",
       "12         0.140000         2.110000          0.050000         -0.230000   \n",
       "13         0.140000         2.110000          0.050000         -0.230000   \n",
       "14         0.140000         2.110000          0.050000         -0.230000   \n",
       "15         0.140000         2.110000          0.050000         -0.230000   \n",
       "16         0.140000         2.110000          0.050000         -0.230000   \n",
       "17         0.140000         2.110000          0.050000         -0.230000   \n",
       "18         0.140000         2.110000          0.050000         -0.230000   \n",
       "19         0.140000         2.110000          0.050000         -0.230000   \n",
       "20         0.140000         2.110000          0.050000         -0.230000   \n",
       "21         0.140000         2.110000          0.050000         -0.230000   \n",
       "22         0.140000         2.110000          0.050000         -0.230000   \n",
       "23         0.140000         2.110000          0.050000         -0.230000   \n",
       "24         0.140000         2.110000          0.050000         -0.230000   \n",
       "25         0.140000         2.110000          0.050000         -0.230000   \n",
       "26         0.140000         2.110000          0.050000         -0.230000   \n",
       "27         0.140000         2.110000          0.050000         -0.230000   \n",
       "28         0.140000         2.110000          0.050000         -0.230000   \n",
       "29         0.140000         2.110000          0.050000         -0.230000   \n",
       "30        -3.614383        -2.648881         -4.036597         -3.976387   \n",
       "\n",
       "    m_avg_L_KneePosZ  m_avg_L_AnklePosX  ...  m_avg_R_CollarRotZ  \\\n",
       "0           2.140000           0.120000  ...          -22.950000   \n",
       "1           2.190000          -0.150000  ...           -9.760000   \n",
       "2           2.190000          -0.150000  ...           -9.760000   \n",
       "3           2.190000          -0.150000  ...           -9.760000   \n",
       "4           2.120000           0.070000  ...          -17.430000   \n",
       "5           2.120000           0.070000  ...          -17.430000   \n",
       "6           2.120000           0.070000  ...          -17.430000   \n",
       "7           2.130000           0.070000  ...          -12.610000   \n",
       "8           2.130000           0.070000  ...          -12.610000   \n",
       "9           2.130000           0.070000  ...          -12.610000   \n",
       "10          2.130000           0.070000  ...          -12.110000   \n",
       "11          2.130000           0.070000  ...          -12.110000   \n",
       "12          2.130000           0.070000  ...          -12.110000   \n",
       "13          2.130000           0.070000  ...          -11.560000   \n",
       "14          2.130000           0.070000  ...          -11.560000   \n",
       "15          2.130000           0.070000  ...          -11.560000   \n",
       "16          2.130000           0.070000  ...          -11.550000   \n",
       "17          2.130000           0.070000  ...          -11.550000   \n",
       "18          2.130000           0.070000  ...          -11.550000   \n",
       "19          2.130000           0.070000  ...          -11.540000   \n",
       "20          2.130000           0.070000  ...          -11.540000   \n",
       "21          2.130000           0.070000  ...          -11.540000   \n",
       "22          2.130000           0.080000  ...           -7.650000   \n",
       "23          2.130000           0.080000  ...           -7.650000   \n",
       "24          2.130000           0.080000  ...           -7.650000   \n",
       "25          2.140000           0.080000  ...           -6.570000   \n",
       "26          2.140000           0.080000  ...           -6.570000   \n",
       "27          2.140000           0.080000  ...           -6.570000   \n",
       "28          2.130000           0.090000  ...           -6.760000   \n",
       "29          2.130000           0.090000  ...           -6.760000   \n",
       "30         -2.598897          -4.010202  ...          -12.678426   \n",
       "\n",
       "    m_avg_R_ShoulderRotX  m_avg_R_ShoulderRotY  m_avg_R_ShoulderRotZ  \\\n",
       "0              10.220000             25.170000              9.440000   \n",
       "1              10.190000             25.240000              9.640000   \n",
       "2              10.210000             25.230000              9.700000   \n",
       "3              10.220000             25.190000              9.740000   \n",
       "4              10.250000             25.120000              9.710000   \n",
       "5              10.270000             25.040000              9.660000   \n",
       "6              10.300000             24.950000              9.570000   \n",
       "7              10.350000             24.770000              9.400000   \n",
       "8              10.400000             24.610000              9.230000   \n",
       "9              10.440000             24.460000              9.050000   \n",
       "10             10.510000             24.260000              8.810000   \n",
       "11             10.560000             24.080000              8.630000   \n",
       "12             10.610000             23.890000              8.380000   \n",
       "13             10.660000             23.700000              8.160000   \n",
       "14             10.700000             23.520000              7.930000   \n",
       "15             10.740000             23.330000              7.720000   \n",
       "16             10.770000             23.140000              7.520000   \n",
       "17             10.800000             22.960000              7.330000   \n",
       "18             10.830000             22.780000              7.150000   \n",
       "19             10.860000             22.620000              6.990000   \n",
       "20             10.880000             22.460000              6.840000   \n",
       "21             10.900000             22.330000              6.710000   \n",
       "22             10.910000             22.200000              6.580000   \n",
       "23             10.930000             22.100000              6.460000   \n",
       "24             10.940000             22.020000              6.360000   \n",
       "25             10.950000             21.910000              6.280000   \n",
       "26             10.970000             21.810000              6.170000   \n",
       "27             10.970000             21.700000              6.110000   \n",
       "28             10.990000             21.590000              6.030000   \n",
       "29             11.000000             21.510000              5.990000   \n",
       "30             -5.941551             -4.171821            -15.924047   \n",
       "\n",
       "    m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "0            9.240000          30.740000           5.800000   \n",
       "1            9.190000          30.800000           5.990000   \n",
       "2            9.200000          30.790000           6.050000   \n",
       "3            9.210000          30.750000           6.090000   \n",
       "4            9.240000          30.680000           6.060000   \n",
       "5            9.270000          30.600000           6.020000   \n",
       "6            9.310000          30.510000           5.940000   \n",
       "7            9.370000          30.330000           5.770000   \n",
       "8            9.440000          30.180000           5.610000   \n",
       "9            9.500000          30.030000           5.430000   \n",
       "10           9.590000          29.840000           5.200000   \n",
       "11           9.650000          29.670000           5.020000   \n",
       "12           9.730000          29.480000           4.790000   \n",
       "13           9.800000          29.300000           4.570000   \n",
       "14           9.860000          29.110000           4.350000   \n",
       "15           9.920000          28.930000           4.140000   \n",
       "16           9.970000          28.740000           3.940000   \n",
       "17          10.020000          28.570000           3.760000   \n",
       "18          10.070000          28.390000           3.590000   \n",
       "19          10.110000          28.230000           3.430000   \n",
       "20          10.150000          28.080000           3.280000   \n",
       "21          10.180000          27.940000           3.150000   \n",
       "22          10.200000          27.820000           3.030000   \n",
       "23          10.230000          27.730000           2.910000   \n",
       "24          10.250000          27.640000           2.810000   \n",
       "25          10.270000          27.530000           2.730000   \n",
       "26          10.300000          27.430000           2.620000   \n",
       "27          10.310000          27.330000           2.570000   \n",
       "28          10.340000          27.220000           2.490000   \n",
       "29          10.350000          27.150000           2.450000   \n",
       "30          -5.421728          32.368019         -20.490168   \n",
       "\n",
       "    m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  \n",
       "0           -1.590000          -1.080000          28.350000  \n",
       "1           -1.950000          -1.220000          28.450000  \n",
       "2           -2.250000          -1.390000          28.470000  \n",
       "3           -2.550000          -1.550000          28.440000  \n",
       "4           -2.900000          -1.710000          28.450000  \n",
       "5           -3.260000          -1.870000          28.500000  \n",
       "6           -3.590000          -2.080000          28.500000  \n",
       "7           -3.910000          -2.330000          28.320000  \n",
       "8           -4.220000          -2.600000          27.990000  \n",
       "9           -4.470000          -2.940000          27.070000  \n",
       "10          -4.780000          -3.220000          26.340000  \n",
       "11          -5.040000          -3.480000          25.640000  \n",
       "12          -5.260000          -3.680000          25.050000  \n",
       "13          -5.220000          -3.750000          24.580000  \n",
       "14          -5.230000          -3.910000          24.100000  \n",
       "15          -5.030000          -4.060000          23.690000  \n",
       "16          -4.810000          -4.180000          23.170000  \n",
       "17          -4.670000          -4.340000          22.580000  \n",
       "18          -4.520000          -4.500000          22.040000  \n",
       "19          -4.450000          -4.620000          21.660000  \n",
       "20          -4.400000          -4.690000          21.350000  \n",
       "21          -4.330000          -4.750000          21.050000  \n",
       "22          -4.290000          -4.810000          20.760000  \n",
       "23          -4.300000          -4.870000          20.830000  \n",
       "24          -4.320000          -4.910000          20.900000  \n",
       "25          -4.350000          -4.970000          20.790000  \n",
       "26          -4.380000          -5.100000          20.490000  \n",
       "27          -4.400000          -5.260000          20.180000  \n",
       "28          -4.340000          -5.490000          19.920000  \n",
       "29          -4.240000          -5.600000          19.770000  \n",
       "30          -6.409177         -39.958096          16.002148  \n",
       "\n",
       "[31 rows x 126 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df.to_csv('./test_combined_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names = ['Pelvis', 'L_Hip', 'R_Hip', 'L_Knee', 'R_Knee', 'L_Ankle', 'R_Ankle', 'L_Foot', 'R_Foot', 'Spine1', 'Spine2', \n",
    "               'L_Collar', 'R_Collar', 'L_Shoulder', 'R_Shoulder', 'L_Elbow', 'R_Elbow', 'L_Wrist', 'R_Wrist', 'Neck', 'Head']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = [\n",
    "    ('Pelvis', 'L_Hip'), ('Pelvis', 'R_Hip'),\n",
    "    ('L_Hip', 'L_Knee'), ('R_Hip', 'R_Knee'),\n",
    "    ('L_Knee', 'L_Ankle'), ('R_Knee', 'R_Ankle'),\n",
    "    ('L_Ankle', 'L_Foot'), ('R_Ankle', 'R_Foot'),\n",
    "    ('Spine1', 'Spine2'), ('Spine2', 'Pelvis'),\n",
    "    ('Spine1', 'Neck'), ('Neck', 'Head'),\n",
    "    ('Spine1', 'L_Collar'), ('Spine1', 'R_Collar'),\n",
    "    ('L_Collar', 'L_Shoulder'), ('R_Collar', 'R_Shoulder'),\n",
    "    ('L_Shoulder', 'L_Elbow'), ('R_Shoulder', 'R_Elbow'),\n",
    "    ('L_Elbow', 'L_Wrist'), ('R_Elbow', 'R_Wrist')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_skeleton(df, connections):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot each joint\n",
    "    for joint in joint_names:\n",
    "        ax.scatter(df[f'm_avg_{joint}PosX'], df[f'm_avg_{joint}PosY'], df[f'm_avg_{joint}PosZ'], label=joint)\n",
    "\n",
    "    # Draw lines (bones) connecting the joints\n",
    "    for connection in connections:\n",
    "        joint_from, joint_to = connection\n",
    "        ax.plot(\n",
    "            [df[f'm_avg_{joint_from}PosX'], df[f'm_avg_{joint_to}PosX']],\n",
    "            [df[f'm_avg_{joint_from}PosY'], df[f'm_avg_{joint_to}PosY']],\n",
    "            [df[f'm_avg_{joint_from}PosZ'], df[f'm_avg_{joint_to}PosZ']],\n",
    "            'k-'\n",
    "        )\n",
    "\n",
    "    # Set labels and legend\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Set the initial viewing angle\n",
    "    ax.view_init(elev=20., azim=-90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the first frame\n",
    "visualize_skeleton(test_predictions.iloc[0], connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
