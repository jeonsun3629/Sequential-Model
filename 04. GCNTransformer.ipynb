{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './csvFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 목록을 랜덤하게 섞습니다.\n",
    "random.seed(42)  # 재현 가능한 결과를 위해 시드 설정\n",
    "random.shuffle(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 파일에서 컬럼 이름을 가져옵니다.\n",
    "first_file_path = os.path.join(directory, csv_files[0])\n",
    "first_df = pd.read_csv(first_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator_token = 999\n",
    "separator = pd.DataFrame({col: separator_token for col in first_df.columns}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.iloc[300:-100]\n",
    "    df_list.append(df)\n",
    "    df_list.append(separator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frame</th>\n",
       "      <th>Time</th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "      <th>Unnamed: 128</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>3.454812</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>352.2222</td>\n",
       "      <td>350.739100</td>\n",
       "      <td>0.733168</td>\n",
       "      <td>0.943708</td>\n",
       "      <td>0.631755</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>359.55840</td>\n",
       "      <td>0.846429</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>302</td>\n",
       "      <td>3.465498</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>352.2729</td>\n",
       "      <td>350.772300</td>\n",
       "      <td>0.733238</td>\n",
       "      <td>0.943977</td>\n",
       "      <td>0.631620</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>359.53570</td>\n",
       "      <td>0.884086</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>303</td>\n",
       "      <td>3.476308</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>352.2918</td>\n",
       "      <td>350.806000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944234</td>\n",
       "      <td>0.631682</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>359.52710</td>\n",
       "      <td>0.923961</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>304</td>\n",
       "      <td>3.487449</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>352.2754</td>\n",
       "      <td>350.814000</td>\n",
       "      <td>0.733297</td>\n",
       "      <td>0.944235</td>\n",
       "      <td>0.631826</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>359.53850</td>\n",
       "      <td>0.959352</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>305</td>\n",
       "      <td>3.498579</td>\n",
       "      <td>0.021056</td>\n",
       "      <td>0.537794</td>\n",
       "      <td>0.689871</td>\n",
       "      <td>358.82500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>359.8412</td>\n",
       "      <td>-0.037938</td>\n",
       "      <td>0.455332</td>\n",
       "      <td>...</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>352.2285</td>\n",
       "      <td>350.846900</td>\n",
       "      <td>0.733279</td>\n",
       "      <td>0.944438</td>\n",
       "      <td>0.631913</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>359.57180</td>\n",
       "      <td>0.958712</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250495</th>\n",
       "      <td>4600</td>\n",
       "      <td>51.260610</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>198.3722</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>-1.323386</td>\n",
       "      <td>1.040075</td>\n",
       "      <td>0.924556</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250496</th>\n",
       "      <td>4601</td>\n",
       "      <td>51.271360</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>198.4202</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>-1.323311</td>\n",
       "      <td>1.040272</td>\n",
       "      <td>0.924719</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250497</th>\n",
       "      <td>4602</td>\n",
       "      <td>51.283000</td>\n",
       "      <td>-0.674628</td>\n",
       "      <td>0.459754</td>\n",
       "      <td>0.821685</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>357.6249</td>\n",
       "      <td>-0.614788</td>\n",
       "      <td>0.391658</td>\n",
       "      <td>...</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>198.4699</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>-1.323236</td>\n",
       "      <td>1.040406</td>\n",
       "      <td>0.924903</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250498</th>\n",
       "      <td>4603</td>\n",
       "      <td>51.293670</td>\n",
       "      <td>-0.674747</td>\n",
       "      <td>0.459201</td>\n",
       "      <td>0.823021</td>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>357.4926</td>\n",
       "      <td>-0.614688</td>\n",
       "      <td>0.391303</td>\n",
       "      <td>...</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>198.5145</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>-1.322936</td>\n",
       "      <td>1.038698</td>\n",
       "      <td>0.926400</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250499</th>\n",
       "      <td>999</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250500 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Frame        Time  m_avg_PelvisPosX  m_avg_PelvisPosY  \\\n",
       "0          301    3.454812          0.021056          0.537794   \n",
       "1          302    3.465498          0.021056          0.537794   \n",
       "2          303    3.476308          0.021056          0.537794   \n",
       "3          304    3.487449          0.021056          0.537794   \n",
       "4          305    3.498579          0.021056          0.537794   \n",
       "...        ...         ...               ...               ...   \n",
       "1250495   4600   51.260610         -0.674628          0.459754   \n",
       "1250496   4601   51.271360         -0.674628          0.459754   \n",
       "1250497   4602   51.283000         -0.674628          0.459754   \n",
       "1250498   4603   51.293670         -0.674747          0.459201   \n",
       "1250499    999  999.000000        999.000000        999.000000   \n",
       "\n",
       "         m_avg_PelvisPosZ  m_avg_PelvisRotX  m_avg_PelvisRotY  \\\n",
       "0                0.689871         358.82500          0.679502   \n",
       "1                0.689871         358.82500          0.679502   \n",
       "2                0.689871         358.82500          0.679502   \n",
       "3                0.689871         358.82500          0.679492   \n",
       "4                0.689871         358.82500          0.679492   \n",
       "...                   ...               ...               ...   \n",
       "1250495          0.821685          18.35563        179.608500   \n",
       "1250496          0.821685          18.35563        179.608500   \n",
       "1250497          0.821685          18.35563        179.608500   \n",
       "1250498          0.823021          18.44855        179.665600   \n",
       "1250499        999.000000         999.00000        999.000000   \n",
       "\n",
       "         m_avg_PelvisRotZ  m_avg_L_HipPosX  m_avg_L_HipPosY  ...  \\\n",
       "0                359.8412        -0.037938         0.455332  ...   \n",
       "1                359.8412        -0.037938         0.455332  ...   \n",
       "2                359.8412        -0.037938         0.455332  ...   \n",
       "3                359.8412        -0.037938         0.455332  ...   \n",
       "4                359.8412        -0.037938         0.455332  ...   \n",
       "...                   ...              ...              ...  ...   \n",
       "1250495          357.6249        -0.614788         0.391658  ...   \n",
       "1250496          357.6249        -0.614788         0.391658  ...   \n",
       "1250497          357.6249        -0.614788         0.391658  ...   \n",
       "1250498          357.4926        -0.614688         0.391303  ...   \n",
       "1250499          999.0000       999.000000       999.000000  ...   \n",
       "\n",
       "         m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  m_avg_R_ElbowRotZ  \\\n",
       "0                 8.262754           352.2222         350.739100   \n",
       "1                 8.274371           352.2729         350.772300   \n",
       "2                 8.273871           352.2918         350.806000   \n",
       "3                 8.267890           352.2754         350.814000   \n",
       "4                 8.257694           352.2285         350.846900   \n",
       "...                    ...                ...                ...   \n",
       "1250495          11.131550           198.3722           3.791916   \n",
       "1250496          11.133720           198.4202           3.836214   \n",
       "1250497          11.135700           198.4699           3.866319   \n",
       "1250498          11.140010           198.5145           3.896140   \n",
       "1250499         999.000000           999.0000         999.000000   \n",
       "\n",
       "         m_avg_R_WristPosX  m_avg_R_WristPosY  m_avg_R_WristPosZ  \\\n",
       "0                 0.733168           0.943708           0.631755   \n",
       "1                 0.733238           0.943977           0.631620   \n",
       "2                 0.733297           0.944234           0.631682   \n",
       "3                 0.733297           0.944235           0.631826   \n",
       "4                 0.733279           0.944438           0.631913   \n",
       "...                    ...                ...                ...   \n",
       "1250495          -1.323386           1.040075           0.924556   \n",
       "1250496          -1.323311           1.040272           0.924719   \n",
       "1250497          -1.323236           1.040406           0.924903   \n",
       "1250498          -1.322936           1.038698           0.926400   \n",
       "1250499         999.000000         999.000000         999.000000   \n",
       "\n",
       "         m_avg_R_WristRotX  m_avg_R_WristRotY  m_avg_R_WristRotZ  Unnamed: 128  \n",
       "0                 0.200809          359.55840           0.846429           NaN  \n",
       "1                 0.114945          359.53570           0.884086           NaN  \n",
       "2                 0.151095          359.52710           0.923961           NaN  \n",
       "3                 0.239347          359.53850           0.959352           NaN  \n",
       "4                 0.385812          359.57180           0.958712           NaN  \n",
       "...                    ...                ...                ...           ...  \n",
       "1250495          47.211770           37.32508         178.617100           NaN  \n",
       "1250496          47.288450           37.23001         178.460700           NaN  \n",
       "1250497          47.363370           37.13769         178.286900           NaN  \n",
       "1250498          47.438840           37.06465         178.126200           NaN  \n",
       "1250499         999.000000          999.00000         999.000000         999.0  \n",
       "\n",
       "[1250500 rows x 129 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250500"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\239680038.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[df[col] != 999, col] = (df[col] - min_val) / (max_val - min_val)\n"
     ]
    }
   ],
   "source": [
    "# Rotation 컬럼만 선택\n",
    "rotation_columns = [col for col in combined_df.columns if 'Rot' in col]\n",
    "\n",
    "# Position 컬럼만 선택\n",
    "position_columns = [col for col in combined_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "rotation_df = combined_df[rotation_columns]\n",
    "position_df = combined_df[position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "rotation_df = rotation_df.apply(lambda col: col.apply(normalize_angle))\n",
    "\n",
    "# Position 컬럼만 0~1 사이로 정규화\n",
    "def normalize_columns(df):\n",
    "    for col in df.columns:\n",
    "        if 'Pos' in col:  # 위치에 대한 컬럼만 정규화\n",
    "            # 999 값을 제외하고 min 및 max를 계산\n",
    "            filtered_df = df[df[col] != 999]\n",
    "            min_val = filtered_df[col].min()\n",
    "            max_val = filtered_df[col].max()\n",
    "\n",
    "            # 999가 아닌 값만 정규화\n",
    "            df.loc[df[col] != 999, col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df\n",
    "\n",
    "position_df = normalize_columns(position_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "범위를 벗어나는 값이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = rotation_df.apply(lambda col: col.apply(lambda x: x != 999 and (x > 180 or x < -180))).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:10: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:13: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_32236\\3915166593.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']\n"
     ]
    }
   ],
   "source": [
    "# 조인트 이름 추출\n",
    "joint_names = [col.split('PosX')[0] for col in position_df.columns if 'PosX' in col]\n",
    "\n",
    "# 빈 데이터 프레임 생성\n",
    "posrot_df = pd.DataFrame()\n",
    "\n",
    "# 각 조인트에 대해 위치 데이터와 회전 데이터를 순차적으로 배열\n",
    "for joint in joint_names:\n",
    "    posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
    "    posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
    "    posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
    "    posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
    "    posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
    "    posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowPosZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425514</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>-7.7778</td>\n",
       "      <td>-9.260900</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>0.437026</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>-0.44160</td>\n",
       "      <td>0.846429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425524</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>-7.7271</td>\n",
       "      <td>-9.227700</td>\n",
       "      <td>0.754483</td>\n",
       "      <td>0.705789</td>\n",
       "      <td>0.437009</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>-0.46430</td>\n",
       "      <td>0.884086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425540</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>-7.7082</td>\n",
       "      <td>-9.194000</td>\n",
       "      <td>0.754499</td>\n",
       "      <td>0.705913</td>\n",
       "      <td>0.437017</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>-0.47290</td>\n",
       "      <td>0.923961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425547</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>-7.7246</td>\n",
       "      <td>-9.186000</td>\n",
       "      <td>0.754499</td>\n",
       "      <td>0.705914</td>\n",
       "      <td>0.437034</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>-0.46150</td>\n",
       "      <td>0.959352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425527</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>-7.7715</td>\n",
       "      <td>-9.153100</td>\n",
       "      <td>0.754494</td>\n",
       "      <td>0.706012</td>\n",
       "      <td>0.437045</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>-0.42820</td>\n",
       "      <td>0.958712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250495</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>-161.6278</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.752044</td>\n",
       "      <td>0.473122</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250496</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>-161.5798</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>0.200541</td>\n",
       "      <td>0.752139</td>\n",
       "      <td>0.473142</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250497</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>-161.5301</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>0.200561</td>\n",
       "      <td>0.752204</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250498</th>\n",
       "      <td>0.372510</td>\n",
       "      <td>0.563243</td>\n",
       "      <td>0.446288</td>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>-2.5074</td>\n",
       "      <td>0.383066</td>\n",
       "      <td>0.563565</td>\n",
       "      <td>0.453621</td>\n",
       "      <td>-0.7340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455199</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>-161.4855</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>0.200642</td>\n",
       "      <td>0.751381</td>\n",
       "      <td>0.473349</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250499</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250500 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  \\\n",
       "0                0.586219          0.625759          0.428094   \n",
       "1                0.586219          0.625759          0.428094   \n",
       "2                0.586219          0.625759          0.428094   \n",
       "3                0.586219          0.625759          0.428094   \n",
       "4                0.586219          0.625759          0.428094   \n",
       "...                   ...               ...               ...   \n",
       "1250495          0.372546          0.563683          0.446105   \n",
       "1250496          0.372546          0.563683          0.446105   \n",
       "1250497          0.372546          0.563683          0.446105   \n",
       "1250498          0.372510          0.563243          0.446288   \n",
       "1250499        999.000000        999.000000        999.000000   \n",
       "\n",
       "         m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  \\\n",
       "0                -1.17500          0.679502           -0.1588   \n",
       "1                -1.17500          0.679502           -0.1588   \n",
       "2                -1.17500          0.679502           -0.1588   \n",
       "3                -1.17500          0.679492           -0.1588   \n",
       "4                -1.17500          0.679492           -0.1588   \n",
       "...                   ...               ...               ...   \n",
       "1250495          18.35563        179.608500           -2.3751   \n",
       "1250496          18.35563        179.608500           -2.3751   \n",
       "1250497          18.35563        179.608500           -2.3751   \n",
       "1250498          18.44855        179.665600           -2.5074   \n",
       "1250499         999.00000        999.000000          999.0000   \n",
       "\n",
       "         m_avg_L_HipPosX  m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_HipRotX  \\\n",
       "0               0.563147         0.614554         0.427315          -0.2778   \n",
       "1               0.563147         0.614554         0.427315          -0.2778   \n",
       "2               0.563147         0.614554         0.427315          -0.2778   \n",
       "3               0.563147         0.614554         0.427315          -0.2778   \n",
       "4               0.563147         0.614554         0.427315          -0.2778   \n",
       "...                  ...              ...              ...              ...   \n",
       "1250495         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250496         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250497         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250498         0.383066         0.563565         0.453621          -0.7340   \n",
       "1250499       999.000000       999.000000       999.000000         999.0000   \n",
       "\n",
       "         ...  m_avg_R_ElbowPosZ  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "0        ...           0.425514           8.262754            -7.7778   \n",
       "1        ...           0.425524           8.274371            -7.7271   \n",
       "2        ...           0.425540           8.273871            -7.7082   \n",
       "3        ...           0.425547           8.267890            -7.7246   \n",
       "4        ...           0.425527           8.257694            -7.7715   \n",
       "...      ...                ...                ...                ...   \n",
       "1250495  ...           0.455031          11.131550          -161.6278   \n",
       "1250496  ...           0.455031          11.133720          -161.5798   \n",
       "1250497  ...           0.455031          11.135700          -161.5301   \n",
       "1250498  ...           0.455199          11.140010          -161.4855   \n",
       "1250499  ...         999.000000         999.000000           999.0000   \n",
       "\n",
       "         m_avg_R_ElbowRotZ  m_avg_R_WristPosX  m_avg_R_WristPosY  \\\n",
       "0                -9.260900           0.754464           0.705660   \n",
       "1                -9.227700           0.754483           0.705789   \n",
       "2                -9.194000           0.754499           0.705913   \n",
       "3                -9.186000           0.754499           0.705914   \n",
       "4                -9.153100           0.754494           0.706012   \n",
       "...                    ...                ...                ...   \n",
       "1250495           3.791916           0.200521           0.752044   \n",
       "1250496           3.836214           0.200541           0.752139   \n",
       "1250497           3.866319           0.200561           0.752204   \n",
       "1250498           3.896140           0.200642           0.751381   \n",
       "1250499         999.000000         999.000000         999.000000   \n",
       "\n",
       "         m_avg_R_WristPosZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "0                 0.437026           0.200809           -0.44160   \n",
       "1                 0.437009           0.114945           -0.46430   \n",
       "2                 0.437017           0.151095           -0.47290   \n",
       "3                 0.437034           0.239347           -0.46150   \n",
       "4                 0.437045           0.385812           -0.42820   \n",
       "...                    ...                ...                ...   \n",
       "1250495           0.473122          47.211770           37.32508   \n",
       "1250496           0.473142          47.288450           37.23001   \n",
       "1250497           0.473165          47.363370           37.13769   \n",
       "1250498           0.473349          47.438840           37.06465   \n",
       "1250499         999.000000         999.000000          999.00000   \n",
       "\n",
       "         m_avg_R_WristRotZ  \n",
       "0                 0.846429  \n",
       "1                 0.884086  \n",
       "2                 0.923961  \n",
       "3                 0.959352  \n",
       "4                 0.958712  \n",
       "...                    ...  \n",
       "1250495         178.617100  \n",
       "1250496         178.460700  \n",
       "1250497         178.286900  \n",
       "1250498         178.126200  \n",
       "1250499         999.000000  \n",
       "\n",
       "[1250500 rows x 126 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Position 컬럼은 그대로 두고, 다시 합치기\n",
    "posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posrot_df.to_csv('./posrot_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 데이터를 DataFrame으로 변환\n",
    "posrot_df = pd.DataFrame(posrot_df, columns=posrot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posrot_df = posrot_df.map(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_avg_PelvisPosX</th>\n",
       "      <th>m_avg_PelvisPosY</th>\n",
       "      <th>m_avg_PelvisPosZ</th>\n",
       "      <th>m_avg_PelvisRotX</th>\n",
       "      <th>m_avg_PelvisRotY</th>\n",
       "      <th>m_avg_PelvisRotZ</th>\n",
       "      <th>m_avg_L_HipPosX</th>\n",
       "      <th>m_avg_L_HipPosY</th>\n",
       "      <th>m_avg_L_HipPosZ</th>\n",
       "      <th>m_avg_L_HipRotX</th>\n",
       "      <th>...</th>\n",
       "      <th>m_avg_R_ElbowPosZ</th>\n",
       "      <th>m_avg_R_ElbowRotX</th>\n",
       "      <th>m_avg_R_ElbowRotY</th>\n",
       "      <th>m_avg_R_ElbowRotZ</th>\n",
       "      <th>m_avg_R_WristPosX</th>\n",
       "      <th>m_avg_R_WristPosY</th>\n",
       "      <th>m_avg_R_WristPosZ</th>\n",
       "      <th>m_avg_R_WristRotX</th>\n",
       "      <th>m_avg_R_WristRotY</th>\n",
       "      <th>m_avg_R_WristRotZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425514</td>\n",
       "      <td>8.262754</td>\n",
       "      <td>-7.7778</td>\n",
       "      <td>-9.260900</td>\n",
       "      <td>0.754464</td>\n",
       "      <td>0.705660</td>\n",
       "      <td>0.437026</td>\n",
       "      <td>0.200809</td>\n",
       "      <td>-0.44160</td>\n",
       "      <td>0.846429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425524</td>\n",
       "      <td>8.274371</td>\n",
       "      <td>-7.7271</td>\n",
       "      <td>-9.227700</td>\n",
       "      <td>0.754483</td>\n",
       "      <td>0.705789</td>\n",
       "      <td>0.437009</td>\n",
       "      <td>0.114945</td>\n",
       "      <td>-0.46430</td>\n",
       "      <td>0.884086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679502</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425540</td>\n",
       "      <td>8.273871</td>\n",
       "      <td>-7.7082</td>\n",
       "      <td>-9.194000</td>\n",
       "      <td>0.754499</td>\n",
       "      <td>0.705913</td>\n",
       "      <td>0.437017</td>\n",
       "      <td>0.151095</td>\n",
       "      <td>-0.47290</td>\n",
       "      <td>0.923961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425547</td>\n",
       "      <td>8.267890</td>\n",
       "      <td>-7.7246</td>\n",
       "      <td>-9.186000</td>\n",
       "      <td>0.754499</td>\n",
       "      <td>0.705914</td>\n",
       "      <td>0.437034</td>\n",
       "      <td>0.239347</td>\n",
       "      <td>-0.46150</td>\n",
       "      <td>0.959352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.586219</td>\n",
       "      <td>0.625759</td>\n",
       "      <td>0.428094</td>\n",
       "      <td>-1.17500</td>\n",
       "      <td>0.679492</td>\n",
       "      <td>-0.1588</td>\n",
       "      <td>0.563147</td>\n",
       "      <td>0.614554</td>\n",
       "      <td>0.427315</td>\n",
       "      <td>-0.2778</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425527</td>\n",
       "      <td>8.257694</td>\n",
       "      <td>-7.7715</td>\n",
       "      <td>-9.153100</td>\n",
       "      <td>0.754494</td>\n",
       "      <td>0.706012</td>\n",
       "      <td>0.437045</td>\n",
       "      <td>0.385812</td>\n",
       "      <td>-0.42820</td>\n",
       "      <td>0.958712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250495</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.131550</td>\n",
       "      <td>-161.6278</td>\n",
       "      <td>3.791916</td>\n",
       "      <td>0.200521</td>\n",
       "      <td>0.752044</td>\n",
       "      <td>0.473122</td>\n",
       "      <td>47.211770</td>\n",
       "      <td>37.32508</td>\n",
       "      <td>178.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250496</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.133720</td>\n",
       "      <td>-161.5798</td>\n",
       "      <td>3.836214</td>\n",
       "      <td>0.200541</td>\n",
       "      <td>0.752139</td>\n",
       "      <td>0.473142</td>\n",
       "      <td>47.288450</td>\n",
       "      <td>37.23001</td>\n",
       "      <td>178.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250497</th>\n",
       "      <td>0.372546</td>\n",
       "      <td>0.563683</td>\n",
       "      <td>0.446105</td>\n",
       "      <td>18.35563</td>\n",
       "      <td>179.608500</td>\n",
       "      <td>-2.3751</td>\n",
       "      <td>0.383035</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.453434</td>\n",
       "      <td>-0.4831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455031</td>\n",
       "      <td>11.135700</td>\n",
       "      <td>-161.5301</td>\n",
       "      <td>3.866319</td>\n",
       "      <td>0.200561</td>\n",
       "      <td>0.752204</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>47.363370</td>\n",
       "      <td>37.13769</td>\n",
       "      <td>178.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250498</th>\n",
       "      <td>0.372510</td>\n",
       "      <td>0.563243</td>\n",
       "      <td>0.446288</td>\n",
       "      <td>18.44855</td>\n",
       "      <td>179.665600</td>\n",
       "      <td>-2.5074</td>\n",
       "      <td>0.383066</td>\n",
       "      <td>0.563565</td>\n",
       "      <td>0.453621</td>\n",
       "      <td>-0.7340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455199</td>\n",
       "      <td>11.140010</td>\n",
       "      <td>-161.4855</td>\n",
       "      <td>3.896140</td>\n",
       "      <td>0.200642</td>\n",
       "      <td>0.751381</td>\n",
       "      <td>0.473349</td>\n",
       "      <td>47.438840</td>\n",
       "      <td>37.06465</td>\n",
       "      <td>178.126200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250499</th>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.0000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.000000</td>\n",
       "      <td>999.00000</td>\n",
       "      <td>999.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1250500 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         m_avg_PelvisPosX  m_avg_PelvisPosY  m_avg_PelvisPosZ  \\\n",
       "0                0.586219          0.625759          0.428094   \n",
       "1                0.586219          0.625759          0.428094   \n",
       "2                0.586219          0.625759          0.428094   \n",
       "3                0.586219          0.625759          0.428094   \n",
       "4                0.586219          0.625759          0.428094   \n",
       "...                   ...               ...               ...   \n",
       "1250495          0.372546          0.563683          0.446105   \n",
       "1250496          0.372546          0.563683          0.446105   \n",
       "1250497          0.372546          0.563683          0.446105   \n",
       "1250498          0.372510          0.563243          0.446288   \n",
       "1250499        999.000000        999.000000        999.000000   \n",
       "\n",
       "         m_avg_PelvisRotX  m_avg_PelvisRotY  m_avg_PelvisRotZ  \\\n",
       "0                -1.17500          0.679502           -0.1588   \n",
       "1                -1.17500          0.679502           -0.1588   \n",
       "2                -1.17500          0.679502           -0.1588   \n",
       "3                -1.17500          0.679492           -0.1588   \n",
       "4                -1.17500          0.679492           -0.1588   \n",
       "...                   ...               ...               ...   \n",
       "1250495          18.35563        179.608500           -2.3751   \n",
       "1250496          18.35563        179.608500           -2.3751   \n",
       "1250497          18.35563        179.608500           -2.3751   \n",
       "1250498          18.44855        179.665600           -2.5074   \n",
       "1250499         999.00000        999.000000          999.0000   \n",
       "\n",
       "         m_avg_L_HipPosX  m_avg_L_HipPosY  m_avg_L_HipPosZ  m_avg_L_HipRotX  \\\n",
       "0               0.563147         0.614554         0.427315          -0.2778   \n",
       "1               0.563147         0.614554         0.427315          -0.2778   \n",
       "2               0.563147         0.614554         0.427315          -0.2778   \n",
       "3               0.563147         0.614554         0.427315          -0.2778   \n",
       "4               0.563147         0.614554         0.427315          -0.2778   \n",
       "...                  ...              ...              ...              ...   \n",
       "1250495         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250496         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250497         0.383035         0.563848         0.453434          -0.4831   \n",
       "1250498         0.383066         0.563565         0.453621          -0.7340   \n",
       "1250499       999.000000       999.000000       999.000000         999.0000   \n",
       "\n",
       "         ...  m_avg_R_ElbowPosZ  m_avg_R_ElbowRotX  m_avg_R_ElbowRotY  \\\n",
       "0        ...           0.425514           8.262754            -7.7778   \n",
       "1        ...           0.425524           8.274371            -7.7271   \n",
       "2        ...           0.425540           8.273871            -7.7082   \n",
       "3        ...           0.425547           8.267890            -7.7246   \n",
       "4        ...           0.425527           8.257694            -7.7715   \n",
       "...      ...                ...                ...                ...   \n",
       "1250495  ...           0.455031          11.131550          -161.6278   \n",
       "1250496  ...           0.455031          11.133720          -161.5798   \n",
       "1250497  ...           0.455031          11.135700          -161.5301   \n",
       "1250498  ...           0.455199          11.140010          -161.4855   \n",
       "1250499  ...         999.000000         999.000000           999.0000   \n",
       "\n",
       "         m_avg_R_ElbowRotZ  m_avg_R_WristPosX  m_avg_R_WristPosY  \\\n",
       "0                -9.260900           0.754464           0.705660   \n",
       "1                -9.227700           0.754483           0.705789   \n",
       "2                -9.194000           0.754499           0.705913   \n",
       "3                -9.186000           0.754499           0.705914   \n",
       "4                -9.153100           0.754494           0.706012   \n",
       "...                    ...                ...                ...   \n",
       "1250495           3.791916           0.200521           0.752044   \n",
       "1250496           3.836214           0.200541           0.752139   \n",
       "1250497           3.866319           0.200561           0.752204   \n",
       "1250498           3.896140           0.200642           0.751381   \n",
       "1250499         999.000000         999.000000         999.000000   \n",
       "\n",
       "         m_avg_R_WristPosZ  m_avg_R_WristRotX  m_avg_R_WristRotY  \\\n",
       "0                 0.437026           0.200809           -0.44160   \n",
       "1                 0.437009           0.114945           -0.46430   \n",
       "2                 0.437017           0.151095           -0.47290   \n",
       "3                 0.437034           0.239347           -0.46150   \n",
       "4                 0.437045           0.385812           -0.42820   \n",
       "...                    ...                ...                ...   \n",
       "1250495           0.473122          47.211770           37.32508   \n",
       "1250496           0.473142          47.288450           37.23001   \n",
       "1250497           0.473165          47.363370           37.13769   \n",
       "1250498           0.473349          47.438840           37.06465   \n",
       "1250499         999.000000         999.000000          999.00000   \n",
       "\n",
       "         m_avg_R_WristRotZ  \n",
       "0                 0.846429  \n",
       "1                 0.884086  \n",
       "2                 0.923961  \n",
       "3                 0.959352  \n",
       "4                 0.958712  \n",
       "...                    ...  \n",
       "1250495         178.617100  \n",
       "1250496         178.460700  \n",
       "1250497         178.286900  \n",
       "1250498         178.126200  \n",
       "1250499         999.000000  \n",
       "\n",
       "[1250500 rows x 126 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = posrot_df.iloc[:1000400]\n",
    "test = posrot_df.iloc[1000400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30 # 시퀀스 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.sequence_length] # : 입력 시퀀스 (30개의 데이터)\n",
    "        y = self.data[index+self.sequence_length-1] # : 예측 시퀀스 (1개의 데이터)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "# 데이터셋과 데이터 로더를 생성\n",
    "train_dataset = TimeseriesDataset(X_train.values, n_input)\n",
    "val_dataset = TimeseriesDataset(X_val.values, n_input)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X batch shape: torch.Size([128, 30, 126])\n",
      "y batch shape: torch.Size([128, 126])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 로더에서 첫 번째 배치를 가져와서 형태를 확인\n",
    "first_batch = next(iter(train_loader))\n",
    "X, y = first_batch\n",
    "\n",
    "print(\"X batch shape:\", X.shape)\n",
    "print(\"y batch shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN skeleton joint learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names = ['Pelvis', 'L_Hip', 'L_Knee', 'L_Ankle', 'L_Foot', \n",
    "                'R_Hip', 'R_Knee', 'R_Ankle', 'R_Foot', 'Spine1', 'Spine2',\n",
    "               'L_Collar', 'L_Shoulder', 'L_Elbow', 'L_Wrist', 'Neck', 'Head',\n",
    "               'R_Collar', 'R_Shoulder', 'R_Elbow', 'R_Wrist']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = [\n",
    "    ('Pelvis', 'L_Hip'), ('Pelvis', 'R_Hip'),\n",
    "    ('L_Hip', 'L_Knee'), ('R_Hip', 'R_Knee'),\n",
    "    ('L_Knee', 'L_Ankle'), ('R_Knee', 'R_Ankle'),\n",
    "    ('L_Ankle', 'L_Foot'), ('R_Ankle', 'R_Foot'),\n",
    "    ('Spine1', 'Spine2'), ('Spine2', 'Pelvis'),\n",
    "    ('Spine1', 'Neck'), ('Neck', 'Head'),\n",
    "    ('Spine1', 'L_Collar'), ('Spine1', 'R_Collar'),\n",
    "    ('L_Collar', 'L_Shoulder'), ('R_Collar', 'R_Shoulder'),\n",
    "    ('L_Shoulder', 'L_Elbow'), ('R_Shoulder', 'R_Elbow'),\n",
    "    ('L_Elbow', 'L_Wrist'), ('R_Elbow', 'R_Wrist')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조인트 이름을 인덱스로 매핑\n",
    "joint_to_index = {joint: idx for idx, joint in enumerate(joint_names)}\n",
    "\n",
    "# 연결 정보를 인덱스 쌍으로 변환\n",
    "edge_pairs = [(joint_to_index[joint_from], joint_to_index[joint_to]) for joint_from, joint_to in connections]\n",
    "\n",
    "# edge_index 생성\n",
    "edge_index = torch.tensor(edge_pairs, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  1,  5,  2,  6,  3,  7,  9, 10,  9, 15,  9,  9, 11, 17, 12, 18,\n",
       "         13, 19],\n",
       "        [ 1,  5,  2,  6,  3,  7,  4,  8, 10,  0, 15, 16, 11, 17, 12, 18, 13, 19,\n",
       "         14, 20]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs: 1\n",
      "Current device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "batch_size = 128\n",
    "num_nodes = 21  # 노드 수 (신체 관절 수)\n",
    "num_node_features = 6  # 각 노드의 특성 수 (위치 및 회전 정보)\n",
    "num_classes = 126  # 출력 클래스 수 (전체 특성 수)\n",
    "sequence_length = 30  # 시퀀스 길이\n",
    "in_channels = 30\n",
    "out_channels = 256\n",
    "\n",
    "lr = 0.0001\n",
    "patience = 10\n",
    "min_delta = 0.001\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TemporalLayer(nn.Module):\n",
    "#     \"\"\" 시간적 특성을 처리하기 위한 레이어 \"\"\"\n",
    "#     def __init__(self, in_channels, out_channels):\n",
    "#         super(TemporalLayer, self).__init__()\n",
    "#         self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "#         self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         print(\"Input shape to TemporalLayer:\", x.shape)  # TemporalLayer 입력 형태 확인\n",
    "#         x = x.permute(0, 2, 1)  # [batch_size, in_channels, sequence_length]\n",
    "#         x = self.conv1(x)\n",
    "#         x = F.relu(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = F.relu(x)\n",
    "#         return x\n",
    "\n",
    "# class GCNWithTemporal(nn.Module):\n",
    "#     \"\"\" 공간적 및 시간적 특성을 모두 고려하는 GCN 모델 \"\"\"\n",
    "#     def __init__(self, num_node_features, num_classes, sequence_length):\n",
    "#         super(GCNWithTemporal, self).__init__()\n",
    "#         self.conv1 = GCNConv(num_node_features, 64)\n",
    "#         self.conv2 = GCNConv(64, 128)\n",
    "#         self.conv3 = GCNConv(128, 256)\n",
    "#         self.temporal_layer = TemporalLayer(sequence_length, 256)\n",
    "#         self.out = nn.Linear(num_nodes * 256 * sequence_length, num_classes)\n",
    "\n",
    "#     def forward(self, x, edge_index):\n",
    "#         # 입력 데이터의 형태 확인\n",
    "#         print(\"Original input shape to GCNWithTemporal:\", x.shape)\n",
    "        \n",
    "#         # x의 형태를 [batch_size, sequence_length, num_nodes * num_node_features]로 변환\n",
    "#         batch_size, sequence_length, num_nodes, num_node_features = x.shape\n",
    "#         x = x.reshape(batch_size * sequence_length, num_nodes, num_node_features)\n",
    "\n",
    "#         # 입력 데이터의 총 요소 수 확인\n",
    "#         input_elements = batch_size * sequence_length * num_nodes * num_node_features\n",
    "#         print(\"Total elements in input:\", input_elements)\n",
    "\n",
    "#         # 새로운 형태로 변환 전에 요소 수 일치 확인\n",
    "#         assert input_elements == x.numel(), \"Mismatch in total elements when reshaping\"\n",
    "\n",
    "#         # 새로운 형태로 변환\n",
    "#         x = x.reshape(batch_size * sequence_length, num_nodes * num_node_features)        \n",
    "        \n",
    "#         # GCN 적용\n",
    "#         x = F.relu(self.conv1(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = F.relu(self.conv2(x, edge_index))\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = F.relu(self.conv3(x, edge_index))\n",
    "        \n",
    "#         # 원래 형태로 복원\n",
    "#         x = x.view(batch_size, sequence_length, -1)\n",
    "        \n",
    "#         # 시간 차원을 처리합니다.\n",
    "#         x = self.temporal_layer(x)\n",
    "#         print(\"Shape after TemporalLayer in GCNWithTemporal:\", x.shape)  # TemporalLayer 처리 후 형태 확인\n",
    "\n",
    "#         # 최종 출력 차원을 확인\n",
    "#         x = x.reshape(x.size(0), -1)  # [batch_size, sequence_length * num_nodes * out_channels]\n",
    "#         print(\"Shape before final linear layer in GCNWithTemporal:\", x.shape)  # 최종 선형 레이어 전 형태 확인\n",
    "\n",
    "#         x = self.out(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLayer(nn.Module):\n",
    "    \"\"\" 시간적 특성을 처리하기 위한 레이어 \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TemporalLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        print(\"TemporalLayer Initialized:\")\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, num_nodes * num_node_features, sequence_length]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class GCNWithTemporal(nn.Module):\n",
    "    \"\"\" 공간적 및 시간적 특성을 모두 고려하는 GCN 모델 \"\"\"\n",
    "    def __init__(self, num_node_features, num_classes, sequence_length, num_nodes):\n",
    "        super(GCNWithTemporal, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.temporal_layer = TemporalLayer(num_nodes * 256, 256)\n",
    "        self.out = nn.Linear(7680, num_classes) # [batch_size, 256 * 30] = [batch_size, 7680]\n",
    "        print(\"GCNWithTemporal Initialized:\")\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 입력 데이터의 형태 확인 (4차원으로 처리)\n",
    "        # print(\"Original input shape to GCNWithTemporal:\", x.shape) [128, 30, 21, 6])\n",
    "        \n",
    "        # x의 형태를 [batch_size, sequence_length, num_nodes, num_node_features]에서\n",
    "        # [batch_size * sequence_length, num_nodes, num_node_features]로 변환\n",
    "        batch_size, sequence_length, num_nodes, num_node_features = x.shape\n",
    "        x = x.reshape(batch_size * sequence_length, num_nodes, num_node_features)\n",
    "\n",
    "        # GCN 적용\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        # print(\"Shape after GCN layers in GCNWithTemporal:\", x.shape) [3840, 21, 256])\n",
    "\n",
    "        # 원래 형태로 복원 (배치 사이즈와 시퀀스 길이를 유지하며 차원 재구성)\n",
    "        x = x.view(batch_size, sequence_length, -1)\n",
    "        # print(\"Shape before TemporalLayer in GCNWithTemporal:\", x.shape) [128, 30, 5376])\n",
    "       \n",
    "        # 시간 차원을 처리합니다.\n",
    "        x = self.temporal_layer(x)\n",
    "        # print(\"Shape after TemporalLayer in GCNWithTemporal:\", x.shape) [128, 256, 30])\n",
    "\n",
    "        # 최종 출력 차원을 확인\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # print(\"Shape before final linear layer in GCNWithTemporal:\", x.shape) [128, 7680])\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TemporalLayer Initialized:\n",
      "TemporalLayer(\n",
      "  (conv1): Conv1d(5376, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      ")\n",
      "GCNWithTemporal Initialized:\n",
      "GCNWithTemporal(\n",
      "  (conv1): GCNConv(6, 64)\n",
      "  (conv2): GCNConv(64, 128)\n",
      "  (conv3): GCNConv(128, 256)\n",
      "  (temporal_layer): TemporalLayer(\n",
      "    (conv1): Conv1d(5376, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  )\n",
      "  (out): Linear(in_features=7680, out_features=126, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델, 손실 함수, 최적화 알고리즘 초기화\n",
    "gcn_model = GCNWithTemporal(num_node_features, num_classes, sequence_length, num_nodes)\n",
    "optimizer = optim.Adam(gcn_model.parameters(), lr=0.0001)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCNWithTemporal(\n",
       "  (conv1): GCNConv(6, 64)\n",
       "  (conv2): GCNConv(64, 128)\n",
       "  (conv3): GCNConv(128, 256)\n",
       "  (temporal_layer): TemporalLayer(\n",
       "    (conv1): Conv1d(5376, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (conv2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  )\n",
       "  (out): Linear(in_features=7680, out_features=126, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 현재 장치로 이동합니다.\n",
    "gcn_model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실을 저장할 리스트 초기화\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 가장 좋은 손실 값 초기화\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 함수\n",
    "def train_loop(gcn_model, dataloader, optimizer, loss_function, device, edge_index, num_nodes, num_node_features, sequence_length):\n",
    "    gcn_model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for data, target in progress_bar:\n",
    "        # 데이터 형태와 edge_index 검증을 위한 출력\n",
    "        # print(\"Original data shape:\", data.shape)\n",
    "        # print(\"Edge index shape:\", edge_index.shape)\n",
    "        # print(\"Edge index:\", edge_index)\n",
    "\n",
    "        # 데이터 차원 변경\n",
    "        edge_index = edge_index.to(device)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, num_nodes, num_node_features, sequence_length)  # 여기에 추가\n",
    "        # print(\"View applied data shape:\", data.shape)\n",
    "        data = data.permute(0, 3, 1, 2)  # 여기에 추가\n",
    "        # print(\"Permute applied data shape:\", data.shape)\n",
    "        target = target.view(-1, num_classes)\n",
    "\n",
    "        # 데이터 형태 출력\n",
    "        # print(\"Transformed data shape:\", data.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = gcn_model(data, edge_index)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# 검증 루프 함수\n",
    "def val_loop(gcn_model, dataloader, loss_function, device, edge_index, num_nodes, num_node_features, sequence_length):\n",
    "    gcn_model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            edge_index = edge_index.to(device)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, num_nodes, num_node_features, sequence_length)  # 여기에 추가\n",
    "            data = data.permute(0, 3, 1, 2)  # 여기에 추가\n",
    "            target = target.view(-1, num_classes)\n",
    "\n",
    "            output = gcn_model(data, edge_index)\n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 711.9244928677884, Validation Loss: 788.8517367573222\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 358.7118828827024, Validation Loss: 560.3730074691549\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 284.41586507318823, Validation Loss: 493.36240486550685\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 247.9497602418751, Validation Loss: 488.89757155656997\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 223.95273410737425, Validation Loss: 471.78337392617226\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 207.6366826467614, Validation Loss: 441.4532932815206\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 194.65514333498348, Validation Loss: 458.93165860823774\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 185.41997616593588, Validation Loss: 436.8026108560823\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 177.42411340910633, Validation Loss: 439.8713241682331\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 170.01063335463618, Validation Loss: 470.71602048026824\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 164.92958290219846, Validation Loss: 412.36857120507756\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 160.12470935548544, Validation Loss: 425.2737136838456\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 155.07263824065888, Validation Loss: 372.98216339507496\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 151.25527531354183, Validation Loss: 405.8693326570255\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 148.20602577254175, Validation Loss: 424.4395577617087\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 144.52426730729607, Validation Loss: 400.9511461082551\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 141.37915951240615, Validation Loss: 398.69595946087946\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 139.20276754430827, Validation Loss: 387.86861488412495\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 136.24398738724506, Validation Loss: 418.6555034609363\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 133.56539861359806, Validation Loss: 362.51707048833407\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 129.00150970478927, Validation Loss: 350.5799358642184\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 127.03575357007071, Validation Loss: 348.2734961447136\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 125.67638763945696, Validation Loss: 354.13868368412244\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 124.5984539591009, Validation Loss: 361.18145520688853\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 123.78965685575591, Validation Loss: 357.86384202619155\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 122.85058636954804, Validation Loss: 362.4070121140105\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 122.19554187827173, Validation Loss: 359.74953651375785\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 121.59978981774866, Validation Loss: 362.2458802988758\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 121.85515792663011, Validation Loss: 352.831356474558\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 121.36539105079727, Validation Loss: 353.1892886817646\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 121.18662284054349, Validation Loss: 352.86293311093124\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 120.88797777334587, Validation Loss: 353.2499092080274\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early Stopping 준비\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# 메인 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    # train_loop 함수 호출 시 sequence_length 추가\n",
    "    avg_train_loss = train_loop(gcn_model, train_loader, optimizer, loss_function, current_device, edge_index, num_nodes, num_node_features, sequence_length)\n",
    "    # val_loop 함수 호출 시 sequence_length 추가\n",
    "    avg_val_loss = val_loop(gcn_model, val_loader, loss_function, current_device, edge_index, num_nodes, num_node_features, sequence_length)\n",
    "\n",
    "    print(f'Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_loss - min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_wts = copy.deepcopy(gcn_model.state_dict())\n",
    "        torch.save(gcn_model.state_dict(), 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 가중치 로드\n",
    "gcn_model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30  # Sequence length\n",
    "n_features = 126  # Number of features\n",
    "output_units = (21 * 6)  # Output shape\n",
    "head_size = 256  # Size of attention head\n",
    "num_heads = 7  # Number of attention heads\n",
    "ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "num_blocks = 4  # Number of transformer blocks\n",
    "mlp_units = [512, 256, 128]  # Size of the dense layers of the final classifier\n",
    "dropout_rate = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: torch.Size([128, 30, 126])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 18\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# # 데이터 차원 확인 및 변환\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# batch_size, seq_len, features = data.shape[0], sequence_length, num_node_features\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# if len(data.shape) != 4 or data.shape[2] * data.shape[3] != features * num_nodes:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 데이터 차원 변환\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, sequence_length, num_nodes, num_node_features]\u001b[39;00m\n\u001b[0;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(batch_size \u001b[38;5;241m*\u001b[39m seq_len, num_nodes\u001b[38;5;241m*\u001b[39mnum_node_features)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4"
     ]
    }
   ],
   "source": [
    "# GCN 모델의 출력을 저장할 리스트 초기화\n",
    "gcn_outputs = []\n",
    "\n",
    "# train_loader에서 배치를 가져와서 GCN 모델의 출력을 수집\n",
    "for data, _ in train_loader:\n",
    "    data = data.to(current_device)\n",
    "    \n",
    "    # 데이터의 현재 차원을 확인\n",
    "    print(\"Data shape:\", data.shape)\n",
    "\n",
    "    # # 데이터 차원 확인 및 변환\n",
    "    # batch_size, seq_len, features = data.shape[0], sequence_length, num_node_features\n",
    "    # if len(data.shape) != 4 or data.shape[2] * data.shape[3] != features * num_nodes:\n",
    "    #     print(f\"Incorrect data shape: {data.shape}\")\n",
    "    #     break\n",
    "\n",
    "    # 데이터 차원 변환\n",
    "    data = data.permute(0, 1, 3, 2)  # [batch_size, sequence_length, num_nodes, num_node_features]\n",
    "    data = data.reshape(batch_size * seq_len, num_nodes*num_node_features)\n",
    "\n",
    "    try:\n",
    "        gcn_output = gcn_model(data, edge_index)\n",
    "        gcn_outputs.append(gcn_output.detach())\n",
    "    except RuntimeError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        print(f\"Data shape: {data.shape}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_features, n_input):\n",
    "        super(PositionalEncoding, self).__init__() # 상속받은 nn.Module 클래스의 __init__() 메서드 호출\n",
    "        pe = torch.zeros(n_input, n_features) # 가장 큰 시퀀스 길이인 max_len을 기준으로 모두 0으로 채워진 크기가 (max_len, n_features)인 텐서 생성\n",
    "        position = torch.arange(0, n_input, dtype=torch.float).unsqueeze(1) # position 텐서 생성\n",
    "\n",
    "        # div_term을 계산하는 방식 수정\n",
    "        div_term = torch.exp(torch.arange(0, n_features, 2).float() * (-math.log(10000.0) / n_features)) # div_term 계산\n",
    "        \n",
    "        # div_term의 길이를 n_features의 절반으로 조정\n",
    "        div_term = div_term.repeat_interleave(2)[:n_features] # div_term 텐서 생성\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2]) # 짝수 인덱스에는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2]) # 홀수 인덱스에는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0) # pe = [bs, seq_len, n_feautres]\n",
    "        self.register_buffer('pe', pe) # pe 텐서를 모델의 버퍼로 등록\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:, :x.size(1), :].to(x.device)\n",
    "        # print(f\"x.size(): {x.size()}, pe.size(): {pe.size()}\")\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "# Transformer Block 정의\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(n_features, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_features, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "# 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout, n_input):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(n_features, num_heads, ff_dim, dropout) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        # 첫 번째 nn.Linear 층의 입력 차원을 n_features로 설정\n",
    "        self.layers.add_module(\"dense_0\", nn.Linear(n_features, mlp_units[0]))\n",
    "        self.layers.add_module(\"relu_0\", nn.ReLU())\n",
    "        self.layers.add_module(\"dropout_0\", nn.Dropout(dropout))\n",
    "        self.layers.add_module(\"norm_0\", nn.LayerNorm(mlp_units[0]))\n",
    "\n",
    "        # 이후 층들에 대한 설정\n",
    "        for i in range(1, len(mlp_units)):\n",
    "            self.layers.add_module(f\"dense_{i}\", nn.Linear(mlp_units[i-1], mlp_units[i]))\n",
    "            self.layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            self.layers.add_module(f\"dropout_{i}\", nn.Dropout(dropout))\n",
    "            self.layers.add_module(f\"norm_{i}\", nn.LayerNorm(mlp_units[i]))\n",
    "\n",
    "        # 최종 출력 층\n",
    "        self.out = nn.Linear(mlp_units[-1], n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.out(x)\n",
    "\n",
    "transformer_model = TransformerModel(n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout_rate, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 적용\n",
    "pos_encoder = PositionalEncoding(n_features=n_features, n_input=n_input)\n",
    "# transformer_input_sequences = pos_encoder(transformer_input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "# x = torch.rand(128, 30, 126)\n",
    "# y = pos_encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'm_avg_PelvisPosX', 'm_avg_PelvisPosY', 'm_avg_PelvisPosZ',\n",
    "    'm_avg_PelvisRotX', 'm_avg_PelvisRotY', 'm_avg_PelvisRotZ',\n",
    "\n",
    "    'm_avg_L_HipPosX', 'm_avg_L_HipPosY', 'm_avg_L_HipPosZ',\n",
    "    'm_avg_L_HipRotX', 'm_avg_L_HipRotY', 'm_avg_L_HipRotZ',\n",
    "\n",
    "    'm_avg_L_KneePosX', 'm_avg_L_KneePosY', 'm_avg_L_KneePosZ',\n",
    "    'm_avg_L_KneeRotX', 'm_avg_L_KneeRotY', 'm_avg_L_KneeRotZ',\n",
    "\n",
    "    'm_avg_L_AnklePosX', 'm_avg_L_AnklePosY', 'm_avg_L_AnklePosZ',\n",
    "    'm_avg_L_AnkleRotX', 'm_avg_L_AnkleRotY', 'm_avg_L_AnkleRotZ',\n",
    "\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "\n",
    "    'm_avg_R_HipPosX', 'm_avg_R_HipPosY', 'm_avg_R_HipPosZ',\n",
    "    'm_avg_R_HipRotX', 'm_avg_R_HipRotY', 'm_avg_R_HipRotZ',\n",
    "\n",
    "    'm_avg_R_KneePosX', 'm_avg_R_KneePosY', 'm_avg_R_KneePosZ',\n",
    "    'm_avg_R_KneeRotX', 'm_avg_R_KneeRotY', 'm_avg_R_KneeRotZ',\n",
    "\n",
    "    'm_avg_R_AnklePosX', 'm_avg_R_AnklePosY', 'm_avg_R_AnklePosZ',\n",
    "    'm_avg_R_AnkleRotX', 'm_avg_R_AnkleRotY', 'm_avg_R_AnkleRotZ',\n",
    "\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "\n",
    "    'm_avg_Spine1PosX', 'm_avg_Spine1PosY', 'm_avg_Spine1PosZ',\n",
    "    'm_avg_Spine1RotX', 'm_avg_Spine1RotY', 'm_avg_Spine1RotZ',\n",
    "\n",
    "    'm_avg_Spine2PosX', 'm_avg_Spine2PosY', 'm_avg_Spine2PosZ',\n",
    "    'm_avg_Spine2RotX', 'm_avg_Spine2RotY', 'm_avg_Spine2RotZ',\n",
    "\n",
    "    'm_avg_L_CollarPosX', 'm_avg_L_CollarPosY', 'm_avg_L_CollarPosZ',\n",
    "    'm_avg_L_CollarRotX', 'm_avg_L_CollarRotY', 'm_avg_L_CollarRotZ',\n",
    "\n",
    "    'm_avg_L_ShoulderPosX', 'm_avg_L_ShoulderPosY', 'm_avg_L_ShoulderPosZ',\n",
    "    'm_avg_L_ShoulderRotX', 'm_avg_L_ShoulderRotY', 'm_avg_L_ShoulderRotZ',\n",
    "\n",
    "    'm_avg_L_ElbowPosX', 'm_avg_L_ElbowPosY', 'm_avg_L_ElbowPosZ',\n",
    "    'm_avg_L_ElbowRotX', 'm_avg_L_ElbowRotY', 'm_avg_L_ElbowRotZ',\n",
    "\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "\n",
    "    'm_avg_NeckPosX', 'm_avg_NeckPosY', 'm_avg_NeckPosZ',\n",
    "    'm_avg_NeckRotX', 'm_avg_NeckRotY', 'm_avg_NeckRotZ',\n",
    "\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "\n",
    "    'm_avg_R_CollarPosX', 'm_avg_R_CollarPosY', 'm_avg_R_CollarPosZ',\n",
    "    'm_avg_R_CollarRotX', 'm_avg_R_CollarRotY', 'm_avg_R_CollarRotZ',\n",
    "\n",
    "    'm_avg_R_ShoulderPosX', 'm_avg_R_ShoulderPosY', 'm_avg_R_ShoulderPosZ',\n",
    "    'm_avg_R_ShoulderRotX', 'm_avg_R_ShoulderRotY', 'm_avg_R_ShoulderRotZ',\n",
    "\n",
    "    'm_avg_R_ElbowPosX', 'm_avg_R_ElbowPosY', 'm_avg_R_ElbowPosZ',\n",
    "    'm_avg_R_ElbowRotX', 'm_avg_R_ElbowRotY', 'm_avg_R_ElbowRotZ',\n",
    "    \n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "]\n",
    "\n",
    "weighted_columns_indices = [column_order.index(name) for name in column_names]\n",
    "print(weighted_columns_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weighted_columns_indices, weight_for_weighted_columns, threshold, penalty_weight):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weighted_columns_indices = torch.tensor(weighted_columns_indices)\n",
    "        self.weight_for_weighted_columns = weight_for_weighted_columns\n",
    "        self.threshold = threshold  # 신체 움직임의 임계값\n",
    "        self.penalty_weight = penalty_weight  # 비정상적 움직임에 대한 패널티 가중치\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # 각 조인트의 6개 특성에 대한 손실을 계산하기 위해 y_true와 y_pred의 마지막 차원을 제거\n",
    "        y_pred_values = y_pred[:, :]\n",
    "\n",
    "        # MSE 계산\n",
    "        mse = F.mse_loss(y_true[:, :], y_pred_values, reduction='none')\n",
    "        mse = mse.mean(axis=-1)\n",
    "\n",
    "        # 특정 joint에 대한 가중치 적용\n",
    "        weighted_mse = y_pred[:, self.weighted_columns_indices]\n",
    "        weighted_mse = (weighted_mse ** 2) * self.weight_for_weighted_columns\n",
    "        mse += weighted_mse.mean(axis=-1)\n",
    "\n",
    "        # 과도한 움직임에 대한 패널티 적용\n",
    "        excessive_movement_penalty = (y_pred - y_true).abs() > self.threshold\n",
    "        penalty = excessive_movement_penalty.type(torch.float32) * self.penalty_weight\n",
    "        mse += penalty.mean(axis=-1)\n",
    "\n",
    "        return mse.mean()  # 전체 배치에 대한 평균 손실 반환\n",
    "\n",
    "# 가중치를 적용할 열 인덱스와 가중치 값\n",
    "weighted_columns_indices = weighted_columns_indices\n",
    "weight_for_weighted_columns = 2.0\n",
    "\n",
    "# 임계값과 패널티 가중치 설정\n",
    "threshold = 10.0  # 예시 임계값\n",
    "penalty_weight = 0.8  # 예시 패널티 가중치\n",
    "\n",
    "# CustomLoss 인스턴스 생성\n",
    "custom_loss_instance = CustomLoss(\n",
    "    weighted_columns_indices=weighted_columns_indices,\n",
    "    weight_for_weighted_columns=weight_for_weighted_columns,\n",
    "    threshold=threshold,\n",
    "    penalty_weight=penalty_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화기와 손실 함수\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001)\n",
    "criterion = custom_loss_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "epochs = 100\n",
    "patience = 7  # Early Stopping patience\n",
    "best_loss = np.inf\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=7, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer_model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(model, input_size=(n_input, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 루프\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 가장 좋은 손실 값 초기화\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    # 훈련 데이터 로더에 대한 프로그레스 바 추가\n",
    "    train_progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    for data, target in train_progress_bar:\n",
    "        # 데이터를 현재 장치로 이동\n",
    "        data = data.to(current_device)\n",
    "        target = target.to(current_device)\n",
    "        edge_index = edge_index.to(current_device)  # edge_index도 같은 장치로 이동\n",
    "\n",
    "        # GCN 모델에 데이터와 edge_index를 전달하고 출력을 얻음\n",
    "        gcn_output = gcn_model(data, edge_index)\n",
    "\n",
    "        # Transformer 모델에 GCN 모델의 출력을 전달\n",
    "        output = transformer_model(gcn_output)  # transformer_model로 이름을 명확히 표시\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(output, target)\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 프로그레스 바 업데이트\n",
    "        train_progress_bar.set_postfix({'train_loss': loss.item()})\n",
    "\n",
    "    # 에포크당 평균 훈련 손실 저장\n",
    "    train_losses.append(train_loss / len(train_loader.dataset))\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    # 검증 데이터 로더에 대한 프로그레스 바 추가\n",
    "    val_progress_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{epochs}', unit='batch')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_progress_bar:\n",
    "            # 데이터를 현재 장치로 이동\n",
    "            data = data.to(current_device)\n",
    "            target = target.to(current_device)\n",
    "            edge_index = edge_index.to(current_device)\n",
    "\n",
    "            # GCN 모델에 데이터를 전달하고 출력을 얻음\n",
    "            gcn_output = gcn_model(data, edge_index)\n",
    "\n",
    "            # Transformer 모델에 GCN 모델의 출력을 전달\n",
    "            output = transformer_model(gcn_output)\n",
    "\n",
    "            # 손실 계산\n",
    "            loss = criterion(output, target)\n",
    "            val_loss += loss.item() * data.size(0)\n",
    "\n",
    "            # 프로그레스 바 업데이트\n",
    "            val_progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "    # 에포크당 평균 검증 손실 저장\n",
    "    val_losses.append(val_loss / len(val_loader.dataset))\n",
    "\n",
    "    # 로그 출력\n",
    "    print(f'Epoch {epoch+1} \\t Training Loss: {train_loss / len(train_loader.dataset)} \\t Validation Loss: {val_loss / len(val_loader.dataset)}')\n",
    "\n",
    "    # Learning Rate Scheduler 업데이트\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early Stopping 체크\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        # 모델 상태 저장 (필요에 따라 GCN 모델의 상태도 저장)\n",
    "        torch.save({'transformer_model': model.state_dict(),\n",
    "                    'gcn_model': gcn_model.state_dict()}, 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 가장 좋은 모델 가중치 로드\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'final_model_Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./B01_TransformData_FinalAvatar_20230922_171230.csv').iloc[300:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rotation_columns = [col for col in test_df.columns if 'Rot' in col]\n",
    "\n",
    "test_position_columns = [col for col in test_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "test_rotation_df = test_df[test_rotation_columns]\n",
    "test_position_df = test_df[test_position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "test_rotation_df = test_rotation_df.apply(lambda col: col.apply(normalize_angle))\n",
    "\n",
    "# Position 컬럼만 0~1 사이로 정규화\n",
    "def normalize_columns(df):\n",
    "    for col in df.columns:\n",
    "        if 'Pos' in col:  # 위치에 대한 컬럼만 정규화\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            df[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df\n",
    "\n",
    "test_position_df = normalize_columns(test_position_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = test_rotation_df.apply(lambda col: col.apply(lambda x: x != 999 and (x > 180 or x < -180))).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position 컬럼은 그대로 두고, 다시 합치기\n",
    "# test_posrot_df = pd.concat([test_position_df, test_rotation_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조인트 이름 추출\n",
    "joint_names = [col.split('PosX')[0] for col in test_position_df.columns if 'PosX' in col]\n",
    "\n",
    "# 빈 데이터 프레임 생성\n",
    "test_posrot_df = pd.DataFrame()\n",
    "\n",
    "# 각 조인트에 대해 위치 데이터와 회전 데이터를 순차적으로 배열\n",
    "for joint in joint_names:\n",
    "    test_posrot_df[f'{joint}PosX'] = test_position_df[f'{joint}PosX']\n",
    "    test_posrot_df[f'{joint}PosY'] = test_position_df[f'{joint}PosY']\n",
    "    test_posrot_df[f'{joint}PosZ'] = test_position_df[f'{joint}PosZ']\n",
    "    test_posrot_df[f'{joint}RotX'] = test_rotation_df[f'{joint}RotX']\n",
    "    test_posrot_df[f'{joint}RotY'] = test_rotation_df[f'{joint}RotY']\n",
    "    test_posrot_df[f'{joint}RotZ'] = test_rotation_df[f'{joint}RotZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_posrot_df = test_posrot_df.map(lambda x: float(f\"{x:.2f}\") if isinstance(x, (int, float)) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 새로운 CSV 파일로 저장합니다.\n",
    "test_posrot_df.to_csv('./test_posrot_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = TransformerModel(\n",
    "    n_features=n_features,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_blocks=num_blocks,\n",
    "    mlp_units=mlp_units,\n",
    "    dropout=dropout_rate,\n",
    "    n_input=n_input\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('final_model_Transformer.pth'))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 값을 넣을 빈 리스트\n",
    "test_predictions = []\n",
    "\n",
    "# 훈련 데이터셋에서 마지막 입력 개수의 값을 가져온 후\n",
    "current_batch = torch.from_numpy(test_posrot_df[-n_input:].values.astype(np.float32)).reshape((1, n_input, n_features))\n",
    "\n",
    "# 모델이 사용하는 디바이스를 확인하고 데이터를 해당 디바이스로 옮깁니다.\n",
    "current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(current_device)\n",
    "current_batch = current_batch.to(current_device)\n",
    "\n",
    "# 예측 과정 반복\n",
    "with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "    for i in range(1):\n",
    "        # 현재 배치에서 다음 포인트를 예측\n",
    "        current_pred = model(current_batch).cpu().numpy()[0]  # 마지막 시퀀스 포인트 예측\n",
    "\n",
    "        # 예측된 마지막 프레임을 리스트에 추가\n",
    "        test_predictions.append(current_pred)\n",
    "\n",
    "        # 새로운 배치 생성: 마지막 시퀀스 제외하고 예측값 추가\n",
    "        current_batch = np.roll(current_batch.cpu().numpy(), -1, axis=1)\n",
    "        current_batch[:, -1, :] = current_pred\n",
    "        current_batch = torch.from_numpy(current_batch).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_array = np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 배열을 데이터프레임으로 변환합니다. 이때 column_order 리스트를 열 이름으로 사용합니다.\n",
    "test_predictions = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "# test_predictions 데이터프레임을 CSV 파일로 저장합니다.\n",
    "test_predictions.to_csv('./test_predictions.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 데이터(test_df)의 마지막 30 프레임과 \n",
    "last_inputs_df = test_posrot_df.iloc[-30:][column_order].reset_index(drop=True)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "test_combined_df = pd.concat([last_inputs_df, test_predictions_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df.to_csv('./test_combined_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_skeleton(df, connections):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot each joint\n",
    "    for joint in joint_names:\n",
    "        ax.scatter(df[f'{joint}PosX'], df[f'{joint}PosY'], df[f'{joint}PosZ'], label=joint)\n",
    "\n",
    "    # Draw lines (bones) connecting the joints\n",
    "    for connection in connections:\n",
    "        joint_from, joint_to = connection\n",
    "        # Update to use the correct column names\n",
    "        ax.plot(\n",
    "            [df[f'm_avg_{joint_from}PosX'], df[f'm_avg_{joint_to}PosX']],\n",
    "            [df[f'm_avg_{joint_from}PosY'], df[f'm_avg_{joint_to}PosY']],\n",
    "            [df[f'm_avg_{joint_from}PosZ'], df[f'm_avg_{joint_to}PosZ']],\n",
    "            'k-'\n",
    "        )\n",
    "\n",
    "    # Set labels and legend\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Set the initial viewing angle\n",
    "    ax.view_init(elev=20., azim=-90)\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_skeleton(test_predictions.iloc[0], connections)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
