{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './csvFiles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [file for file in os.listdir(directory) if file.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 목록을 랜덤하게 섞습니다.\n",
    "random.seed(42)  # 재현 가능한 결과를 위해 시드 설정\n",
    "random.shuffle(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 파일에서 컬럼 이름을 가져옵니다.\n",
    "first_file_path = os.path.join(directory, csv_files[0])\n",
    "first_df = pd.read_csv(first_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator_token = 999\n",
    "separator = pd.DataFrame({col: separator_token for col in first_df.columns}, index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m csv_files:\n\u001b[0;32m      3\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(directory, file)\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m300\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m      6\u001b[0m     df_list\u001b[38;5;241m.\u001b[39mappend(df)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\transformer\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\transformer\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\transformer\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1749\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\transformer\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:843\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:920\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1082\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:1455\u001b[0m, in \u001b[0;36mpandas._libs.parsers._maybe_upcast\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\transformer\\lib\\site-packages\\numpy\\core\\multiarray.py:1131\u001b[0m, in \u001b[0;36mputmask\u001b[1;34m(a, mask, values)\u001b[0m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;124;03m    copyto(dst, src, casting='same_kind', where=True)\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \n\u001b[0;32m   1127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dst, src, where)\n\u001b[1;32m-> 1131\u001b[0m \u001b[38;5;129m@array_function_from_c_func_and_dispatcher\u001b[39m(_multiarray_umath\u001b[38;5;241m.\u001b[39mputmask)\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mputmask\u001b[39m(a, \u001b[38;5;241m/\u001b[39m, mask, values):\n\u001b[0;32m   1133\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;124;03m    putmask(a, mask, values)\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1171\u001b[0m \n\u001b[0;32m   1172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (a, mask, values)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_list = []\n",
    "for file in csv_files:\n",
    "    file_path = os.path.join(directory, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.iloc[300:-100]\n",
    "    df_list.append(df)\n",
    "    df_list.append(separator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation 컬럼만 선택\n",
    "rotation_columns = [col for col in combined_df.columns if 'Rot' in col]\n",
    "\n",
    "# Position 컬럼만 선택\n",
    "position_columns = [col for col in combined_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "rotation_df = combined_df[rotation_columns]\n",
    "position_df = combined_df[position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "rotation_df = rotation_df.apply(lambda col: col.apply(normalize_angle))\n",
    "\n",
    "# Position 컬럼만 0~1 사이로 정규화\n",
    "def normalize_columns(df):\n",
    "    for col in df.columns:\n",
    "        if 'Pos' in col:  # 위치에 대한 컬럼만 정규화\n",
    "            # 999 값을 제외하고 min 및 max를 계산\n",
    "            filtered_df = df[df[col] != 999]\n",
    "            min_val = filtered_df[col].min()\n",
    "            max_val = filtered_df[col].max()\n",
    "\n",
    "            # 999가 아닌 값만 정규화\n",
    "            df.loc[df[col] != 999, col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df\n",
    "\n",
    "position_df = normalize_columns(position_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = rotation_df.apply(lambda col: col.apply(lambda x: x != 999 and (x > 180 or x < -180))).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조인트 이름 추출\n",
    "joint_names = [col.split('PosX')[0] for col in position_df.columns if 'PosX' in col]\n",
    "\n",
    "# 빈 데이터 프레임 생성\n",
    "posrot_df = pd.DataFrame()\n",
    "\n",
    "# 각 조인트에 대해 위치 데이터와 회전 데이터를 순차적으로 배열\n",
    "for joint in joint_names:\n",
    "    posrot_df[f'{joint}PosX'] = position_df[f'{joint}PosX']\n",
    "    posrot_df[f'{joint}PosY'] = position_df[f'{joint}PosY']\n",
    "    posrot_df[f'{joint}PosZ'] = position_df[f'{joint}PosZ']\n",
    "    posrot_df[f'{joint}RotX'] = rotation_df[f'{joint}RotX']\n",
    "    posrot_df[f'{joint}RotY'] = rotation_df[f'{joint}RotY']\n",
    "    posrot_df[f'{joint}RotZ'] = rotation_df[f'{joint}RotZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Position 컬럼은 그대로 두고, 다시 합치기\n",
    "posrot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 데이터를 DataFrame으로 변환\n",
    "posrot_df = pd.DataFrame(posrot_df, columns=posrot_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = posrot_df.iloc[:1000400]\n",
    "test = posrot_df.iloc[1000400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(train, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30 # 시퀀스 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeseriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length):\n",
    "        self.data = data\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.sequence_length + 1\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.sequence_length] # : 입력 시퀀스 (30개의 데이터)\n",
    "        y = self.data[index+self.sequence_length-1] # : 예측 시퀀스 (1개의 데이터)\n",
    "        return torch.from_numpy(x).float(), torch.from_numpy(y).float()\n",
    "\n",
    "# 데이터셋과 데이터 로더를 생성\n",
    "train_dataset = TimeseriesDataset(X_train.values, n_input)\n",
    "val_dataset = TimeseriesDataset(X_val.values, n_input)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로더에서 첫 번째 배치를 가져와서 형태를 확인\n",
    "first_batch = next(iter(train_loader))\n",
    "X, y = first_batch\n",
    "\n",
    "print(\"X batch shape:\", X.shape)\n",
    "print(\"y batch shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN skeleton joint learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_names = ['Pelvis', 'L_Hip', 'L_Knee', 'L_Ankle', 'L_Foot', \n",
    "                'R_Hip', 'R_Knee', 'R_Ankle', 'R_Foot', 'Spine1', 'Spine2',\n",
    "               'L_Collar', 'L_Shoulder', 'L_Elbow', 'L_Wrist', 'Neck', 'Head',\n",
    "               'R_Collar', 'R_Shoulder', 'R_Elbow', 'R_Wrist']\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connections = [\n",
    "    ('Pelvis', 'L_Hip'), ('Pelvis', 'R_Hip'),\n",
    "    ('L_Hip', 'L_Knee'), ('R_Hip', 'R_Knee'),\n",
    "    ('L_Knee', 'L_Ankle'), ('R_Knee', 'R_Ankle'),\n",
    "    ('L_Ankle', 'L_Foot'), ('R_Ankle', 'R_Foot'),\n",
    "    ('Spine1', 'Spine2'), ('Spine2', 'Pelvis'),\n",
    "    ('Spine1', 'Neck'), ('Neck', 'Head'),\n",
    "    ('Spine1', 'L_Collar'), ('Spine1', 'R_Collar'),\n",
    "    ('L_Collar', 'L_Shoulder'), ('R_Collar', 'R_Shoulder'),\n",
    "    ('L_Shoulder', 'L_Elbow'), ('R_Shoulder', 'R_Elbow'),\n",
    "    ('L_Elbow', 'L_Wrist'), ('R_Elbow', 'R_Wrist')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조인트 이름을 인덱스로 매핑\n",
    "joint_to_index = {joint: idx for idx, joint in enumerate(joint_names)}\n",
    "\n",
    "# 연결 정보를 인덱스 쌍으로 변환\n",
    "edge_pairs = [(joint_to_index[joint_from], joint_to_index[joint_to]) for joint_from, joint_to in connections]\n",
    "\n",
    "# edge_index 생성\n",
    "edge_index = torch.tensor(edge_pairs, dtype=torch.long).t().contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "batch_size = 128\n",
    "num_nodes = 21  # 노드 수 (신체 관절 수)\n",
    "num_node_features = 6  # 각 노드의 특성 수 (위치 및 회전 정보)\n",
    "num_classes = 126  # 출력 클래스 수 (전체 특성 수)\n",
    "sequence_length = 30  # 시퀀스 길이\n",
    "in_channels = 30\n",
    "out_channels = 256\n",
    "\n",
    "lr = 0.0001\n",
    "patience = 10\n",
    "min_delta = 0.001\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalLayer(nn.Module):\n",
    "    \"\"\" 시간적 특성을 처리하기 위한 레이어 \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(TemporalLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        print(\"TemporalLayer Initialized:\")\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # [batch_size, num_nodes * num_node_features, sequence_length]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        return x\n",
    "\n",
    "class GCNWithTemporal(nn.Module):\n",
    "    \"\"\" 공간적 및 시간적 특성을 모두 고려하는 GCN 모델 \"\"\"\n",
    "    def __init__(self, num_node_features, num_classes, sequence_length, num_nodes):\n",
    "        super(GCNWithTemporal, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 256)\n",
    "        self.temporal_layer = TemporalLayer(num_nodes * 256, 256)\n",
    "        self.out = nn.Linear(7680, num_classes) # [batch_size, 256 * 30] = [batch_size, 7680]\n",
    "        print(\"GCNWithTemporal Initialized:\")\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 입력 데이터의 형태 확인 (4차원으로 처리)\n",
    "        # print(\"Original input shape to GCNWithTemporal:\", x.shape) [128, 30, 21, 6])\n",
    "        \n",
    "        # x의 형태를 [batch_size, sequence_length, num_nodes, num_node_features]에서\n",
    "        # [batch_size * sequence_length, num_nodes, num_node_features]로 변환\n",
    "        batch_size, sequence_length, num_nodes, num_node_features = x.shape\n",
    "        x = x.reshape(batch_size * sequence_length, num_nodes, num_node_features)\n",
    "\n",
    "        # GCN 적용\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        # print(\"Shape after GCN layers in GCNWithTemporal:\", x.shape) [3840, 21, 256])\n",
    "\n",
    "        # 원래 형태로 복원 (배치 사이즈와 시퀀스 길이를 유지하며 차원 재구성)\n",
    "        x = x.view(batch_size, sequence_length, -1)\n",
    "        # print(\"Shape before TemporalLayer in GCNWithTemporal:\", x.shape) [128, 30, 5376])\n",
    "       \n",
    "        # 시간 차원을 처리합니다.\n",
    "        x = self.temporal_layer(x)\n",
    "        # print(\"Shape after TemporalLayer in GCNWithTemporal:\", x.shape) [128, 256, 30])\n",
    "\n",
    "        # 최종 출력 차원을 확인\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        # print(\"Shape before final linear layer in GCNWithTemporal:\", x.shape) [128, 7680])\n",
    "\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델, 손실 함수, 최적화 알고리즘 초기화\n",
    "gcn_model = GCNWithTemporal(num_node_features, num_classes, sequence_length, num_nodes)\n",
    "optimizer = optim.Adam(gcn_model.parameters(), lr=0.0001)\n",
    "loss_function = torch.nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 현재 장치로 이동합니다.\n",
    "gcn_model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실을 저장할 리스트 초기화\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "# 가장 좋은 손실 값 초기화\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 루프 함수\n",
    "def train_loop(gcn_model, dataloader, optimizer, loss_function, device, edge_index, num_nodes, num_node_features, sequence_length):\n",
    "    gcn_model.train()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for data, target in progress_bar:\n",
    "        # 데이터 형태와 edge_index 검증을 위한 출력\n",
    "        # print(\"Original data shape:\", data.shape)\n",
    "        # print(\"Edge index shape:\", edge_index.shape)\n",
    "        # print(\"Edge index:\", edge_index)\n",
    "\n",
    "        # 데이터 차원 변경\n",
    "        edge_index = edge_index.to(device)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1, num_nodes, num_node_features, sequence_length)  \n",
    "        # print(\"View applied data shape:\", data.shape)\n",
    "        data = data.permute(0, 3, 1, 2)  \n",
    "        # print(\"Permute applied data shape:\", data.shape)\n",
    "        target = target.view(-1, num_classes)\n",
    "\n",
    "        # 데이터 형태 출력\n",
    "        # print(\"Transformed data shape:\", data.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = gcn_model(data, edge_index)\n",
    "        loss = loss_function(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * data.size(0)\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader.dataset)\n",
    "    return avg_loss\n",
    "\n",
    "# 검증 루프 함수\n",
    "def val_loop(gcn_model, dataloader, loss_function, device, edge_index, num_nodes, num_node_features, sequence_length):\n",
    "    gcn_model.eval()\n",
    "    total_loss = 0.0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            edge_index = edge_index.to(device)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1, num_nodes, num_node_features, sequence_length)  \n",
    "            data = data.permute(0, 3, 1, 2)  \n",
    "            target = target.view(-1, num_classes)\n",
    "\n",
    "            output = gcn_model(data, edge_index)\n",
    "            loss = loss_function(output, target)\n",
    "\n",
    "            total_loss += loss.item() * data.size(0)\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader.dataset)\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping 준비\n",
    "best_loss = float('inf')\n",
    "early_stopping_counter = 0\n",
    "\n",
    "# 메인 학습 루프\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch+1}/{epochs}')\n",
    "    # train_loop 함수 호출 시 sequence_length 추가\n",
    "    avg_train_loss = train_loop(gcn_model, train_loader, optimizer, loss_function, current_device, edge_index, num_nodes, num_node_features, sequence_length)\n",
    "    # val_loop 함수 호출 시 sequence_length 추가\n",
    "    avg_val_loss = val_loop(gcn_model, val_loader, loss_function, current_device, edge_index, num_nodes, num_node_features, sequence_length)\n",
    "\n",
    "    print(f'Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}')\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_loss - min_delta:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_wts = copy.deepcopy(gcn_model.state_dict())\n",
    "        torch.save(gcn_model.state_dict(), 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "# 최적 모델 가중치 로드\n",
    "gcn_model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "n_input = 30  # Sequence length\n",
    "n_features = 126  # Number of features\n",
    "output_units = (21 * 6)  # Output shape\n",
    "head_size = 256  # Size of attention head\n",
    "num_heads = 7  # Number of attention heads\n",
    "ff_dim = 512  # Hidden layer size in feed forward network inside transformer\n",
    "num_blocks = 4  # Number of transformer blocks\n",
    "mlp_units = [512, 256, 128]  # Size of the dense layers of the final classifier\n",
    "dropout_rate = 0.3\n",
    "pad_value = 0  # 패딩에 사용할 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCN 모델의 출력을 저장할 리스트 초기화\n",
    "gcn_outputs = []\n",
    "targets_list = []\n",
    "\n",
    "for data, target in train_loader:\n",
    "    data = data.to(current_device)\n",
    "    target = target.to(current_device)\n",
    "    edge_index = edge_index.to(current_device)\n",
    "\n",
    "    batch_size, seq_len, total_features = data.shape\n",
    "    if total_features == num_nodes * num_node_features:\n",
    "        data = data.view(batch_size, seq_len, num_node_features, num_nodes)\n",
    "        data = data.permute(0, 1, 3, 2)\n",
    "\n",
    "        try:\n",
    "            gcn_output = gcn_model(data, edge_index)\n",
    "            gcn_outputs.append(gcn_output.detach())\n",
    "            targets_list.append(target)  # 해당 배치의 타겟 데이터 저장\n",
    "        except RuntimeError as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "# GCN 모델의 출력 텐서들의 차원을 확인\n",
    "for i, output in enumerate(gcn_outputs):\n",
    "    print(f\"Output {i} shape: {output.shape}\")\n",
    "\n",
    "# 모든 배치에 대한 GCN 모델의 출력을 하나의 텐서로 결합\n",
    "gcn_outputs_tensor = torch.cat(gcn_outputs, dim=0)\n",
    "print(\"Combined GCN outputs tensor shape:\", gcn_outputs_tensor.shape)\n",
    "\n",
    "# 시퀀스 분할 및 패딩 추가\n",
    "transformer_inputs = []\n",
    "for i in range(0, gcn_outputs_tensor.size(0), n_input):\n",
    "    seq = gcn_outputs_tensor[i:i+n_input]  # n_input 길이의 시퀀스 추출\n",
    "    if seq.size(0) < n_input:\n",
    "        # 패딩이 필요한 경우\n",
    "        padding = torch.full((n_input - seq.size(0), n_features), pad_value, device=seq.device)\n",
    "        seq = torch.cat([seq, padding], dim=0)  # 시퀀스 끝에 패딩 추가\n",
    "    transformer_inputs.append(seq.unsqueeze(0))  # 배치 차원 추가\n",
    "\n",
    "# transformer_inputs 리스트를 하나의 텐서로 결합\n",
    "transformer_inputs_tensor = torch.cat(transformer_inputs, dim=0)\n",
    "print(\"Transformer inputs tensor shape:\", transformer_inputs_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_tensor = torch.cat(targets_list, dim=0)\n",
    "\n",
    "# Custom Dataset class to handle inputs and targets together\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.targets[idx]\n",
    "    \n",
    "# 데이터셋 인스턴스 생성\n",
    "dataset = CustomDataset(transformer_inputs_tensor, targets_tensor)\n",
    "\n",
    "# 데이터셋을 훈련 세트와 검증 세트로 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 정의\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, n_features, n_input):\n",
    "        super(PositionalEncoding, self).__init__() # 상속받은 nn.Module 클래스의 __init__() 메서드 호출\n",
    "        pe = torch.zeros(n_input, n_features) # 가장 큰 시퀀스 길이인 max_len을 기준으로 모두 0으로 채워진 크기가 (max_len, n_features)인 텐서 생성\n",
    "        position = torch.arange(0, n_input, dtype=torch.float).unsqueeze(1) # position 텐서 생성\n",
    "\n",
    "        # div_term을 계산하는 방식 수정\n",
    "        div_term = torch.exp(torch.arange(0, n_features, 2).float() * (-math.log(10000.0) / n_features)) # div_term 계산\n",
    "        \n",
    "        # div_term의 길이를 n_features의 절반으로 조정\n",
    "        div_term = div_term.repeat_interleave(2)[:n_features] # div_term 텐서 생성\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term[0::2]) # 짝수 인덱스에는 sin 함수 적용\n",
    "        pe[:, 1::2] = torch.cos(position * div_term[1::2]) # 홀수 인덱스에는 cos 함수 적용\n",
    "        pe = pe.unsqueeze(0) # pe = [bs, seq_len, n_feautres]\n",
    "        self.register_buffer('pe', pe) # pe 텐서를 모델의 버퍼로 등록\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.pe[:, :x.size(1), :].to(x.device)\n",
    "        # print(f\"x.size(): {x.size()}, pe.size(): {pe.size()}\")\n",
    "        x = x + pe\n",
    "        return x\n",
    "\n",
    "# Transformer Block 정의\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, dropout):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(n_features, num_heads, dropout=dropout)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(n_features, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, n_features)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(n_features)\n",
    "        self.norm2 = nn.LayerNorm(n_features)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attention_output, _ = self.attention(x, x, x)\n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        return x\n",
    "    \n",
    "# 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout, n_input):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.pos_encoder = PositionalEncoding(n_features, n_input)\n",
    "        self.transformer_blocks = nn.ModuleList([TransformerBlock(n_features, num_heads, ff_dim, dropout) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.layers = nn.Sequential()\n",
    "        # 첫 번째 nn.Linear 층의 입력 차원을 n_features로 설정\n",
    "        self.layers.add_module(\"dense_0\", nn.Linear(n_features, mlp_units[0]))\n",
    "        self.layers.add_module(\"relu_0\", nn.ReLU())\n",
    "        self.layers.add_module(\"dropout_0\", nn.Dropout(dropout))\n",
    "        self.layers.add_module(\"norm_0\", nn.LayerNorm(mlp_units[0]))\n",
    "\n",
    "        # 이후 층들에 대한 설정\n",
    "        for i in range(1, len(mlp_units)):\n",
    "            self.layers.add_module(f\"dense_{i}\", nn.Linear(mlp_units[i-1], mlp_units[i]))\n",
    "            self.layers.add_module(f\"relu_{i}\", nn.ReLU())\n",
    "            self.layers.add_module(f\"dropout_{i}\", nn.Dropout(dropout))\n",
    "            self.layers.add_module(f\"norm_{i}\", nn.LayerNorm(mlp_units[i]))\n",
    "\n",
    "        # 최종 출력 층\n",
    "        self.out = nn.Linear(mlp_units[-1], n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pos_encoder(x)\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.layers(x)\n",
    "        return self.out(x)\n",
    "\n",
    "transformer_model = TransformerModel(n_features, num_heads, ff_dim, num_blocks, mlp_units, dropout_rate, n_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional Encoding 적용\n",
    "pos_encoder = PositionalEncoding(n_features=n_features, n_input=n_input)\n",
    "# transformer_input_sequences = pos_encoder(transformer_input_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_order = [\n",
    "    'm_avg_PelvisPosX', 'm_avg_PelvisPosY', 'm_avg_PelvisPosZ',\n",
    "    'm_avg_PelvisRotX', 'm_avg_PelvisRotY', 'm_avg_PelvisRotZ',\n",
    "\n",
    "    'm_avg_L_HipPosX', 'm_avg_L_HipPosY', 'm_avg_L_HipPosZ',\n",
    "    'm_avg_L_HipRotX', 'm_avg_L_HipRotY', 'm_avg_L_HipRotZ',\n",
    "\n",
    "    'm_avg_L_KneePosX', 'm_avg_L_KneePosY', 'm_avg_L_KneePosZ',\n",
    "    'm_avg_L_KneeRotX', 'm_avg_L_KneeRotY', 'm_avg_L_KneeRotZ',\n",
    "\n",
    "    'm_avg_L_AnklePosX', 'm_avg_L_AnklePosY', 'm_avg_L_AnklePosZ',\n",
    "    'm_avg_L_AnkleRotX', 'm_avg_L_AnkleRotY', 'm_avg_L_AnkleRotZ',\n",
    "\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "\n",
    "    'm_avg_R_HipPosX', 'm_avg_R_HipPosY', 'm_avg_R_HipPosZ',\n",
    "    'm_avg_R_HipRotX', 'm_avg_R_HipRotY', 'm_avg_R_HipRotZ',\n",
    "\n",
    "    'm_avg_R_KneePosX', 'm_avg_R_KneePosY', 'm_avg_R_KneePosZ',\n",
    "    'm_avg_R_KneeRotX', 'm_avg_R_KneeRotY', 'm_avg_R_KneeRotZ',\n",
    "\n",
    "    'm_avg_R_AnklePosX', 'm_avg_R_AnklePosY', 'm_avg_R_AnklePosZ',\n",
    "    'm_avg_R_AnkleRotX', 'm_avg_R_AnkleRotY', 'm_avg_R_AnkleRotZ',\n",
    "\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "\n",
    "    'm_avg_Spine1PosX', 'm_avg_Spine1PosY', 'm_avg_Spine1PosZ',\n",
    "    'm_avg_Spine1RotX', 'm_avg_Spine1RotY', 'm_avg_Spine1RotZ',\n",
    "\n",
    "    'm_avg_Spine2PosX', 'm_avg_Spine2PosY', 'm_avg_Spine2PosZ',\n",
    "    'm_avg_Spine2RotX', 'm_avg_Spine2RotY', 'm_avg_Spine2RotZ',\n",
    "\n",
    "    'm_avg_L_CollarPosX', 'm_avg_L_CollarPosY', 'm_avg_L_CollarPosZ',\n",
    "    'm_avg_L_CollarRotX', 'm_avg_L_CollarRotY', 'm_avg_L_CollarRotZ',\n",
    "\n",
    "    'm_avg_L_ShoulderPosX', 'm_avg_L_ShoulderPosY', 'm_avg_L_ShoulderPosZ',\n",
    "    'm_avg_L_ShoulderRotX', 'm_avg_L_ShoulderRotY', 'm_avg_L_ShoulderRotZ',\n",
    "\n",
    "    'm_avg_L_ElbowPosX', 'm_avg_L_ElbowPosY', 'm_avg_L_ElbowPosZ',\n",
    "    'm_avg_L_ElbowRotX', 'm_avg_L_ElbowRotY', 'm_avg_L_ElbowRotZ',\n",
    "\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "\n",
    "    'm_avg_NeckPosX', 'm_avg_NeckPosY', 'm_avg_NeckPosZ',\n",
    "    'm_avg_NeckRotX', 'm_avg_NeckRotY', 'm_avg_NeckRotZ',\n",
    "\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "\n",
    "    'm_avg_R_CollarPosX', 'm_avg_R_CollarPosY', 'm_avg_R_CollarPosZ',\n",
    "    'm_avg_R_CollarRotX', 'm_avg_R_CollarRotY', 'm_avg_R_CollarRotZ',\n",
    "\n",
    "    'm_avg_R_ShoulderPosX', 'm_avg_R_ShoulderPosY', 'm_avg_R_ShoulderPosZ',\n",
    "    'm_avg_R_ShoulderRotX', 'm_avg_R_ShoulderRotY', 'm_avg_R_ShoulderRotZ',\n",
    "\n",
    "    'm_avg_R_ElbowPosX', 'm_avg_R_ElbowPosY', 'm_avg_R_ElbowPosZ',\n",
    "    'm_avg_R_ElbowRotX', 'm_avg_R_ElbowRotY', 'm_avg_R_ElbowRotZ',\n",
    "    \n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'm_avg_HeadPosX', 'm_avg_HeadPosY', 'm_avg_HeadPosZ',\n",
    "    'm_avg_L_WristPosX', 'm_avg_L_WristPosY', 'm_avg_L_WristPosZ',\n",
    "    'm_avg_R_WristPosX', 'm_avg_R_WristPosY', 'm_avg_R_WristPosZ',\n",
    "    'm_avg_L_FootPosX', 'm_avg_L_FootPosY', 'm_avg_L_FootPosZ',\n",
    "    'm_avg_R_FootPosX', 'm_avg_R_FootPosY', 'm_avg_R_FootPosZ',\n",
    "    'm_avg_HeadRotX', 'm_avg_HeadRotY', 'm_avg_HeadRotZ',\n",
    "    'm_avg_L_WristRotX', 'm_avg_L_WristRotY', 'm_avg_L_WristRotZ',\n",
    "    'm_avg_R_WristRotX', 'm_avg_R_WristRotY', 'm_avg_R_WristRotZ',\n",
    "    'm_avg_L_FootRotX', 'm_avg_L_FootRotY', 'm_avg_L_FootRotZ',\n",
    "    'm_avg_R_FootRotX', 'm_avg_R_FootRotY', 'm_avg_R_FootRotZ',\n",
    "]\n",
    "\n",
    "weighted_columns_indices = [column_order.index(name) for name in column_names]\n",
    "print(weighted_columns_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, weighted_columns_indices, weight_for_weighted_columns, threshold, penalty_weight):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.weighted_columns_indices = torch.tensor(weighted_columns_indices)\n",
    "        self.weight_for_weighted_columns = weight_for_weighted_columns\n",
    "        self.threshold = threshold  # 신체 움직임의 임계값\n",
    "        self.penalty_weight = penalty_weight  # 비정상적 움직임에 대한 패널티 가중치\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        # 각 조인트의 6개 특성에 대한 손실을 계산하기 위해 y_true와 y_pred의 마지막 차원을 제거\n",
    "        y_pred_values = y_pred[:, :]\n",
    "\n",
    "        # MSE 계산\n",
    "        mse = F.mse_loss(y_true[:, :], y_pred_values, reduction='none')\n",
    "        mse = mse.mean(axis=-1)\n",
    "\n",
    "        # 특정 joint에 대한 가중치 적용\n",
    "        weighted_mse = y_pred[:, self.weighted_columns_indices]\n",
    "        weighted_mse = (weighted_mse ** 2) * self.weight_for_weighted_columns\n",
    "        mse += weighted_mse.mean(axis=-1)\n",
    "\n",
    "        # 과도한 움직임에 대한 패널티 적용\n",
    "        excessive_movement_penalty = (y_pred - y_true).abs() > self.threshold\n",
    "        penalty = excessive_movement_penalty.type(torch.float32) * self.penalty_weight\n",
    "        mse += penalty.mean(axis=-1)\n",
    "\n",
    "        return mse.mean()  # 전체 배치에 대한 평균 손실 반환\n",
    "\n",
    "# 가중치를 적용할 열 인덱스와 가중치 값\n",
    "weighted_columns_indices = weighted_columns_indices\n",
    "weight_for_weighted_columns = 2.0\n",
    "\n",
    "# 임계값과 패널티 가중치 설정\n",
    "threshold = 10.0  # 예시 임계값\n",
    "penalty_weight = 0.8  # 예시 패널티 가중치\n",
    "\n",
    "# CustomLoss 인스턴스 생성\n",
    "custom_loss_instance = CustomLoss(\n",
    "    weighted_columns_indices=weighted_columns_indices,\n",
    "    weight_for_weighted_columns=weight_for_weighted_columns,\n",
    "    threshold=threshold,\n",
    "    penalty_weight=penalty_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화기와 손실 함수\n",
    "optimizer = optim.Adam(transformer_model.parameters(), lr=0.0001)\n",
    "criterion = custom_loss_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설정\n",
    "epochs = 500\n",
    "patience = 7  # Early Stopping patience\n",
    "best_loss = np.inf\n",
    "early_stopping_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Rate Scheduler 설정\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.2, patience=7, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 가능한 GPU 목록을 출력\n",
    "available_gpus = torch.cuda.device_count()\n",
    "print(\"Available GPUs:\", available_gpus)\n",
    "\n",
    "# 현재 장치를 출력 (GPU 사용 가능시 CUDA 장치, 그렇지 않으면 CPU)\n",
    "current_device = torch.cuda.current_device() if torch.cuda.is_available() else 'CPU'\n",
    "print(\"Current device:\", torch.cuda.get_device_name(current_device) if torch.cuda.is_available() else current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = transformer_model.to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(transformer_model, input_size=(n_input, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    transformer_model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    train_progress_bar = tqdm(train_loader, desc=f'Train Epoch {epoch+1}/{epochs}', leave=False)\n",
    "    for batch_inputs, batch_targets in train_progress_bar:\n",
    "        batch_inputs = batch_inputs.to(current_device)\n",
    "        batch_targets = batch_targets.to(current_device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = transformer_model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_dataset)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    transformer_model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    val_progress_bar = tqdm(val_loader, desc=f'Validation Epoch {epoch+1}/{epochs}', leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch_inputs, batch_targets in val_progress_bar:\n",
    "            batch_inputs = batch_inputs.to(current_device)\n",
    "            batch_targets = batch_targets.to(current_device)\n",
    "\n",
    "            outputs = transformer_model(batch_inputs)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            total_val_loss += loss.item() * batch_inputs.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataset)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_loss < best_loss:\n",
    "        best_loss = avg_val_loss\n",
    "        best_model_wts = copy.deepcopy(transformer_model.state_dict())\n",
    "        torch.save({'transformer_model': transformer_model.state_dict(), 'gcn_model': gcn_model.state_dict()}, 'best_model.pth')\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter >= patience:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "# 최고의 모델 가중치 로드\n",
    "transformer_model.load_state_dict(best_model_wts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer_model.state_dict(), 'final_model_Transformer.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Training Loss', color='blue')\n",
    "plt.plot(val_losses, label='Validation Loss', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./B01_TransformData_FinalAvatar_20230922_171230.csv').iloc[300:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rotation_columns = [col for col in test_df.columns if 'Rot' in col]\n",
    "\n",
    "test_position_columns = [col for col in test_df.columns if 'Pos' in col]\n",
    "\n",
    "# DataFrame 분리\n",
    "test_rotation_df = test_df[test_rotation_columns]\n",
    "test_position_df = test_df[test_position_columns]\n",
    "\n",
    "# Rotation 컬럼만 -180~180 사이로 정규화\n",
    "normalize_angle = lambda x:x if x == 999 else (x - 360) if x > 180 else (x + 360) if x < -180 else x\n",
    "test_rotation_df = test_rotation_df.apply(lambda col: col.apply(normalize_angle))\n",
    "\n",
    "# Position 컬럼만 0~1 사이로 정규화\n",
    "def normalize_columns(df):\n",
    "    for col in df.columns:\n",
    "        if 'Pos' in col:  # 위치에 대한 컬럼만 정규화\n",
    "            min_val = df[col].min()\n",
    "            max_val = df[col].max()\n",
    "            df[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "    return df\n",
    "\n",
    "test_position_df = normalize_columns(test_position_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -180 ~ 180 범위를 벗어나는 값이 있는지 확인\n",
    "num_values_out_of_range = test_rotation_df.apply(lambda col: col.apply(lambda x: x != 999 and (x > 180 or x < -180))).sum().sum()\n",
    "\n",
    "# 결과 확인\n",
    "if num_values_out_of_range > 0:\n",
    "    print(f\"범위를 벗어나는 값의 수: {num_values_out_of_range}\")\n",
    "else:\n",
    "    print(\"범위를 벗어나는 값이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조인트 이름 추출\n",
    "joint_names = [col.split('PosX')[0] for col in test_position_df.columns if 'PosX' in col]\n",
    "\n",
    "# 빈 데이터 프레임 생성\n",
    "test_posrot_df = pd.DataFrame()\n",
    "\n",
    "# 각 조인트에 대해 위치 데이터와 회전 데이터를 순차적으로 배열\n",
    "for joint in joint_names:\n",
    "    test_posrot_df[f'{joint}PosX'] = test_position_df[f'{joint}PosX']\n",
    "    test_posrot_df[f'{joint}PosY'] = test_position_df[f'{joint}PosY']\n",
    "    test_posrot_df[f'{joint}PosZ'] = test_position_df[f'{joint}PosZ']\n",
    "    test_posrot_df[f'{joint}RotX'] = test_rotation_df[f'{joint}RotX']\n",
    "    test_posrot_df[f'{joint}RotY'] = test_rotation_df[f'{joint}RotY']\n",
    "    test_posrot_df[f'{joint}RotZ'] = test_rotation_df[f'{joint}RotZ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_posrot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 새로운 CSV 파일로 저장합니다.\n",
    "test_posrot_df.to_csv('./test_posrot_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기\n",
    "model = TransformerModel(\n",
    "    n_features=n_features,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_blocks=num_blocks,\n",
    "    mlp_units=mlp_units,\n",
    "    dropout=dropout_rate,\n",
    "    n_input=n_input\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load('final_model_Transformer.pth'))\n",
    "\n",
    "# 모델을 평가 모드로 설정\n",
    "model.eval()\n",
    "\n",
    "# 예측 값을 넣을 빈 리스트\n",
    "test_predictions = []\n",
    "\n",
    "# 훈련 데이터셋에서 마지막 입력 개수의 값을 가져온 후\n",
    "current_batch = torch.from_numpy(test_posrot_df[-n_input:].values.astype(np.float32)).reshape((1, n_input, n_features))\n",
    "\n",
    "# 모델이 사용하는 디바이스를 확인하고 데이터를 해당 디바이스로 옮깁니다.\n",
    "current_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(current_device)\n",
    "current_batch = current_batch.to(current_device)\n",
    "\n",
    "# 예측 과정 반복\n",
    "with torch.no_grad():  # 그래디언트 계산을 비활성화\n",
    "    for i in range(1):\n",
    "        # 현재 배치에서 다음 포인트를 예측\n",
    "        current_pred = model(current_batch).cpu().numpy()[0]  # 마지막 시퀀스 포인트 예측\n",
    "\n",
    "        # 예측된 마지막 프레임을 리스트에 추가\n",
    "        test_predictions.append(current_pred)\n",
    "\n",
    "        # 새로운 배치 생성: 마지막 시퀀스 제외하고 예측값 추가\n",
    "        current_batch = np.roll(current_batch.cpu().numpy(), -1, axis=1)\n",
    "        current_batch[:, -1, :] = current_pred\n",
    "        current_batch = torch.from_numpy(current_batch).to(current_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions_array = np.array(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변환된 배열을 데이터프레임으로 변환합니다. 이때 column_order 리스트를 열 이름으로 사용합니다.\n",
    "test_predictions = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "# test_predictions 데이터프레임을 CSV 파일로 저장합니다.\n",
    "test_predictions.to_csv('./test_predictions.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input 데이터(test_df)의 마지막 30 프레임과 \n",
    "last_inputs_df = test_posrot_df.iloc[-30:][column_order].reset_index(drop=True)\n",
    "test_predictions_df = pd.DataFrame(test_predictions_array, columns=column_order)\n",
    "\n",
    "test_combined_df = pd.concat([last_inputs_df, test_predictions_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_combined_df.to_csv('./test_combined_df.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_skeleton(df, connections):\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    # Plot each joint\n",
    "    for joint in joint_names:\n",
    "        ax.scatter(df[f'{joint}PosX'], df[f'{joint}PosY'], df[f'{joint}PosZ'], label=joint)\n",
    "\n",
    "    # Draw lines (bones) connecting the joints\n",
    "    for connection in connections:\n",
    "        joint_from, joint_to = connection\n",
    "        # Update to use the correct column names\n",
    "        ax.plot(\n",
    "            [df[f'm_avg_{joint_from}PosX'], df[f'm_avg_{joint_to}PosX']],\n",
    "            [df[f'm_avg_{joint_from}PosY'], df[f'm_avg_{joint_to}PosY']],\n",
    "            [df[f'm_avg_{joint_from}PosZ'], df[f'm_avg_{joint_to}PosZ']],\n",
    "            'k-'\n",
    "        )\n",
    "\n",
    "    # Set labels and legend\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.legend(loc='best')\n",
    "\n",
    "    # Set the initial viewing angle\n",
    "    ax.view_init(elev=20., azim=-90)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "visualize_skeleton(test_predictions.iloc[0], connections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
